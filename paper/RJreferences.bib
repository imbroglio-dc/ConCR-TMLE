
@incollection{lilienfeld_blind_2017,
	address = {Hoboken, NJ, USA},
	title = {Blind {Analysis} as a {Correction} for {Confirmatory} {Bias} in {Physics} and in {Psychology}},
	isbn = {978-1-119-09591-0 978-1-118-66107-9},
	url = {http://doi.wiley.com/10.1002/9781119095910.ch15},
	language = {en},
	urldate = {2020-09-21},
	booktitle = {Psychological {Science} {Under} {Scrutiny}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {MacCoun, Robert J. and Perlmutter, Saul},
	editor = {Lilienfeld, Scott O. and Waldman, Irwin D.},
	month = feb,
	year = {2017},
	doi = {10.1002/9781119095910.ch15},
	pages = {295--322},
	file = {MacCoun and Perlmutter - 2017 - Blind Analysis as a Correction for Confirmatory Bi.pdf:/home/imbroglio/Zotero/storage/SFL4PMRI/MacCoun and Perlmutter - 2017 - Blind Analysis as a Correction for Confirmatory Bi.pdf:application/pdf},
}

@article{hernan_per-protocol_2017,
	title = {Per-{Protocol} {Analyses} of {Pragmatic} {Trials}},
	volume = {377},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMsm1605385},
	doi = {10.1056/NEJMsm1605385},
	abstract = {Pragmatic trials are designed to address real-world questions about care options. This article addresses issues that may arise from per-protocol and intention-to-treat analyses of such trials, outlines alternative analytic approaches, and provides guidance on how to choose among them.},
	number = {14},
	urldate = {2020-08-14},
	journal = {New England Journal of Medicine},
	author = {Hernán, Miguel A. and Robins, James M.},
	month = oct,
	year = {2017},
	pmid = {28976864},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/NEJMsm1605385},
	pages = {1391--1398},
	file = {Snapshot:/home/imbroglio/Zotero/storage/6XTSS9WQ/NEJMsm1605385.html:text/html},
}

@misc{noauthor_tlverse_nodate,
	title = {tlverse},
	url = {https://github.com/tlverse},
	abstract = {The tlverse is the ecosystem of R packages for Targeted Learning, built at the intersection of extensibility and usability - tlverse},
	language = {en},
	urldate = {2020-08-13},
	journal = {GitHub},
	file = {Snapshot:/home/imbroglio/Zotero/storage/DX273ADI/tlverse.html:text/html},
}

@misc{van_der_laan_entering_2014,
	type = {Review {Article}},
	title = {Entering the {Era} of {Data} {Science}: {Targeted} {Learning} and the {Integration} of {Statistics} and {Computational} {Data} {Analysis}},
	shorttitle = {Entering the {Era} of {Data} {Science}},
	url = {https://www.hindawi.com/journals/as/2014/502678/},
	abstract = {This outlook paper reviews the research of van der Laan’s group on Targeted Learning, a subfield of statistics that is concerned with the construction of data adaptive estimators of user-supplied target parameters of the probability distribution of the data and corresponding confidence intervals, aiming at only relying on realistic statistical assumptions. Targeted Learning fully utilizes the state of the art in machine learning tools, while still preserving the important identity of statistics as a field that is concerned with both accurate estimation of the true target parameter value and assessment of uncertainty in order to make sound statistical conclusions. We also provide a philosophical historical perspective on Targeted Learning, also relating it to the new developments in Big Data. We conclude with some remarks explaining the immediate relevance of Targeted Learning to the current Big Data movement.},
	language = {en},
	urldate = {2020-08-11},
	journal = {Advances in Statistics},
	author = {van der Laan, Mark J. and Starmans, Richard J. C. M.},
	month = sep,
	year = {2014},
	doi = {https://doi.org/10.1155/2014/502678},
	doi = {https://doi.org/10.1155/2014/502678},
	note = {ISSN: 2356-6892
Pages: e502678
Publisher: Hindawi
Volume: 2014},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/RPJBEEBZ/van der Laan and Starmans - 2014 - Entering the Era of Data Science Targeted Learnin.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/4QWYSE2W/502678.html:text/html},
}

@article{andersen_competing_2012,
	title = {Competing risks in epidemiology: possibilities and pitfalls},
	volume = {41},
	issn = {0300-5771},
	shorttitle = {Competing risks in epidemiology},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3396320/},
	doi = {10.1093/ije/dyr213},
	abstract = {Background In studies of all-cause mortality, the fundamental epidemiological concepts of rate and risk are connected through a well-defined one-to-one relation. An important consequence of this relation is that regression models such as the proportional hazards model that are defined through the hazard (the rate) immediately dictate how the covariates relate to the survival function (the risk)., Methods This introductory paper reviews the concepts of rate and risk and their one-to-one relation in all-cause mortality studies and introduces the analogous concepts of rate and risk in the context of competing risks, the cause-specific hazard and the cause-specific cumulative incidence function., Results The key feature of competing risks is that the one-to-one correspondence between cause-specific hazard and cumulative incidence, between rate and risk, is lost. This fact has two important implications. First, the naïve Kaplan–Meier that takes the competing events as censored observations, is biased. Secondly, the way in which covariates are associated with the cause-specific hazards may not coincide with the way these covariates are associated with the cumulative incidence. An example with relapse and non-relapse mortality as competing risks in a stem cell transplantation study is used for illustration., Conclusion The two implications of the loss of one-to-one correspondence between cause-specific hazard and cumulative incidence should be kept in mind when deciding on how to make inference in a competing risks situation.},
	number = {3},
	urldate = {2020-08-11},
	journal = {International Journal of Epidemiology},
	author = {Andersen, Per Kragh and Geskus, Ronald B and de Witte, Theo and Putter, Hein},
	month = jun,
	year = {2012},
	pmid = {22253319},
	pmcid = {PMC3396320},
	pages = {861--870},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/7TK6PATZ/Andersen et al. - 2012 - Competing risks in epidemiology possibilities and.pdf:application/pdf},
}

@article{ascierto_ipilimumab_2017,
	title = {Ipilimumab 10 mg/kg versus ipilimumab 3 mg/kg in patients with unresectable or metastatic melanoma: a randomised, double-blind, multicentre, phase 3 trial},
	volume = {18},
	issn = {1470-2045},
	shorttitle = {Ipilimumab 10 mg/kg versus ipilimumab 3 mg/kg in patients with unresectable or metastatic melanoma},
	url = {http://www.sciencedirect.com/science/article/pii/S1470204517302310},
	doi = {10.1016/S1470-2045(17)30231-0},
	abstract = {Background
A phase 2 trial suggested increased overall survival and increased incidence of treatment-related grade 3–4 adverse events with ipilimumab 10 mg/kg compared with ipilimumab 3 mg/kg in patients with advanced melanoma. We report a phase 3 trial comparing the benefit–risk profile of ipilimumab 10 mg/kg versus 3 mg/kg.
Methods
This randomised, double-blind, multicentre, phase 3 trial was done in 87 centres in 21 countries worldwide. Patients with untreated or previously treated unresectable stage III or IV melanoma, without previous treatment with BRAF inhibitors or immune checkpoint inhibitors, were randomly assigned (1:1) with an interactive voice response system by the permuted block method using block size 4 to ipilimumab 10 mg/kg or 3 mg/kg, administered by intravenous infusion for 90 min every 3 weeks for four doses. Patients were stratified by metastasis stage, previous treatment for metastatic melanoma, and Eastern Cooperative Oncology Group performance status. The patients, investigators, and site staff were masked to treatment assignment. The primary endpoint was overall survival in the intention-to-treat population and safety was assessed in all patients who received at least one dose of study treatment. This study is completed and was registered with ClinicalTrials.gov, number NCT01515189.
Findings
Between Feb 29, and July 9, 2012, 727 patients were enrolled and randomly assigned to ipilimumab 10 mg/kg (365 patients; 364 treated) or ipilimumab 3 mg/kg (362 patients; all treated). Median follow-up was 14·5 months (IQR 4·6–42·3) for the ipilimumab 10 mg/kg group and 11·2 months (4·9–29·4) for the ipilimumab 3 mg/kg group. Median overall survival was 15·7 months (95\% CI 11·6–17·8) for ipilimumab 10 mg/kg compared with 11·5 months (9·9–13·3) for ipilimumab 3 mg/kg (hazard ratio 0·84, 95\% CI 0·70–0·99; p=0·04). The most common grade 3–4 treatment-related adverse events were diarrhoea (37 [10\%] of 364 patients in the 10 mg/kg group vs 21 [6\%] of 362 patients in the 3 mg/kg group), colitis (19 [5\%] vs nine [2\%]), increased alanine aminotransferase (12 [3\%] vs two [1\%]), and hypophysitis (ten [3\%] vs seven [2\%]). Treatment-related serious adverse events were reported in 133 (37\%) patients in the 10 mg/kg group and 66 (18\%) patients in the 3 mg/kg group; four (1\%) versus two ({\textless}1\%) patients died from treatment-related adverse events.
Interpretation
In patients with advanced melanoma, ipilimumab 10 mg/kg resulted in significantly longer overall survival than did ipilimumab 3 mg/kg, but with increased treatment-related adverse events. Although the treatment landscape for advanced melanoma has changed since this study was initiated, the clinical use of ipilimumab in refractory patients with unmet medical needs could warrant further assessment.
Funding
Bristol-Myers Squibb.},
	language = {en},
	number = {5},
	urldate = {2020-08-11},
	journal = {The Lancet Oncology},
	author = {Ascierto, Paolo A and Del Vecchio, Michele and Robert, Caroline and Mackiewicz, Andrzej and Chiarion-Sileni, Vanna and Arance, Ana and Lebbé, Céleste and Bastholt, Lars and Hamid, Omid and Rutkowski, Piotr and McNeil, Catriona and Garbe, Claus and Loquai, Carmen and Dreno, Brigitte and Thomas, Luc and Grob, Jean-Jacques and Liszkay, Gabriella and Nyakas, Marta and Gutzmer, Ralf and Pikiel, Joanna and Grange, Florent and Hoeller, Christoph and Ferraresi, Virginia and Smylie, Michael and Schadendorf, Dirk and Mortier, Laurent and Svane, Inge Marie and Hennicken, Delphine and Qureshi, Anila and Maio, Michele},
	month = may,
	year = {2017},
	pages = {611--622},
	file = {ScienceDirect Full Text PDF:/home/imbroglio/Zotero/storage/SPW6S7J8/Ascierto et al. - 2017 - Ipilimumab 10 mgkg versus ipilimumab 3 mgkg in p.pdf:application/pdf;ScienceDirect Snapshot:/home/imbroglio/Zotero/storage/AWQ5WSDU/S1470204517302310.html:text/html},
}

@article{mok_gefitinib_2009,
	title = {Gefitinib or {Carboplatin}–{Paclitaxel} in {Pulmonary} {Adenocarcinoma}},
	volume = {361},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMoa0810699},
	doi = {10.1056/NEJMoa0810699},
	abstract = {This trial compared gefitinib, an inhibitor of the tyrosine kinase of epidermal growth factor (EGFR), with carboplatin plus paclitaxel as initial treatment of pulmonary adenocarcinoma in more than 1200 East Asian patients. The primary end point, progression-free survival, was significantly longer with gefitinib therapy among patients whose tumors carried an EGFR mutation and with carboplatin plus paclitaxel therapy among patients with mutation-negative tumors.},
	number = {10},
	urldate = {2020-08-11},
	journal = {New England Journal of Medicine},
	author = {Mok, Tony S. and Wu, Yi-Long and Thongprasert, Sumitra and Yang, Chih-Hsin and Chu, Da-Tong and Saijo, Nagahiro and Sunpaweravong, Patrapim and Han, Baohui and Margono, Benjamin and Ichinose, Yukito and Nishiwaki, Yutaka and Ohe, Yuichiro and Yang, Jin-Ji and Chewaskulyong, Busyamas and Jiang, Haiyi and Duffield, Emma L. and Watkins, Claire L. and Armour, Alison A. and Fukuoka, Masahiro},
	month = sep,
	year = {2009},
	pmid = {19692680},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/NEJMoa0810699},
	pages = {947--957},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/5UF57IPC/Mok et al. - 2009 - Gefitinib or Carboplatin–Paclitaxel in Pulmonary A.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/EBJGY76N/NEJMoa0810699.html:text/html},
}

@article{guyot_enhanced_2012,
	title = {Enhanced secondary analysis of survival data: reconstructing the data from published {Kaplan}-{Meier} survival curves},
	volume = {12},
	issn = {1471-2288},
	shorttitle = {Enhanced secondary analysis of survival data},
	url = {https://doi.org/10.1186/1471-2288-12-9},
	doi = {10.1186/1471-2288-12-9},
	abstract = {The results of Randomized Controlled Trials (RCTs) on time-to-event outcomes that are usually reported are median time to events and Cox Hazard Ratio. These do not constitute the sufficient statistics required for meta-analysis or cost-effectiveness analysis, and their use in secondary analyses requires strong assumptions that may not have been adequately tested. In order to enhance the quality of secondary data analyses, we propose a method which derives from the published Kaplan Meier survival curves a close approximation to the original individual patient time-to-event data from which they were generated.},
	number = {1},
	urldate = {2020-08-11},
	journal = {BMC Medical Research Methodology},
	author = {Guyot, Patricia and Ades, AE and Ouwens, Mario JNM and Welton, Nicky J.},
	month = feb,
	year = {2012},
	pages = {9},
	file = {Full Text:/home/imbroglio/Zotero/storage/KM6VR7EM/Guyot et al. - 2012 - Enhanced secondary analysis of survival data reco.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/FUXUYRY2/1471-2288-12-9.html:text/html},
}

@article{zukin_randomized_2013,
	title = {Randomized phase {III} trial of single-agent pemetrexed versus carboplatin and pemetrexed in patients with advanced non-small-cell lung cancer and {Eastern} {Cooperative} {Oncology} {Group} performance status of 2},
	volume = {31},
	issn = {1527-7755},
	doi = {10.1200/JCO.2012.48.1911},
	abstract = {PURPOSE: To compare single-agent pemetrexed (P) versus the combination of carboplatin and pemetrexed (CP) in first-line therapy for patients with advanced non-small-cell lung cancer (NSCLC) with an Eastern Cooperative Oncology Group (ECOG) performance status (PS) of 2.
PATIENTS AND METHODS: In a multicenter phase III randomized trial, patients with advanced NSCLC, ECOG PS of 2, any histology at first and later amended to nonsquamous only, no prior chemotherapy, and adequate organ function were randomly assigned to P alone (500 mg/m(2)) or CP (area under the curve of 5 and 500 mg/m(2), respectively) administered every 3 weeks for a total of four cycles. The primary end point was overall survival (OS).
RESULTS: A total of 205 eligible patients were enrolled from eight centers in Brazil and one in the United States from April 2008 to July 2011. The response rates were 10.3\% for P and 23.8\% for CP (P = .032). In the intent-to-treat population, the median PFS was 2.8 months for P and 5.8 months for CP (hazard ratio [HR], 0.46; 95\% CI, 0.35 to 0.63; P {\textless} .001), and the median OS was 5.3 months for P and 9.3 months for CP (HR, 0.62; 95\% CI, 0.46 to 0.83; P = .001). One-year survival rates were 21.9\% and 40.1\%, respectively. Similar results were seen when patients with squamous disease were excluded from the analysis. Anemia (grade 3, 3.9\%; grade 4, 11.7\%) and neutropenia (grade 3, 1\%; grade 4, 6.8\%) were more frequent with CP. There were four treatment-related deaths in the CP arm.
CONCLUSION: Combination chemotherapy with CP significantly improves survival in patients with advanced NSCLC and ECOG PS of 2.},
	language = {eng},
	number = {23},
	journal = {Journal of Clinical Oncology: Official Journal of the American Society of Clinical Oncology},
	author = {Zukin, Mauro and Barrios, Carlos H. and Pereira, Jose Rodrigues and Ribeiro, Ronaldo De Albuquerque and Beato, Carlos Augusto de Mendonça and do Nascimento, Yeni Neron and Murad, Andre and Franke, Fabio A. and Precivale, Maristela and Araujo, Luiz Henrique de Lima and Baldotto, Clarissa Serodio Da Rocha and Vieira, Fernando Meton and Small, Isabele A. and Ferreira, Carlos G. and Lilenbaum, Rogerio C.},
	month = aug,
	year = {2013},
	pmid = {23775961},
	keywords = {Adult, Aged, Aged, 80 and over, Antimetabolites, Antineoplastic, Antineoplastic Combined Chemotherapy Protocols, Carboplatin, Carcinoma, Non-Small-Cell Lung, Disease-Free Survival, Female, Glutamates, Guanine, Humans, Lung Neoplasms, Male, Middle Aged, Pemetrexed, Prospective Studies, Survival Rate},
	pages = {2849--2853},
	file = {Submitted Version:/home/imbroglio/Zotero/storage/DY6M5CNC/Zukin et al. - 2013 - Randomized phase III trial of single-agent pemetre.pdf:application/pdf},
}

@article{balzer_statistical_2018,
	title = {Statistical {Analysis} {Plan} for {SEARCH} {Phase} {I}: {Health} {Outcomes} among {Adults}},
	shorttitle = {Statistical {Analysis} {Plan} for {SEARCH} {Phase} {I}},
	url = {http://arxiv.org/abs/1808.03231},
	abstract = {This document provides the analytic plan for evaluating adult HIV incidence, health, and implementation outcomes for the first phase of the SEARCH Study. Locked: November 27, 2017. Embargoed until July 25, 2018.},
	urldate = {2020-08-11},
	journal = {arXiv:1808.03231 [stat]},
	author = {Balzer, Laura B. and Havlir, Diane V. and Schwab, Joshua and Van Der Laan, Mark J. and Petersen, Maya L.},
	month = jul,
	year = {2018},
	note = {arXiv: 1808.03231},
	keywords = {Statistics - Applications},
	annote = {Comment: 40 pgs},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/DZ6Y4PSW/Balzer et al. - 2018 - Statistical Analysis Plan for SEARCH Phase I Heal.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/VC7KXHE7/1808.html:text/html},
}

@article{kahan_how_2020,
	title = {How to design a pre-specified statistical analysis approach to limit p-hacking in clinical trials: the {Pre}-{SPEC} framework},
	shorttitle = {How to design a pre-specified statistical analysis approach to limit p-hacking in clinical trials},
	url = {http://arxiv.org/abs/1907.04078},
	abstract = {Results from clinical trials can be susceptible to bias if investigators choose their analysis approach after seeing trial data, as this can allow them to perform multiple analyses and then choose the method that provides the most favourable result (commonly referred to as 'p-hacking'). Pre-specification of the planned analysis approach is essential to help reduce such bias, as it ensures analytical methods are chosen in advance of seeing the trial data. However, pre-specification is only effective if done in a way that does not allow p-hacking. For example, investigators may pre-specify a certain statistical method such as multiple imputation, but give little detail on how it will be implemented. Because there are many different ways to perform multiple imputation, this approach to pre-specification is ineffective, as it still allows investigators to analyse the data in different ways before deciding on a final approach. In this article we describe a five-point framework (the Pre-SPEC framework) for designing a pre-specified analysis approach that does not allow p-hacking. This framework is intended to be used in conjunction with the SPIRIT (Standard Protocol Items: Recommendations for Interventional Trials) statement and other similar guidelines to help investigators design the statistical analysis strategy for the trial's primary outcome in the trial protocol.},
	urldate = {2020-08-11},
	journal = {arXiv:1907.04078 [stat]},
	author = {Kahan, Brennan C. and Forbes, Gordon and Cro, Suzie},
	month = jul,
	year = {2020},
	note = {arXiv: 1907.04078},
	keywords = {Statistics - Methodology},
	annote = {Comment: 26 pages, 3 tables, 0 figures},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/LZB3L4Q8/Kahan et al. - 2020 - How to design a pre-specified statistical analysis.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/3HXUCPTN/1907.html:text/html},
}

@article{kreif_evaluation_2015,
	title = {Evaluation of the {Effect} of a {Continuous} {Treatment}: {A} {Machine} {Learning} {Approach} with an {Application} to {Treatment} for {Traumatic} {Brain} {Injury}},
	volume = {24},
	issn = {1057-9230},
	shorttitle = {Evaluation of the {Effect} of a {Continuous} {Treatment}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4744663/},
	doi = {10.1002/hec.3189},
	abstract = {For a continuous treatment, the generalised propensity score (GPS) is defined as the conditional density of the treatment, given covariates. GPS adjustment may be implemented by including it as a covariate in an outcome regression. Here, the unbiased estimation of the dose–response function assumes correct specification of both the GPS and the outcome‐treatment relationship. This paper introduces a machine learning method, the ‘Super Learner’, to address model selection in this context. In the two‐stage estimation approach proposed, the Super Learner selects a GPS and then a dose–response function conditional on the GPS, as the convex combination of candidate prediction algorithms. We compare this approach with parametric implementations of the GPS and to regression methods. We contrast the methods in the Risk Adjustment in Neurocritical care cohort study, in which we estimate the marginal effects of increasing transfer time from emergency departments to specialised neuroscience centres, for patients with acute traumatic brain injury. With parametric models for the outcome, we find that dose–response curves differ according to choice of specification. With the Super Learner approach to both regression and the GPS, we find that transfer time does not have a statistically significant marginal effect on the outcomes. © 2015 The Authors. Health Economics Published by John Wiley \& Sons Ltd.},
	number = {9},
	urldate = {2020-08-11},
	journal = {Health Economics},
	author = {Kreif, Noémi and Grieve, Richard and Díaz, Iván and Harrison, David},
	month = sep,
	year = {2015},
	pmid = {26059721},
	pmcid = {PMC4744663},
	pages = {1213--1228},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/5M4CHU9K/Kreif et al. - 2015 - Evaluation of the Effect of a Continuous Treatment.pdf:application/pdf},
}

@article{zheng_constrained_2018,
	title = {Constrained binary classification using ensemble learning: an application to cost-efficient targeted {PrEP} strategies},
	volume = {37},
	issn = {0277-6715},
	shorttitle = {Constrained binary classification using ensemble learning},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5701877/},
	doi = {10.1002/sim.7296},
	abstract = {Binary classifications problems are ubiquitous in health and social sciences. In many cases, one wishes to balance two competing optimality considerations for a binary classifier. For instance, in resource-limited settings, an HIV prevention program based on offering Pre-Exposure Prophylaxis (PrEP) to select high-risk individuals must balance the sensitivity of the binary classifier in detecting future seroconverters (and hence offering them PrEP regimens) with the total number of PrEP regimens that is financially and logistically feasible for the program. In this article, we consider a general class of constrained binary classification problems wherein the objective function and the constraint are both monotonic with respect to a threshold. These include the minimization of the rate of positive predictions subject to a minimum sensitivity, the maximization of sensitivity subject to a maximum rate of positive predictions, and the Neyman-Pearson paradigm, which minimizes the type II error subject to an upper bound on the type I error. We propose an ensemble approach to these binary classification problems based on the Super Learner methodology. This approach linearly combines a user-supplied library of scoring algorithms, with combination weights and a discriminating threshold chosen to minimize the constrained optimality criterion. We then illustrate the application of the proposed classifier to develop an individualized PrEP targeting strategy in a resource-limited setting, with the goal of minimizing the number of PrEP offerings while achieving a minimum required sensitivity. This proof of concept data analysis uses baseline data from the ongoing Sustainable East Africa Research in Community Health study.},
	number = {2},
	urldate = {2020-08-11},
	journal = {Statistics in medicine},
	author = {Zheng, Wenjing and Balzer, Laura and van der Laan, Mark and Petersen, Maya},
	month = jan,
	year = {2018},
	pmid = {28384841},
	pmcid = {PMC5701877},
	pages = {261--279},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/XUV2Y9SQ/Zheng et al. - 2018 - Constrained binary classification using ensemble l.pdf:application/pdf},
}

@article{pirracchio_mortality_2015,
	title = {Mortality prediction in the {ICU}: can we do better? {Results} from the {Super} {ICU} {Learner} {Algorithm} ({SICULA}) project, a population-based study},
	volume = {3},
	issn = {2213-2600},
	shorttitle = {Mortality prediction in the {ICU}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4321691/},
	doi = {10.1016/S2213-2600(14)70239-5},
	abstract = {Background
Improved mortality prediction for patients in intensive care units (ICU) remains an important challenge. Many severity scores have been proposed but validation studies have concluded that they are not adequately calibrated. Many flexible algorithms are available, yet none of these individually outperform all others regardless of context. In contrast, the Super Learner (SL), an ensemble machine learning technique that leverages on multiple learning algorithms to obtain better prediction performance, has been shown to perform at least as well as the optimal member of its library. It might provide an ideal opportunity to construct a novel severity score with an improved performance profile. The aim of the present study was to provide a new mortality prediction algorithm for ICU patients using an implementation of the Super Learner, and to assess its performance relative to prediction based on the SAPS II, APACHE II and SOFA scores.

Methods
We used the Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II) database (v26) including all patients admitted to an ICU at Boston’s Beth Israel Deaconess Medical Center from 2001 to 2008. The calibration, discrimination and risk classification of predicted hospital mortality based on SAPS II, on APACHE II, on SOFA and on our Super Learned-based proposal were evaluated. Performance measures were calculated using cross-validation to avoid making biased assessments. Our proposed score was then externally validated on a dataset of 200 randomly selected patients admitted at the ICU of Hôpital Européen Georges-Pompidou in Paris, France between September 2013 and June 2014. The primary outcome was hospital mortality. The explanatory variables were the same as those included in the SAPS II score.

Results
24,508 patients were included, with median SAPS II 38 (IQR: 27–51), median SOFA 5 (IQR: 2–8). A total of 3,002/24,508(12.2\%) patients died in the hospital. The two versions of our Super Learner-based proposal yielded average predicted probabilities of death of 0.12 (IQR: 0.02–0.16) and 0.13 (IQR: 0.01–0.19), whereas the corresponding values for the SOFA and SAPS II scores were, respectively, 0.12 (IQR: 0.05–0.15) and 0.30 (IQR: 0.08–0.48). The cross-validated area under the receiver operating characteristics curve (AUROC) for SAPS II and SOFA were 0.78(95\%CI: 0.77–0.78) and 0.71 (95\%CI: 0.71–0.72), respectively. Our proposal reached an AUROC of 0.85 (95\%CI: 0.84–0.85) when the explanatory variables were categorized as in SAPS II, and of 0.88 (95\%CI: 0.87–0.89) when the same explanatory variables were included without any transformation. In addition, it exhibited better calibration properties than previous score systems. On the external validation dataset, the AUROC was 0.94 (95\%CI: 0.90–0.98) and calibration properties were good.

Interpretation
As compared to conventional severity scores, our Super Learner-based proposal offers improved performance for predicting hospital mortality in ICU patients. A user-friendly implementation is available online and should prove useful to clinicians seeking to validate our score.

Funding
Fulbright Foundation, Assistance Publique – Hôpitaux de Paris (RP); Doris Duke Clinical Scientist Development Award (MP) and NIH Grant \# 2R01AI074345-06A1(MvdL).},
	number = {1},
	urldate = {2020-08-11},
	journal = {The Lancet. Respiratory medicine},
	author = {Pirracchio, Romain and Petersen, Maya L. and Carone, Marco and Rigon, Matthieu Resche and Chevret, Sylvie and van der LAAN, Mark J.},
	month = jan,
	year = {2015},
	pmid = {25466337},
	pmcid = {PMC4321691},
	pages = {42--52},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/52ZQY528/Pirracchio et al. - 2015 - Mortality prediction in the ICU can we do better.pdf:application/pdf},
}

@article{uno_alternatives_2015,
	title = {Alternatives to hazard ratios for comparing efficacy or safety of therapies in noninferiority studies},
	volume = {163},
	issn = {0003-4819},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4510023/},
	doi = {10.7326/M14-1741},
	abstract = {A noninferiority study is often used to investigate whether a treatment’s efficacy or safety profile is acceptable compared to an alternative therapy regarding the time to a clinical event. The empirical quantification of the treatment difference for such a study is routinely based on the hazard ratio estimate. The hazard ratio, which is not a relative risk, may be difficult to interpret clinically, especially when the underlying proportional hazards assumption is violated. The precision of the hazard ratio estimate depends primarily on the number of observed events, but not directly on either exposure times or sample size of the study population. If the event rate is low, the study may require an impractically large number of events to ensure that the prespecified noninferiority criterion for the hazard ratio is attainable. This article discusses deficiencies of the current approach for design and analysis of a noninferiority study. We then provide alternative procedures, which do not depend on any model assumption, to compare two treatments. For a noninferiority safety study, the patients’ exposure times are more clinically important than the observed number of events. If the study patients’ exposure times are long enough to evaluate safety reliably, these alternative procedures can effectively provide clinically interpretable evidence on safety, even with relatively few observed events. We illustrate these procedures with data from two studies. One explores the cardiovascular safety of a pain medicine; the second examines the cardiovascular safety of a new treatment for diabetes. These alternative strategies to evaluate safety or efficacy of an intervention lead to more meaningful interpretations of the analysis results than the conventional one via the hazard ratio estimate.},
	number = {2},
	urldate = {2020-08-11},
	journal = {Annals of internal medicine},
	author = {Uno, Hajime and Wittes, Janet and Fu, Haoda and Solomon, Scott D. and Claggett, Brian and Tian, Lu and Cai, Tianxi and Pfeffer, Marc A. and Evans, Scott R. and Wei, Lee-Jen},
	month = jul,
	year = {2015},
	pmid = {26054047},
	pmcid = {PMC4510023},
	pages = {127--134},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/GYIF2E3R/Uno et al. - 2015 - Alternatives to hazard ratios for comparing effica.pdf:application/pdf},
}

@article{lin_alternative_2020,
	title = {Alternative {Analysis} {Methods} for {Time} to {Event} {Endpoints} {Under} {Nonproportional} {Hazards}: {A} {Comparative} {Analysis}},
	volume = {12},
	issn = {null},
	shorttitle = {Alternative {Analysis} {Methods} for {Time} to {Event} {Endpoints} {Under} {Nonproportional} {Hazards}},
	url = {https://doi.org/10.1080/19466315.2019.1697738},
	doi = {10.1080/19466315.2019.1697738},
	abstract = {The log-rank test is most powerful under proportional hazards (PH). In practice, non-PH patterns are often observed in clinical trials, such as in immuno-oncology; therefore, alternative methods are needed to restore the efficiency of statistical testing. Three categories of testing methods were evaluated, including weighted log-rank tests, Kaplan–Meier curve-based tests (including weighted Kaplan–Meier and restricted mean survival time), and combination tests (including Breslow test, Lee’s combo test, and MaxCombo test). Nine scenarios representing the PH and various non-PH patterns were simulated. The power, Type I error, and effect estimate of each method were compared. In general, all tests control Type I error well. There is not a single most powerful test across all scenarios. In the absence of prior knowledge regarding the underlying or non-PH patterns, the MaxCombo test is relatively robust across patterns. Since the treatment effect changes over time under non-PH, the overall profile of the treatment effect may not be represented comprehensively based on a single measure. Thus, multiple measures of the treatment effect should be prespecified as sensitivity analyses to describe the totality of the data. Supplementary materials for this article are available online.},
	number = {2},
	urldate = {2020-08-11},
	journal = {Statistics in Biopharmaceutical Research},
	author = {Lin, Ray S. and Lin, Ji and Roychoudhury, Satrajit and Anderson, Keaven M. and Hu, Tianle and Huang, Bo and Leon, Larry F. and Liao, Jason J. Z. and Liu, Rong and Luo, Xiaodong and Mukhopadhyay, Pralay and Qin, Rui and Tatsuoka, Kay and Wang, Xuejing and Wang, Yang and Zhu, Jian and Chen, Tai-Tsang and Iacona, Renee and Group, Cross-Pharma Non-proportional Hazards Working},
	month = apr,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/19466315.2019.1697738},
	keywords = {Fleming–Harrington test, Log-rank test, Nonproportional hazards, Oncology trial, Survival analysis},
	pages = {187--198},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/NJ7QZSGD/Lin et al. - 2020 - Alternative Analysis Methods for Time to Event End.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/MT4LVX9G/19466315.2019.html:text/html},
}

@article{uno_moving_2014,
	title = {Moving {Beyond} the {Hazard} {Ratio} in {Quantifying} the {Between}-{Group} {Difference} in {Survival} {Analysis}},
	volume = {32},
	issn = {0732-183X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4105489/},
	doi = {10.1200/JCO.2014.55.2208},
	abstract = {In a longitudinal clinical study to compare two groups, the primary end point is often the time to a specific event (eg, disease progression, death). The hazard ratio estimate is routinely used to empirically quantify the between-group difference under the assumption that the ratio of the two hazard functions is approximately constant over time. When this assumption is plausible, such a ratio estimate may capture the relative difference between two survival curves. However, the clinical meaning of such a ratio estimate is difficult, if not impossible, to interpret when the underlying proportional hazards assumption is violated (ie, the hazard ratio is not constant over time). Although this issue has been studied extensively and various alternatives to the hazard ratio estimator have been discussed in the statistical literature, such crucial information does not seem to have reached the broader community of health science researchers. In this article, we summarize several critical concerns regarding this conventional practice and discuss various well-known alternatives for quantifying the underlying differences between groups with respect to a time-to-event end point. The data from three recent cancer clinical trials, which reflect a variety of scenarios, are used throughout to illustrate our discussions. When there is not sufficient information about the profile of the between-group difference at the design stage of the study, we encourage practitioners to consider a prespecified, clinically meaningful, model-free measure for quantifying the difference and to use robust estimation procedures to draw primary inferences.},
	number = {22},
	urldate = {2020-08-11},
	journal = {Journal of Clinical Oncology},
	author = {Uno, Hajime and Claggett, Brian and Tian, Lu and Inoue, Eisuke and Gallo, Paul and Miyata, Toshio and Schrag, Deborah and Takeuchi, Masahiro and Uyama, Yoshiaki and Zhao, Lihui and Skali, Hicham and Solomon, Scott and Jacobus, Susanna and Hughes, Michael and Packer, Milton and Wei, Lee-Jen},
	month = aug,
	year = {2014},
	pmid = {24982461},
	pmcid = {PMC4105489},
	pages = {2380--2385},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/WHGILSVC/Uno et al. - 2014 - Moving Beyond the Hazard Ratio in Quantifying the .pdf:application/pdf},
}

@article{stensrud_limitations_2019,
	title = {Limitations of hazard ratios in clinical trials},
	volume = {40},
	issn = {0195-668X},
	url = {https://academic.oup.com/eurheartj/article/40/17/1378/5219649},
	doi = {10.1093/eurheartj/ehy770},
	abstract = {Abstract.},
	language = {en},
	number = {17},
	urldate = {2020-08-11},
	journal = {European Heart Journal},
	author = {Stensrud, Mats J. and Aalen, John M. and Aalen, Odd O. and Valberg, Morten},
	month = may,
	year = {2019},
	note = {Publisher: Oxford Academic},
	pages = {1378--1383},
	file = {Cai, vdLaan - One-Step TMLE for Time-to-Event Outcomes - 20192019.pdf:/home/imbroglio/Zotero/storage/KU8872AD/Cai, vdLaan - One-Step TMLE for Time-to-Event Outcomes - 20192019.pdf:application/pdf;Full Text PDF:/home/imbroglio/Zotero/storage/GHUAEA3X/Stensrud et al. - 2019 - Limitations of hazard ratios in clinical trials.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/9BY5RQ4B/5219649.html:text/html},
}

@article{aalen_does_2015,
	title = {Does {Cox} analysis of a randomized survival study yield a causal treatment effect?},
	volume = {21},
	issn = {1572-9249},
	url = {https://doi.org/10.1007/s10985-015-9335-y},
	doi = {10.1007/s10985-015-9335-y},
	abstract = {Statistical methods for survival analysis play a central role in the assessment of treatment effects in randomized clinical trials in cardiovascular disease, cancer, and many other fields. The most common approach to analysis involves fitting a Cox regression model including a treatment indicator, and basing inference on the large sample properties of the regression coefficient estimator. Despite the fact that treatment assignment is randomized, the hazard ratio is not a quantity which admits a causal interpretation in the case of unmodelled heterogeneity. This problem arises because the risk sets beyond the first event time are comprised of the subset of individuals who have not previously failed. The balance in the distribution of potential confounders between treatment arms is lost by this implicit conditioning, whether or not censoring is present. Thus while the Cox model may be used as a basis for valid tests of the null hypotheses of no treatment effect if robust variance estimates are used, modeling frameworks more compatible with causal reasoning may be preferrable in general for estimation.},
	language = {en},
	number = {4},
	urldate = {2020-08-11},
	journal = {Lifetime Data Analysis},
	author = {Aalen, Odd O. and Cook, Richard J. and Røysland, Kjetil},
	month = oct,
	year = {2015},
	pages = {579--593},
	file = {Springer Full Text PDF:/home/imbroglio/Zotero/storage/F22R5W6X/Aalen et al. - 2015 - Does Cox analysis of a randomized survival study y.pdf:application/pdf},
}

@article{hernan_hazards_2010,
	title = {The {Hazards} of {Hazard} {Ratios}},
	volume = {21},
	issn = {1044-3983},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3653612/},
	doi = {10.1097/EDE.0b013e3181c1ea43},
	number = {1},
	urldate = {2020-08-11},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Hernán, Miguel A.},
	month = jan,
	year = {2010},
	pmid = {20010207},
	pmcid = {PMC3653612},
	pages = {13--15},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/3ZCCFDST/Hernán - 2010 - The Hazards of Hazard Ratios.pdf:application/pdf},
}

@article{mehrotra_seeking_2016,
	title = {Seeking harmony: estimands and sensitivity analyses for confirmatory clinical trials},
	volume = {13},
	issn = {1740-7745},
	shorttitle = {Seeking harmony},
	url = {https://doi.org/10.1177/1740774516633115},
	doi = {10.1177/1740774516633115},
	abstract = {In October 2014, the Steering Committee of the International Conference on Harmonization endorsed the formation of an expert working group to develop an addendum to the International Conference on Harmonization E9 guideline (“Statistical Principles for Clinical Trials”). The addendum will focus on two topics involving randomized confirmatory clinical trials: estimands and sensitivity analyses. Both topics are motivated, in part, by the need to improve the precision with which scientific questions of interest are formulated and addressed by clinical trialists and regulators, specifically in the context of post-randomization events such as use of rescue medication or missing data resulting from dropouts. Given the importance of these topics for the statistical and medical community, we articulate the reasons for the planned addendum. The resulting “ICH E9/R1” guideline will include a framework for improved trial planning, conduct, analysis, and interpretation; a draft is expected to be ready for public comment in the second half of 2016.},
	language = {en},
	number = {4},
	urldate = {2020-08-11},
	journal = {Clinical Trials},
	author = {Mehrotra, Devan V and Hemmings, Robert J and Russek-Cohen, Estelle},
	month = aug,
	year = {2016},
	note = {Publisher: SAGE Publications},
	pages = {456--458},
}

@article{lu_improving_2008,
	title = {Improving the efficiency of the log-rank test using auxiliary covariates},
	volume = {95},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/95/3/679/217101},
	doi = {10.1093/biomet/asn003},
	abstract = {Abstract.  Under the assumption of proportional hazards, the log-rank test is optimal for testing the null hypothesis , where  denotes the logarithm of the haza},
	language = {en},
	number = {3},
	urldate = {2020-08-11},
	journal = {Biometrika},
	author = {Lu, Xiaomin and Tsiatis, Anastasios A.},
	month = sep,
	year = {2008},
	note = {Publisher: Oxford Academic},
	pages = {679--694},
	file = {Snapshot:/home/imbroglio/Zotero/storage/P4G5Y2KU/217101.html:text/html;Submitted Version:/home/imbroglio/Zotero/storage/M6WK5KX4/Lu and Tsiatis - 2008 - Improving the efficiency of the log-rank test usin.pdf:application/pdf},
}

@article{american_diabetes_association_10_2020,
	title = {10. {Cardiovascular} {Disease} and {Risk} {Management}: {Standards} of {Medical} {Care} in {Diabetes}-2020},
	volume = {43},
	issn = {1935-5548},
	shorttitle = {10. {Cardiovascular} {Disease} and {Risk} {Management}},
	doi = {10.2337/dc20-S010},
	abstract = {The American Diabetes Association (ADA) "Standards of Medical Care in Diabetes" includes the ADA's current clinical practice recommendations and is intended to provide the components of diabetes care, general treatment goals and guidelines, and tools to evaluate quality of care. Members of the ADA Professional Practice Committee, a multidisciplinary expert committee (https://doi.org/10.2337/dc20-SPPC), are responsible for updating the Standards of Care annually, or more frequently as warranted. For a detailed description of ADA standards, statements, and reports, as well as the evidence-grading system for ADA's clinical practice recommendations, please refer to the Standards of Care Introduction (https://doi.org/10.2337/dc20-SINT). Readers who wish to comment on the Standards of Care are invited to do so at professional.diabetes.org/SOC.},
	language = {eng},
	number = {Suppl 1},
	journal = {Diabetes Care},
	author = {{American Diabetes Association}},
	month = jan,
	year = {2020},
	pmid = {31862753},
	pages = {S111--S134},
	file = {Full Text:/home/imbroglio/Zotero/storage/N37UU2LT/American Diabetes Association - 2020 - 10. Cardiovascular Disease and Risk Management St.pdf:application/pdf},
}

@article{hayward_follow-up_2015,
	title = {Follow-up of {Glycemic} {Control} and {Cardiovascular} {Outcomes} in {Type} 2 {Diabetes}},
	volume = {372},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMoa1414266},
	doi = {10.1056/NEJMoa1414266},
	abstract = {BACKGROUND The Veterans Affairs Diabetes Trial previously showed that intensive glucose lowering, as compared with standard therapy, did not significantly reduce the rate of major cardiovascular events among 1791 military veterans (median follow-up, 5.6 years). We report the extended follow-up of the study participants.
METHODS After the conclusion of the clinical trial, we followed participants, using central databases to identify procedures, hospitalizations, and deaths (complete cohort, with follow-up data for 92.4\% of participants). Most participants agreed to additional data collection by means of annual surveys and periodic chart reviews (survey cohort, with 77.7\% follow-up). The primary outcome was the time to the first major cardiovascular event (heart attack, stroke, new or worsening congestive heart failure, amputation for ischemic gangrene, or cardiovascular-related death). Secondary outcomes were cardiovascular mortality and all-cause mortality.
RESULTS The difference in glycated hemoglobin levels between the intensive-therapy group and the standard-therapy group averaged 1.5 percentage points during the trial (median level, 6.9\% vs. 8.4\%) and declined to 0.2 to 0.3 percentage points by 3 years after the trial ended. Over a median follow-up of 9.8 years, the intensive-therapy group had a significantly lower risk of the primary outcome than did the standardtherapy group (hazard ratio, 0.83; 95\% confidence interval [CI], 0.70 to 0.99; P = 0.04), with an absolute reduction in risk of 8.6 major cardiovascular events per 1000 person-years, but did not have reduced cardiovascular mortality (hazard ratio, 0.88; 95\% CI, 0.64 to 1.20; P = 0.42). No reduction in total mortality was evident (hazard ratio in the intensive-therapy group, 1.05; 95\% CI, 0.89 to 1.25; P = 0.54; median follow-up, 11.8 years). From the Veterans Affairs (VA) Center for Clinical Management Research, VA Ann Arbor Healthcare System, Ann Arbor, MI (R.A.H., W.L.W.); Phoenix VA Health Care System, Phoenix, AZ (P.D.R., W.C.D.); and the Hines VA Cooperative Studies Program Coordinating Center and Edward Hines, Jr., VA Hospital (G.D.B., D.J.R., L.G., N.V.E.), and VA Pharmacy Benefits Management Services (M.M.) — all in Hines, IL. Address reprint requests to Dr. Hayward at the Robert Wood Johnson Foundation Clinical Scholars Program, 2800 Plymouth Rd., North Campus Research Complex, Bldg. 10, Rm. G016, Ann Arbor, MI 48019-2800, or at r­hayward@ ­umich.­edu. *A complete list of the investigators in the Veterans Affairs Diabetes Trial (VADT) is provided in the Supplementary Appendix, available at NEJM.org. N Engl J Med 2015;372:2197-206. DOI: 10.1056/NEJMoa1414266 Copyright © 2015 Massachusetts Medical Society.
CONCLUSIONS After nearly 10 years of follow-up, patients with type 2 diabetes who had been randomly assigned to intensive glucose control for 5.6 years had 8.6 fewer major cardiovascular events per 1000 person-years than those assigned to standard therapy, but no improvement was seen in the rate of overall survival. (Funded by the VA Cooperative Studies Program and others; VADT ClinicalTrials.gov number, NCT00032487.)},
	language = {en},
	number = {23},
	urldate = {2020-08-11},
	journal = {New England Journal of Medicine},
	author = {Hayward, Rodney A. and Reaven, Peter D. and Wiitala, Wyndy L. and Bahn, Gideon D. and Reda, Domenic J. and Ge, Ling and McCarren, Madeline and Duckworth, William C. and Emanuele, Nicholas V.},
	month = jun,
	year = {2015},
	pages = {2197--2206},
	file = {Hayward et al. - 2015 - Follow-up of Glycemic Control and Cardiovascular O.pdf:/home/imbroglio/Zotero/storage/EZMKMXU3/Hayward et al. - 2015 - Follow-up of Glycemic Control and Cardiovascular O.pdf:application/pdf},
}

@article{duckworth_glucose_2009,
	title = {Glucose {Control} and {Vascular} {Complications} in {Veterans} with {Type} 2 {Diabetes}},
	volume = {360},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/abs/10.1056/NEJMoa0808431},
	doi = {10.1056/NEJMoa0808431},
	abstract = {Background The effects of intensive glucose control on cardiovascular events in patients with longstanding type 2 diabetes mellitus remain uncertain.
Methods We randomly assigned 1791 military veterans (mean age, 60.4 years) who had a suboptimal response to therapy for type 2 diabetes to receive either intensive or standard glucose control. Other cardiovascular risk factors were treated uniformly. The mean number of years since the diagnosis of diabetes was 11.5, and 40\% of the patients had already had a cardiovascular event. The goal in the intensive-therapy group was an absolute reduction of 1.5 percentage points in the glycated hemoglobin level, as compared with the standard-therapy group. The primary outcome was the time from randomization to the first occurrence of a major cardiovascular event, a composite of myocardial infarction, stroke, death from cardiovascular causes, congestive heart failure, surgery for vascular disease, inoperable coronary disease, and amputation for ischemic gangrene.
Results The median follow-up was 5.6 years. Median glycated hemoglobin levels were 8.4\% in the standard-therapy group and 6.9\% in the intensive-therapy group. The primary outcome occurred in 264 patients in the standard-therapy group and 235 patients in the intensive-therapy group (hazard ratio in the intensive-therapy group, 0.88; 95\% confidence interval [CI], 0.74 to 1.05; P = 0.14). There was no significant difference between the two groups in any component of the primary outcome or in the rate of death from any cause (hazard ratio, 1.07; 95\% CI, 0.81 to 1.42; P = 0.62). No differences between the two groups were observed for microvascular complications. The rates of adverse events, predominantly hypoglycemia, were 17.6\% in the standard-therapy group and 24.1\% in the intensive-therapy group. From the Phoenix Veterans Affairs (VA) Health Care Center, Phoenix, AZ (W.D., P.D.R.); Miami VA Medical Center, Miami (C.A., J.M.); Hines VA Cooperative Studies Program Coordinating Center (T.M., D.R., M.M., M.E.V., W.G.H.) and Hines VA Hospital (N.E.) — both in Hines, IL; Hunter Holmes McGuire VA Medical Center, Richmond, VA (F.J.Z.); Tennessee Valley Health Care System, Nashville (S.N.D.); VA Ann Arbor Healthcare System, Ann Arbor, MI (R.H.); VA Cooperative Studies Program Clinical Research Pharmacy Coordinating Center, Albuquerque, NM (S.R.W.); Southern Arizona VA Health Care System, Tucson (S.G.); and the Cooperative Studies Program Central Office, VA Office of Research and Development, Washington, DC (G.D.H.). Address reprint requests to Dr. Duckworth at the Phoenix VA Health Care System, 650 E. Indian School Rd., Phoenix, AZ 85012, or at william.duckworth@va.gov. *Investigators in the Veterans Affairs Diabetes Trial (VADT) are listed in the Appendix. This article (10.1056/NEJMoa0808431) was published at NEJM.org on December 17, 2008. N Engl J Med 2009;360. Copyright © 2009 Massachusetts Medical Society.
Conclusions Intensive glucose control in patients with poorly controlled type 2 diabetes had no significant effect on the rates of major cardiovascular events, death, or microvascular complications. (ClinicalTrials.gov number, NCT00032487.)},
	language = {en},
	number = {2},
	urldate = {2020-08-11},
	journal = {New England Journal of Medicine},
	author = {Duckworth, William and Abraira, Carlos and Moritz, Thomas and Reda, Domenic and Emanuele, Nicholas and Reaven, Peter D. and Zieve, Franklin J. and Marks, Jennifer and Davis, Stephen N. and Hayward, Rodney and Warren, Stuart R. and Goldman, Steven and McCarren, Madeline and Vitek, Mary Ellen and Henderson, William G. and Huang, Grant D.},
	month = jan,
	year = {2009},
	pages = {129--139},
	file = {Duckworth et al. - 2009 - Glucose Control and Vascular Complications in Vete.pdf:/home/imbroglio/Zotero/storage/996L6I8X/Duckworth et al. - 2009 - Glucose Control and Vascular Complications in Vete.pdf:application/pdf},
}

@article{noauthor_progression_1995,
	title = {Progression of {Retinopathy} with {Intensive} versus {Conventional} {Treatment} in the {Diabetes} {Control} and {Complications} {Trial}},
	volume = {102},
	issn = {01616420},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0161642095309736},
	doi = {10.1016/S0161-6420(95)30973-6},
	abstract = {Purpose: To answer the following questions regarding the effect of intensive diabetes management on retinopathy in insulin-dependent diabetes mellitus (lOOM): (1) Does intensive therapy completely prevent the development of retinopathy? (2) Are some states of retinopathy too advanced to benefit from intensive therapy? (3) Are the retinopathy endpoints in the Diabetes Control and Complications Trial (DCCT) clinically important? and (4) What other factors influence the effectiveness of therapy?
Methods: A total of 1441 patients, ranging in age from 13 and 39 years and with lOOM of 1 to 5 years' duration and no retinopathy at baseline (primary prevention cohort) or with 1 to 15 years' duration and minimal to moderate nonproliferative retinopathy (secondary intervention cohort), were assigned randomly to either intensive or conventional diabetes therapy. Intensive therapy, aimed at achieving glycemic levels as close to the normal range as possible, included three or more daily insulin injections or a continuous subcutaneous insulin infusion, guided by four or more glucose tests daily. Conventional therapy included one or two daily injections. Seven-field stereoscopic fundus photography was performed every 6 months, for a mean follow-up of 6.5 years (range, 4-9 years).
Results: Intensive therapy reduced the risk of any retinopathy (;::: 1 microaneurysm) developing in the primary prevention cohort (70\% of intensive versus 90\% of conventional treatment group; P = 0.002) by 27\%. It reduced the risk of retinopathy developing or progressing to clinically significant degrees by 34\% to 76\%. Intensive therapy was most effective when initiated early in the course of lOOM. It had a substantial beneficial effect over the entire spectrum of retinopathy studied in the DCCT and, with rare exceptions, in all patient subgroups.
Conclusion: Although intensive therapy does not prevent retinopathy completely, it has a beneficial effect that begins after 3 years of therapy on all levels of retinopathy studied in the DCCT. The reduction in risk observed in the study is translatable directly into reduced need for laser treatment and saved sight. Intensive therapy should form the backbone of any healthcare strategy aimed at reducing the risk of visual loss from diabetic retinopathy. Ophthalmology 1995;102:647-661},
	language = {en},
	number = {4},
	urldate = {2020-08-11},
	journal = {Ophthalmology},
	month = apr,
	year = {1995},
	pages = {647--661},
	file = {1995 - Progression of Retinopathy with Intensive versus C.pdf:/home/imbroglio/Zotero/storage/LSTCIHCI/1995 - Progression of Retinopathy with Intensive versus C.pdf:application/pdf},
}

@article{bellera_variables_2010,
	title = {Variables with time-varying effects and the {Cox} model: {Some} statistical concepts illustrated with a prognostic factor study in breast cancer},
	volume = {10},
	issn = {1471-2288},
	shorttitle = {Variables with time-varying effects and the {Cox} model},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-10-20},
	doi = {10.1186/1471-2288-10-20},
	abstract = {Background: The Cox model relies on the proportional hazards (PH) assumption, implying that the factors investigated have a constant impact on the hazard - or risk - over time. We emphasize the importance of this assumption and the misleading conclusions that can be inferred if it is violated; this is particularly essential in the presence of long follow-ups.
Methods: We illustrate our discussion by analyzing prognostic factors of metastases in 979 women treated for breast cancer with surgery. Age, tumour size and grade, lymph node involvement, peritumoral vascular invasion (PVI), status of hormone receptors (HRec), Her2, and Mib1 were considered.
Results: Median follow-up was 14 years; 264 women developed metastases. The conventional Cox model suggested that all factors but HRec, Her2, and Mib1 status were strong prognostic factors of metastases. Additional tests indicated that the PH assumption was not satisfied for some variables of the model. Tumour grade had a significant time-varying effect, but although its effect diminished over time, it remained strong. Interestingly, while the conventional Cox model did not show any significant effect of the HRec status, tests provided strong evidence that this variable had a non-constant effect over time. Negative HRec status increased the risk of metastases early but became protective thereafter. This reversal of effect may explain non-significant hazard ratios provided by previous conventional Cox analyses in studies with long follow-ups.
Conclusions: Investigating time-varying effects should be an integral part of Cox survival analyses. Detecting and accounting for time-varying effects provide insights on some specific time patterns, and on valuable biological information that could be missed otherwise.},
	language = {en},
	number = {1},
	urldate = {2020-08-11},
	journal = {BMC Medical Research Methodology},
	author = {Bellera, Carine A and MacGrogan, Gaëtan and Debled, Marc and de Lara, Christine Tunon and Brouste, Véronique and Mathoulin-Pélissier, Simone},
	month = dec,
	year = {2010},
	pages = {20},
	file = {Bellera et al. - 2010 - Variables with time-varying effects and the Cox mo.pdf:/home/imbroglio/Zotero/storage/XNE8UMYS/Bellera et al. - 2010 - Variables with time-varying effects and the Cox mo.pdf:application/pdf},
}

@article{noauthor_intensive_2008,
	title = {Intensive {Blood} {Glucose} {Control} and {Vascular} {Outcomes} in {Patients} with {Type} 2 {Diabetes}},
	volume = {358},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/abs/10.1056/NEJMoa0802987},
	doi = {10.1056/NEJMoa0802987},
	abstract = {Background In patients with type 2 diabetes, the effects of intensive glucose control on vascular outcomes remain uncertain.
Methods We randomly assigned 11,140 patients with type 2 diabetes to undergo either standard glucose control or intensive glucose control, defined as the use of gliclazide (modified release) plus other drugs as required to achieve a glycated hemoglobin value of 6.5\% or less. Primary end points were composites of major macrovascular events (death from cardiovascular causes, nonfatal myocardial infarction, or nonfatal stroke) and major microvascular events (new or worsening nephropathy or retinopathy), assessed both jointly and separately.
Results After a median of 5 years of follow-up, the mean glycated hemoglobin level was lower in the intensive-control group (6.5\%) than in the standard-control group (7.3\%). Intensive control reduced the incidence of combined major macrovascular and microvascular events (18.1\%, vs. 20.0\% with standard control; hazard ratio, 0.90; 95\% confidence interval [CI], 0.82 to 0.98; P = 0.01), as well as that of major microvascular events (9.4\% vs. 10.9\%; hazard ratio, 0.86; 95\% CI, 0.77 to 0.97; P = 0.01), primarily because of a reduction in the incidence of nephropathy (4.1\% vs. 5.2\%; hazard ratio, 0.79; 95\% CI, 0.66 to 0.93; P = 0.006), with no significant effect on retinopathy (P = 0.50). There were no significant effects of the type of glucose control on major macrovascular events (hazard ratio with intensive control, 0.94; 95\% CI, 0.84 to 1.06; P = 0.32), death from cardiovascular causes (hazard ratio with intensive control, 0.88; 95\% CI, 0.74 to 1.04; P = 0.12), or death from any cause (hazard ratio with intensive control, 0.93; 95\% CI, 0.83 to 1.06; P = 0.28). Severe hypoglycemia, although uncommon, was more common in the intensive-control group (2.7\%, vs. 1.5\% in the standard-control group; hazard ratio, 1.86; 95\% CI, 1.42 to 2.40; P{\textless}0.001).
Conclusions A strategy of intensive glucose control, involving gliclazide (modified release) and other drugs as required, that lowered the glycated hemoglobin value to 6.5\% yielded a 10\% relative reduction in the combined outcome of major macrovascular and microvascular events, primarily as a consequence of a 21\% relative reduction in nephropathy. (ClinicalTrials.gov number, NCT00145925.)},
	language = {en},
	number = {24},
	urldate = {2020-08-11},
	journal = {New England Journal of Medicine},
	month = jun,
	year = {2008},
	pages = {2560--2572},
	file = {2008 - Intensive Blood Glucose Control and Vascular Outco.pdf:/home/imbroglio/Zotero/storage/8IAUZYWH/2008 - Intensive Blood Glucose Control and Vascular Outco.pdf:application/pdf},
}

@article{noauthor_long-term_2011,
	title = {Long-{Term} {Effects} of {Intensive} {Glucose} {Lowering} on {Cardiovascular} {Outcomes}},
	abstract = {Background Intensive glucose lowering has previously been shown to increase mortality among persons with advanced type 2 diabetes and a high risk of cardiovascular disease. This report describes the 5-year outcomes of a mean of 3.7 years of intensive glucose lowering on mortality and key cardiovascular events.
Methods We randomly assigned participants with type 2 diabetes and cardiovascular disease or additional cardiovascular risk factors to receive intensive therapy (targeting a glycated hemoglobin level below 6.0\%) or standard therapy (targeting a level of 7 to 7.9\%). After termination of the intensive therapy, due to higher mortality in the intensive-therapy group, the target glycated hemoglobin level was 7 to 7.9\% for all participants, who were followed until the planned end of the trial.
Results Before the intensive therapy was terminated, the intensive-therapy group did not differ significantly from the standard-therapy group in the rate of the primary outcome (a composite of nonfatal myocardial infarction, nonfatal stroke, or death from cardiovascular causes) (P = 0.13) but had more deaths from any cause (primarily cardiovascular) (hazard ratio, 1.21; 95\% confidence interval [CI], 1.02 to 1.44) and fewer nonfatal myocardial infarctions (hazard ratio, 0.79; 95\% CI, 0.66 to 0.95). These trends persisted during the entire follow-up period (hazard ratio for death, 1.19; 95\% CI, 1.03 to 1.38; and hazard ratio for nonfatal myocardial infarction, 0.82; 95\% CI, 0.70 to 0.96). After the intensive intervention was terminated, the median glycated hemoglobin level in the intensive-therapy group rose from 6.4\% to 7.2\%, and the use of glucose-lowering medications and rates of severe hypoglycemia and other adverse events were similar in the two groups.
Conclusions As compared with standard therapy, the use of intensive therapy for 3.7 years to target a glycated hemoglobin level below 6\% reduced 5-year nonfatal myocardial infarctions but increased 5-year mortality. Such a strategy cannot be recommended for high-risk patients with advanced type 2 diabetes. (Funded by the National Heart, Lung and Blood Institute; ClinicalTrials.gov number, NCT00000620.)},
	language = {en},
	journal = {n engl j med},
	year = {2011},
	pages = {11},
	file = {2011 - Long-Term Effects of Intensive Glucose Lowering on.pdf:/home/imbroglio/Zotero/storage/FK84MKSN/2011 - Long-Term Effects of Intensive Glucose Lowering on.pdf:application/pdf},
}

@article{noauthor_effects_2008,
	title = {Effects of {Intensive} {Glucose} {Lowering} in {Type} 2 {Diabetes}},
	volume = {358},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/abs/10.1056/NEJMoa0802743},
	doi = {10.1056/NEJMoa0802743},
	abstract = {Background Epidemiologic studies have shown a relationship between glycated hemoglobin levels and cardiovascular events in patients with type 2 diabetes. We investigated whether intensive therapy to target normal glycated hemoglobin levels would reduce cardiovascular events in patients with type 2 diabetes who had either established cardiovascular disease or additional cardiovascular risk factors.
Methods In this randomized study, 10,251 patients (mean age, 62.2 years) with a median glycated hemoglobin level of 8.1\% were assigned to receive intensive therapy (targeting a glycated hemoglobin level below 6.0\%) or standard therapy (targeting a level from 7.0 to 7.9\%). Of these patients, 38\% were women, and 35\% had had a previous cardiovascular event. The primary outcome was a composite of nonfatal myocardial infarction, nonfatal stroke, or death from cardiovascular causes. The finding of higher mortality in the intensive-therapy group led to a discontinuation of intensive therapy after a mean of 3.5 years of follow-up.
Results At 1 year, stable median glycated hemoglobin levels of 6.4\% and 7.5\% were achieved in the intensive-therapy group and the standard-therapy group, respectively. During follow-up, the primary outcome occurred in 352 patients in the intensive-therapy group, as compared with 371 in the standard-therapy group (hazard ratio, 0.90; 95\% confidence interval [CI], 0.78 to 1.04; P = 0.16). At the same time, 257 patients in the intensive-therapy group died, as compared with 203 patients in the standardtherapy group (hazard ratio, 1.22; 95\% CI, 1.01 to 1.46; P = 0.04). Hypoglycemia requiring assistance and weight gain of more than 10 kg were more frequent in the intensive-therapy group (P{\textless}0.001).
Conclusions As compared with standard therapy, the use of intensive therapy to target normal glycated hemoglobin levels for 3.5 years increased mortality and did not significantly reduce major cardiovascular events. These findings identify a previously unrecognized harm of intensive glucose lowering in high-risk patients with type 2 diabetes. (ClinicalTrials.gov number, NCT00000620.)},
	language = {en},
	number = {24},
	urldate = {2020-08-11},
	journal = {New England Journal of Medicine},
	month = jun,
	year = {2008},
	pages = {2545--2559},
	file = {2008 - Effects of Intensive Glucose Lowering in Type 2 Di.pdf:/home/imbroglio/Zotero/storage/9WY6U6EW/2008 - Effects of Intensive Glucose Lowering in Type 2 Di.pdf:application/pdf},
}

@article{stone_five-year_2019,
	title = {Five-{Year} {Outcomes} after {PCI} or {CABG} for {Left} {Main} {Coronary} {Disease}},
	volume = {381},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMoa1909406},
	doi = {10.1056/NEJMoa1909406},
	abstract = {BACKGROUND Long-term outcomes after percutaneous coronary intervention (PCI) with contemporary drug-eluting stents, as compared with coronary-artery bypass grafting (CABG), in patients with left main coronary artery disease are not clearly established.
METHODS We randomly assigned 1905 patients with left main coronary artery disease of low or intermediate anatomical complexity (according to assessment at the participating centers) to undergo either PCI with fluoropolymer-based cobalt–chromium everolimus-eluting stents (PCI group, 948 patients) or CABG (CABG group, 957 patients). The primary outcome was a composite of death, stroke, or myocardial infarction.
RESULTS At 5 years, a primary outcome event had occurred in 22.0\% of the patients in the PCI group and in 19.2\% of the patients in the CABG group (difference, 2.8 percentage points; 95\% confidence interval [CI], −0.9 to 6.5; P = 0.13). Death from any cause occurred more frequently in the PCI group than in the CABG group (in 13.0\% vs. 9.9\%; difference, 3.1 percentage points; 95\% CI, 0.2 to 6.1). In the PCI and CABG groups, the incidences of definite cardiovascular death (5.0\% and 4.5\%, respectively; difference, 0.5 percentage points; 95\% CI, −1.4 to 2.5) and myocardial infarction (10.6\% and 9.1\%; difference, 1.4 percentage points; 95\% CI, −1.3 to 4.2) were not significantly different. All cerebrovascular events were less frequent after PCI than after CABG (3.3\% vs. 5.2\%; difference, −1.9 percentage points; 95\% CI, −3.8 to 0), although the incidence of stroke was not significantly different between the two groups (2.9\% and 3.7\%; difference, −0.8 percentage points; 95\% CI, −2.4 to 0.9). Ischemia-driven revascularization was more frequent after PCI than after CABG (16.9\% vs. 10.0\%; difference, 6.9 percentage points; 95\% CI, 3.7 to 10.0).
CONCLUSIONS In patients with left main coronary artery disease of low or intermediate anatomical complexity, there was no significant difference between PCI and CABG with respect to the rate of the composite outcome of death, stroke, or myocardial infarction at 5 years. (Funded by Abbott Vascular; EXCEL ClinicalTrials.gov number, NCT01205776.)},
	language = {en},
	number = {19},
	urldate = {2020-08-11},
	journal = {New England Journal of Medicine},
	author = {Stone, Gregg W. and Kappetein, A. Pieter and Sabik, Joseph F. and Pocock, Stuart J. and Morice, Marie-Claude and Puskas, John and Kandzari, David E. and Karmpaliotis, Dimitri and Brown, W. Morris and Lembo, Nicholas J. and Banning, Adrian and Merkely, Béla and Horkay, Ferenc and Boonstra, Piet W. and van Boven, Ad J. and Ungi, Imre and Bogáts, Gabor and Mansour, Samer and Noiseux, Nicolas and Sabaté, Manel and Pomar, Jose and Hickey, Mark and Gershlick, Anthony and Buszman, Pawel E. and Bochenek, Andrzej and Schampaert, Erick and Pagé, Pierre and Modolo, Rodrigo and Gregson, John and Simonton, Charles A. and Mehran, Roxana and Kosmidou, Ioanna and Généreux, Philippe and Crowley, Aaron and Dressler, Ovidiu and Serruys, Patrick W.},
	month = nov,
	year = {2019},
	pages = {1820--1830},
	file = {Stone et al. - 2019 - Five-Year Outcomes after PCI or CABG for Left Main.pdf:/home/imbroglio/Zotero/storage/PFJC4YGU/Stone et al. - 2019 - Five-Year Outcomes after PCI or CABG for Left Main.pdf:application/pdf},
}

@article{reaven_intensive_2019,
	title = {Intensive {Glucose} {Control} in {Patients} with {Type} 2 {Diabetes} — 15-{Year} {Follow}-up},
	volume = {380},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMoa1806802},
	doi = {10.1056/NEJMoa1806802},
	abstract = {BACKGROUND We previously reported that a median of 5.6 years of intensive as compared with standard glucose lowering in 1791 military veterans with type 2 diabetes resulted in a risk of major cardiovascular events that was significantly lower (by 17\%) after a total of 10 years of combined intervention and observational follow-up. We now report the full 15-year follow-up.
METHODS We observationally followed enrolled participants (complete cohort) after the conclusion of the original clinical trial by using central databases to identify cardiovascular events, hospitalizations, and deaths. Participants were asked whether they would be willing to provide additional data by means of surveys and chart reviews (survey cohort). The prespecified primary outcome was a composite of major cardiovascular events, including nonfatal myocardial infarction, nonfatal stroke, new or worsening congestive heart failure, amputation for ischemic gangrene, and death from cardiovascular causes. Death from any cause was a prespecified secondary outcome.
RESULTS There were 1655 participants in the complete cohort and 1391 in the survey cohort. During the trial (which originally enrolled 1791 participants), the separation of the glycated hemoglobin curves between the intensive-therapy group (892 participants) and the standard-therapy group (899 participants) averaged 1.5 percentage points, and this difference declined to 0.2 to 0.3 percentage points by 3 years after the trial ended. Over a period of 15 years of follow-up (active treatment plus post-trial observation), the risks of major cardiovascular events or death were not lower in the intensive-therapy group than in the standard-therapy group (hazard ratio for primary outcome, 0.91; 95\% confidence interval [CI], 0.78 to 1.06; P = 0.23; hazard ratio for death, 1.02; 95\% CI, 0.88 to 1.18). The risk of major cardiovascular disease outcomes was reduced, however, during an extended interval of separation of the glycated hemoglobin curves (hazard ratio, 0.83; 95\% CI, 0.70 to 0.99), but this benefit did not continue after equalization of the glycated hemoglobin levels (hazard ratio, 1.26; 95\% CI, 0.90 to 1.75). From the Phoenix Veterans Affairs (VA) Health Care System, Phoenix (P.D.R., W.C.D.); the Hines VA Cooperative Studies Program Coordinating Center and Hines VA Hospital (N.V.E., G.D.B., D.J.R.) and the VA Pharmacy Benefits Management Services (M.M.), Hines, IL; and the VA Center for Clinical Management Research, VA Ann Arbor Healthcare System, Ann Arbor, MI (W.L.W., R.A.H.). Address reprint requests to Dr. Reaven at the Phoenix VA Health Care System, 650 E. Indian School Rd., Phoenix, AZ 85012, or at ­peter.­reaven@­va.­gov. *A complete list of the investigators in the Veterans Affairs Diabetes Trial (VADT) is provided in the Supplementary Appendix, available at NEJM.org. N Engl J Med 2019;380:2215-24. DOI: 10.1056/NEJMoa1806802 Copyright © 2019 Massachusetts Medical Society.
CONCLUSIONS Participants with type 2 diabetes who had been randomly assigned to intensive glucose control for 5.6 years had a lower risk of cardiovascular events than those who received standard therapy only during the prolonged period in which the glycated hemoglobin curves were separated. There was no evidence of a legacy effect or a mortality benefit with intensive glucose control. (Funded by the VA Cooperative Studies Program; VADT ClinicalTrials.gov number, NCT00032487.)},
	language = {en},
	number = {23},
	urldate = {2020-08-11},
	journal = {New England Journal of Medicine},
	author = {Reaven, Peter D. and Emanuele, Nicholas V. and Wiitala, Wyndy L. and Bahn, Gideon D. and Reda, Domenic J. and McCarren, Madeline and Duckworth, William C. and Hayward, Rodney A.},
	month = jun,
	year = {2019},
	pages = {2215--2224},
	file = {Reaven et al. - 2019 - Intensive Glucose Control in Patients with Type 2 .pdf:/home/imbroglio/Zotero/storage/WYI6X6DP/VADT 15-yr FU Reaven NEJM 2019.pdf:application/pdf},
}

@article{tsiatis_comment_2007,
	title = {Comment: {Demystifying} {Double} {Robustness}: {A} {Comparison} of {Alternative} {Strategies} for {Estimating} a {Population} {Mean} from {Incomplete} {Data}},
	volume = {22},
	issn = {0883-4237},
	shorttitle = {Comment},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2397555/},
	doi = {10.1214/07-STS227},
	number = {4},
	urldate = {2020-02-26},
	journal = {Statistical science : a review journal of the Institute of Mathematical Statistics},
	author = {Tsiatis, Anastasios A. and Davidian, Marie},
	year = {2007},
	pmid = {18516239},
	pmcid = {PMC2397555},
	pages = {569--573},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/RKPSW42M/Tsiatis and Davidian - 2007 - Comment Demystifying Double Robustness A Compari.pdf:application/pdf},
}

@article{kang_demystifying_2007,
	title = {Demystifying {Double} {Robustness}: {A} {Comparison} of {Alternative} {Strategies} for {Estimating} a {Population} {Mean} from {Incomplete} {Data}},
	volume = {22},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Demystifying {Double} {Robustness}},
	url = {https://projecteuclid.org/euclid.ss/1207580167},
	doi = {10.1214/07-STS227},
	abstract = {When outcomes are missing for reasons beyond an investigator’s control, there are two different ways to adjust a parameter estimate for covariates that may be related both to the outcome and to missingness. One approach is to model the relationships between the covariates and the outcome and use those relationships to predict the missing values. Another is to model the probabilities of missingness given the covariates and incorporate them into a weighted or stratified estimate. Doubly robust (DR) procedures apply both types of model simultaneously and produce a consistent estimate of the parameter if either of the two models has been correctly specified. In this article, we show that DR estimates can be constructed in many ways. We compare the performance of various DR and non-DR estimates of a population mean in a simulated example where both models are incorrect but neither is grossly misspecified. Methods that use inverse-probabilities as weights, whether they are DR or not, are sensitive to misspecification of the propensity model when some estimated propensities are small. Many DR methods perform better than simple inverse-probability weighting. None of the DR methods we tried, however, improved upon the performance of simple regression-based prediction of the missing values. This study does not represent every missing-data problem that will arise in practice. But it does demonstrate that, in at least some settings, two wrong models are not better than one.},
	language = {EN},
	number = {4},
	urldate = {2020-02-26},
	journal = {Statistical Science},
	author = {Kang, Joseph D. Y. and Schafer, Joseph L.},
	month = nov,
	year = {2007},
	mrnumber = {MR2420458},
	zmnumber = {1246.62073},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Causal inference, missing data, model-assisted survey estimation, propensity score, weighted estimating equations},
	pages = {523--539},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/638R8M2K/Kang and Schafer - 2007 - Demystifying Double Robustness A Comparison of Al.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/J5UKF7W9/1207580167.html:text/html},
}

@misc{noauthor_kang_nodate,
	title = {Kang , {Schafer} : {Demystifying} {Double} {Robustness}: {A} {Comparison} of {Alternative} {Strategies} for {Estimating} a {Population} {Mean} from {Incomplete} {Data}},
	url = {https://projecteuclid.org/euclid.ss/1207580167},
	urldate = {2020-02-26},
	file = {Full Text:/home/imbroglio/Zotero/storage/TGVVNQNI/Kang , Schafer  Demystifying Double Robustness A.pdf:application/pdf},
}

@article{pirracchio_improving_2015,
	title = {Improving {Propensity} {Score} {Estimators}' {Robustness} to {Model} {Misspecification} {Using} {Super} {Learner}},
	volume = {181},
	issn = {0002-9262},
	url = {https://academic.oup.com/aje/article/181/2/108/2739158},
	doi = {10.1093/aje/kwu253},
	abstract = {The consistency of propensity score (PS) estimators relies on correct specification of the PS model. The PS is frequently estimated using main-effects logistic},
	language = {en},
	number = {2},
	urldate = {2020-02-25},
	journal = {American Journal of Epidemiology},
	author = {Pirracchio, Romain and Petersen, Maya L. and van der Laan, Mark},
	month = jan,
	year = {2015},
	pages = {108--119},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/FRSKEI7N/Pirracchio et al. - 2015 - Improving Propensity Score Estimators' Robustness .pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/IEC3TPU9/2739158.html:text/html},
}

@article{van_history-adjusted_2005,
	title = {History-{Adjusted} {Marginal} {Structural} {Models} and {Statically}-{Optimal} {Dynamic} {Treatment} {Regimens}},
	volume = {1},
	issn = {1557-4679},
	url = {https://www.degruyter.com/view/j/ijb.2005.1.1/ijb.2005.1.1.1003/ijb.2005.1.1.1003.xml},
	doi = {10.2202/1557-4679.1003},
	abstract = {Marginal structural models (MSM) provide a powerful tool for estimating the causal effect of a treatment. These models, introduced by Robins, model the marginal distributions of treatment-specific counterfactual outcomes, possibly conditional on a subset of the baseline covariates. Marginal structural models are particularly useful in the context of longitudinal data structures, in which each subject's treatment and covariate history are measured over time, and an outcome is recorded at a final time point. However, the utility of these models for some applications has been limited by their inability to incorporate modification of the causal effect of treatment by time-varying covariates. Particularly in the context of clinical decision making, such time-varying effect modifiers are often of considerable or even primary interest, as they are used in practice to guide treatment decisions for an individual. In this article we propose a generalization of marginal structural models, which we call history-adjusted marginal structural models (HA-MSM). These models allow estimation of adjusted causal effects of treatment, given the observed past, and are therefore more suitable for making treatment decisions at the individual level and for identification of time-dependent effect modifiers. Specifically, a HA-MSM models the conditional distribution of treatment-specific counterfactual outcomes, conditional on the whole or a subset of the observed past up till a time-point, simultaneously for all time-points. Double robust inverse probability of treatment weighted estimators have been developed and studied in detail for standard MSM. We extend these results by proposing a class of double robust inverse probability of treatment weighted estimators for the unknown parameters of the HA-MSM. In addition, we show that HA-MSM provide a natural approach to identifying the dynamic treatment regimen which follows, at each time-point, the history-adjusted (up till the most recent time point) optimal static treatment regimen. We illustrate our results using an example drawn from the treatment of HIV infection.},
	number = {1},
	urldate = {2020-02-23},
	journal = {The International Journal of Biostatistics},
	author = {van, der Laan Mark J. and Petersen, Maya L and Joffe, Marshall M},
	year = {2005},
	keywords = {antiretroviral resistance, antiretroviral therapy, causal inference, confounding, counterfactual, double robust estimation, dynamic treatment regimen, G-computation estimation, HIV, inverse probability of treatment weighted estimation, longitudinal data, optimal dynamic treatment regimen},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/I5FZYGRR/van et al. - 2005 - History-Adjusted Marginal Structural Models and St.pdf:application/pdf},
}

@article{sinisi_super_2007,
	title = {Super {Learning}: {An} {Application} to the {Prediction} of {HIV}-1 {Drug} {Resistance}},
	volume = {6},
	issn = {1544-6115},
	shorttitle = {Super {Learning}},
	url = {https://www.degruyter.com/view/j/sagmb.2007.6.issue-1/sagmb.2007.6.1.1240/sagmb.2007.6.1.1240.xml},
	doi = {10.2202/1544-6115.1240},
	abstract = {Many alternative data-adaptive algorithms can be used to learn a predictor based on observed data. Examples of such learners include decision trees, neural networks, support vector regression, least angle regression, logic regression, and the Deletion/Substitution/Addition algorithm. The optimal learner for prediction will vary depending on the underlying data-generating distribution. In this article we introduce the "super learner", a prediction algorithm that applies any set of candidate learners and uses cross-validation to select between them. Theory shows that asymptotically the super learner performs essentially as well as or better than any of the candidate learners. In this article we present the theory behind the super learner, and illustrate its performance using simulations. We further apply the super learner to a data example, in which we predict the phenotypic antiretroviral susceptibility of HIV based on viral genotype. Specifically, we apply the super learner to predict susceptibility to a specific protease inhibitor, nelfinavir, using a set of database-derived non-polymorphic treatment-selected mutations.},
	number = {1},
	urldate = {2020-02-23},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Sinisi, Sandra E. and Polley, Eric C and Petersen, Maya L and Rhee, Soo-Yon and van, der Laan Mark J.},
	year = {2007},
	keywords = {antiretroviral, cross-validation, genomics, loss-based estimation, machine learning},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/QBN86SUR/Sinisi et al. - 2007 - Super Learning An Application to the Prediction o.pdf:application/pdf},
}

@article{petersen_super_2015,
	title = {Super learner analysis of electronic adherence data improves viral prediction and may provide strategies for selective {HIV} {RNA} monitoring},
	volume = {69},
	issn = {1525-4135},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4421909/},
	doi = {10.1097/QAI.0000000000000548},
	abstract = {Objective
Regular HIV RNA testing for all HIV positive patients on antiretroviral therapy (ART) is expensive and has low yield since most tests are undetectable. Selective testing of those at higher risk of failure may improve efficiency. We investigated whether a novel analysis of adherence data could correctly classify virological failure and potentially inform a selective testing strategy.

Design
Multisite prospective cohort consortium.

Methods
We evaluated longitudinal data on 1478 adult patients treated with ART and monitored using the Medication Event Monitoring System (MEMS) in 16 United States cohorts contributing to the MACH14 consortium. Since the relationship between adherence and virological failure is complex and heterogeneous, we applied a machine-learning algorithm (Super Learner) to build a model for classifying failure and evaluated its performance using cross-validation.

Results
Application of the Super Learner algorithm to MEMS data, combined with data on CD4+ T cell counts and ART regimen, significantly improved classification of virological failure over a single MEMS adherence measure. Area under the ROC curve, evaluated on data not used in model fitting, was 0.78 (95\% CI: 0.75, 0.80) and 0.79 (95\% CI: 0.76, 0.81) for failure defined as single HIV RNA level {\textgreater}1000 copies/ml or {\textgreater}400 copies/ml, respectively. Our results suggest 25–31\% of viral load tests could be avoided while maintaining sensitivity for failure detection at or above 95\%, for a cost savings of \$16–\$29 per person-month.

Conclusions
Our findings provide initial proof-of-concept for the potential use of electronic medication adherence data to reduce costs through behavior-driven HIV RNA testing.},
	number = {1},
	urldate = {2020-02-23},
	journal = {Journal of acquired immune deficiency syndromes (1999)},
	author = {Petersen, Maya L. and LeDell, Erin and Schwab, Joshua and Sarovar, Varada and Gross, Robert and Reynolds, Nancy and Haberer, Jessica E. and Goggin, Kathy and Golin, Carol and Arnsten, Julia and Rosen, Marc and Remien, Robert and Etoori, David and Wilson, Ira and Simoni, Jane M. and Erlen, Judith A. and van der Laan, Mark J. and Liu, Honghu and Bangsberg, David R},
	month = may,
	year = {2015},
	pmid = {25942462},
	pmcid = {PMC4421909},
	pages = {109--118},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/DGA2BBX4/Petersen et al. - 2015 - Super learner analysis of electronic adherence dat.pdf:application/pdf},
}

@article{schuler_targeted_2017,
	title = {Targeted {Maximum} {Likelihood} {Estimation} for {Causal} {Inference} in {Observational} {Studies}},
	volume = {185},
	issn = {0002-9262},
	url = {https://academic.oup.com/aje/article/185/1/65/2662306},
	doi = {10.1093/aje/kww165},
	abstract = {Abstract.  Estimation of causal effects using observational data continues to grow in popularity in the epidemiologic literature. While many applications of cau},
	language = {en},
	number = {1},
	urldate = {2020-02-19},
	journal = {American Journal of Epidemiology},
	author = {Schuler, Megan S. and Rose, Sherri},
	month = jan,
	year = {2017},
	pages = {65--73},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/3QAFU7GN/Schuler and Rose - 2017 - Targeted Maximum Likelihood Estimation for Causal .pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/T2VAVUVE/2662306.html:text/html},
}

@book{american_statistical_association_1999_1999,
	address = {Alexandria, Va},
	series = {{PROCEEDINGS}- {SECTION} {ON} {BAYESIAN} {STATISTICAL} {SCIENCE} {AMERICAN} {STATISTICAL} {ASSOCIATION}},
	title = {1999 proceedings of the {Section} on {Bayesian} {Statistical} {Science}: papers presented at the annual meeting of the {American} {Statistical} {Association}, {Baltimore}, {Maryland}, {August} 8 - 12, 1999, under the sponsorship of the the {Section} on {Bayesian} {Statistical} {Science}},
	isbn = {978-1-883276-85-0},
	shorttitle = {1999 proceedings of the {Section} on {Bayesian} {Statistical} {Science}},
	language = {eng},
	publisher = {ASA},
	editor = {American Statistical Association},
	year = {1999},
	note = {OCLC: 247713362},
	annote = {Literaturangaben},
	file = {Robust Estimation in Sequentially Ignorable Missing Data - Technische Informationsbibliothek (TIB):/home/imbroglio/Zotero/storage/QBCCPTCS/Robust-Estimation-in-Sequentially-Ignorable-Missing.html:text/html},
}

@misc{noauthor_robust_nodate,
	title = {Robust {Estimation} in {Sequentially} {Ignorable} {Missing} {Data} - {Technische} {Informationsbibliothek} ({TIB})},
	url = {https://www.tib.eu/en/search/id/BLCP%3ACN039001731/Robust-Estimation-in-Sequentially-Ignorable-Missing/},
	urldate = {2020-02-14},
	file = {Robust Estimation in Sequentially Ignorable Missing Data - Technische Informationsbibliothek (TIB):/home/imbroglio/Zotero/storage/DVFXAYAW/Robust-Estimation-in-Sequentially-Ignorable-Missing.html:text/html},
}

@article{bang_doubly_2005,
	title = {Doubly {Robust} {Estimation} in {Missing} {Data} and {Causal} {Inference} {Models}},
	volume = {61},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2005.00377.x},
	doi = {10.1111/j.1541-0420.2005.00377.x},
	abstract = {The goal of this article is to construct doubly robust (DR) estimators in ignorable missing data and causal inference models. In a missing data model, an estimator is DR if it remains consistent when either (but not necessarily both) a model for the missingness mechanism or a model for the distribution of the complete data is correctly specified. Because with observational data one can never be sure that either a missingness model or a complete data model is correct, perhaps the best that can be hoped for is to find a DR estimator. DR estimators, in contrast to standard likelihood-based or (nonaugmented) inverse probability-weighted estimators, give the analyst two chances, instead of only one, to make a valid inference. In a causal inference model, an estimator is DR if it remains consistent when either a model for the treatment assignment mechanism or a model for the distribution of the counterfactual data is correctly specified. Because with observational data one can never be sure that a model for the treatment assignment mechanism or a model for the counterfactual data is correct, inference based on DR estimators should improve upon previous approaches. Indeed, we present the results of simulation studies which demonstrate that the finite sample performance of DR estimators is as impressive as theory would predict. The proposed method is applied to a cardiovascular clinical trial.},
	language = {en},
	number = {4},
	urldate = {2020-02-14},
	journal = {Biometrics},
	author = {Bang, Heejung and Robins, James M.},
	year = {2005},
	keywords = {Causal inference, Doubly robust estimation, Longitudinal data, Marginal structural model, Missing data, Semiparametrics},
	pages = {962--973},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/AAAI37QE/Bang and Robins - 2005 - Doubly Robust Estimation in Missing Data and Causa.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/QW25JGLM/j.1541-0420.2005.00377.html:text/html},
}

@article{scharfstein_adjusting_1999,
	title = {Adjusting for {Nonignorable} {Drop}-{Out} {Using} {Semiparametric} {Nonresponse} {Models}},
	volume = {94},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473862},
	doi = {10.1080/01621459.1999.10473862},
	abstract = {Consider a study whose design calls for the study subjects to be followed from enrollment (time t = 0) to time t = T, at which point a primary endpoint of interest Y is to be measured. The design of the study also calls for measurements on a vector V t) of covariates to be made at one or more times t during the interval [0, T). We are interested in making inferences about the marginal mean μ0 of Y when some subjects drop out of the study at random times Q prior to the common fixed end of follow-up time T. The purpose of this article is to show how to make inferences about μ0 when the continuous drop-out time Q is modeled semiparametrically and no restrictions are placed on the joint distribution of the outcome and other measured variables. In particular, we consider two models for the conditional hazard of drop-out given (V(T), Y), where V(t) denotes the history of the process V t) through time t, t ∈ [0, T). In the first model, we assume that λQ(t{\textbar}V(T), Y) exp(α0 Y), where α0 is a scalar parameter and λ0(t{\textbar}V(t)) is an unrestricted positive function of t and the process V(t). When the process Vt) is high dimensional, estimation in this model is not feasible with moderate sample sizes, due to the curse of dimensionality. For such situations, we consider a second model that imposes the additional restriction that λ0(t{\textbar}V(t)) = λ0(t) exp(γ′0(t)), where λ0 t) is an unspecified baseline hazard function, W(t) = w(t, V(t)), w(·,·) is a known function that maps (t, V(t)) to Rq , and γ0 is a q × 1 unknown parameter vector. When α0 ≠ 0, then drop-out is nonignorable. On account of identifiability problems, joint estimation of the mean μ0 of Y and the selection bias parameter α0 may be difficult or impossible. Therefore, we propose regarding the selection bias parameter α0 as known, rather than estimating it from the data. We then perform a sensitivity analysis to see how inference about α0 changes as we vary α0 over a plausible range of values. We apply our approach to the analysis of ACTG 175, an AIDS clinical trial.},
	number = {448},
	urldate = {2020-02-14},
	journal = {Journal of the American Statistical Association},
	author = {Scharfstein, Daniel O. and Rotnitzky, Andrea and Robins, James M.},
	month = dec,
	year = {1999},
	keywords = {Augmented inverse probability of censoring weighted estimators, Cox proportional hazards model, Identification; Missing data, Noncompliance; Nonparametric methods, Randomized trials, Sensitivity analysis, Time-dependent covariates.},
	pages = {1096--1120},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/MHLY2JX9/Scharfstein et al. - 1999 - Adjusting for Nonignorable Drop-Out Using Semipara.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/7QISYE6L/01621459.1999.html:text/html},
}

@article{van_targeted_2012,
	title = {Targeted {Minimum} {Loss} {Based} {Estimation} of {Causal} {Effects} of {Multiple} {Time} {Point} {Interventions}},
	volume = {8},
	issn = {1557-4679},
	url = {https://www.degruyter.com/view/j/ijb.2012.8.issue-1/1557-4679.1370/1557-4679.1370.xml},
	doi = {10.1515/1557-4679.1370},
	abstract = {We consider estimation of the effect of a multiple time point intervention on an outcome of interest, where the intervention nodes are subject to time-dependent confounding by intermediate covariates.In previous work van der Laan (2010) and Stitelman and van der Laan (2011a) developed and implemented a closed form targeted maximum likelihood estimator (TMLE) relying on the log-likelihood loss function, and demonstrated important gains relative to inverse probability of treatment weighted estimators and estimating equation based estimators. This TMLE relies on an initial estimator of the entire probability distribution of the longitudinal data structure. To enhance the finite sample performance of the TMLE of the target parameter it is of interest to select the smallest possible relevant part of the data generating distribution, which is estimated and updated by TMLE. Inspired by this goal, we develop a new closed form TMLE of an intervention specific mean outcome based on general longitudinal data structures. The target parameter is represented as an iterative sequence of conditional expectations of the outcome of interest. This collection of conditional means represents the relevant part, which is estimated and updated using the general TMLE algorithm. We also develop this new TMLE for other causal parameters, such as parameters defined by working marginal structural models. The theoretical properties of the TMLE are also practically demonstrated with a small scale simulation study.The proposed TMLE is building upon a previously proposed estimator Bang and Robins (2005) by integrating some of its key and innovative ideas into the TMLE framework.},
	number = {1},
	urldate = {2020-02-14},
	journal = {The International Journal of Biostatistics},
	author = {van, der Laan Mark J. and Gruber, Susan},
	year = {2012},
	keywords = {confounding, longitudinal data, Asymptotic linearity of an estimator, causal effect, efficient influence curve, G-computation formula, influence curve, loss function, marginal structural working model, nonparametric structural equation model, positivity assumption, randomization assumption, semiparametric statistical model, targeted maximum likelihood estimation, targeted minimum loss based estimation, TMLE, treatment regimen},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/2N2GLHTZ/van and Gruber - 2012 - Targeted Minimum Loss Based Estimation of Causal E.pdf:application/pdf},
}

@article{van_asymptotic_2004,
	title = {Asymptotic {Optimality} of {Likelihood}-{Based} {Cross}-{Validation}},
	volume = {3},
	issn = {1544-6115},
	url = {https://www.degruyter.com/view/j/sagmb.2004.3.issue-1/sagmb.2004.3.1.1036/sagmb.2004.3.1.1036.xml},
	doi = {10.2202/1544-6115.1036},
	abstract = {Likelihood-based cross-validation is a statistical tool for selecting a density estimate based on n i.i.d. observations from the true density among a collection of candidate density estimators. General examples are the selection of a model indexing a maximum likelihood estimator, and the selection of a bandwidth indexing a nonparametric (e.g. kernel) density estimator. In this article, we establish a finite sample result for a general class of likelihood-based cross-validation procedures (as indexed by the type of sample splitting used, e.g. V-fold cross-validation). This result implies that the cross-validation selector performs asymptotically as well (w.r.t. to the Kullback-Leibler distance to the true density) as a benchmark model selector which is optimal for each given dataset and depends on the true density. Crucial conditions of our theorem are that the size of the validation sample converges to infinity, which excludes leave-one-out cross-validation, and that the candidate density estimates are bounded away from zero and infinity. We illustrate these asymptotic results and the practical performance of likelihood-based cross-validation for the purpose of bandwidth selection with a simulation study. Moreover, we use likelihood-based cross-validation in the context of regulatory motif detection in DNA sequences.},
	number = {1},
	urldate = {2020-02-14},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {van, der Laan Mark J. and Dudoit, Sandrine and Keles, Sunduz},
	year = {2004},
	keywords = {bandwidth selection, density estimation, Kullback-Leibler divergence, Likelihood cross-validation, maximum likelihood estimation, model selection, variable selection},
	pages = {1--23},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/UNP5N44M/van et al. - 2004 - Asymptotic Optimality of Likelihood-Based Cross-Va.pdf:application/pdf},
}

@inproceedings{benkeser_highly_2016,
	title = {The {Highly} {Adaptive} {Lasso} {Estimator}},
	doi = {10.1109/DSAA.2016.93},
	abstract = {Estimation of a regression functions is a common goal of statistical learning. We propose a novel nonparametric regression estimator that, in contrast to many existing methods, does not rely on local smoothness assumptions nor is it constructed using local smoothing techniques. Instead, our estimator respects global smoothness constraints by virtue of falling in a class of right-hand continuous functions with left-hand limits that have variation norm bounded by a constant. Using empirical process theory, we establish a fast minimal rate of convergence of our proposed estimator and illustrate how such an estimator can be constructed using standard software. In simulations, we show that the finite-sample performance of our estimator is competitive with other popular machine learning techniques across a variety of data generating mechanisms. We also illustrate competitive performance in real data examples using several publicly available data sets.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Benkeser, David and Van Der Laan, Mark},
	month = oct,
	year = {2016},
	note = {ISSN: null},
	keywords = {Convergence, finite-sample performance, highly adaptive lasso estimator, Kernel, lasso, learning (artificial intelligence), local smoothing techniques, machine learning techniques, Manganese, Maximum likelihood estimation, nonparametric regression, novel nonparametric regression estimator, prediction, regression analysis, regression functions, Standards, statistical learning},
	pages = {689--696},
	file = {IEEE Xplore Abstract Record:/home/imbroglio/Zotero/storage/2E8GWI5P/7796956.html:text/html;IEEE Xplore Full Text PDF:/home/imbroglio/Zotero/storage/W36YLWAB/Benkeser and Van Der Laan - 2016 - The Highly Adaptive Lasso Estimator.pdf:application/pdf},
}

@article{balzer_targeted_2016,
	title = {Targeted estimation and inference for the sample average treatment effect in trials with and without pair-matching},
	volume = {35},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6965},
	doi = {10.1002/sim.6965},
	abstract = {In cluster randomized trials, the study units usually are not a simple random sample from some clearly defined target population. Instead, the target population tends to be hypothetical or ill-defined, and the selection of study units tends to be systematic, driven by logistical and practical considerations. As a result, the population average treatment effect (PATE) may be neither well defined nor easily interpretable. In contrast, the sample average treatment effect (SATE) is the mean difference in the counterfactual outcomes for the study units. The sample parameter is easily interpretable and arguably the most relevant when the study units are not sampled from some specific super-population of interest. Furthermore, in most settings, the sample parameter will be estimated more efficiently than the population parameter. To the best of our knowledge, this is the first paper to propose using targeted maximum likelihood estimation (TMLE) for estimation and inference of the sample effect in trials with and without pair-matching. We study the asymptotic and finite sample properties of the TMLE for the sample effect and provide a conservative variance estimator. Finite sample simulations illustrate the potential gains in precision and power from selecting the sample effect as the target of inference. This work is motivated by the Sustainable East Africa Research in Community Health (SEARCH) study, a pair-matched, community randomized trial to estimate the effect of population-based HIV testing and streamlined ART on the 5-year cumulative HIV incidence (NCT01864603). The proposed methodology will be used in the primary analysis for the SEARCH trial. Copyright © 2016 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {21},
	urldate = {2020-02-11},
	journal = {Statistics in Medicine},
	author = {Balzer, Laura B. and Petersen, Maya L. and Laan, Mark J. van der},
	year = {2016},
	keywords = {cluster randomized trials, pair-matching, population average treatment effect (PATE), sample average treatment effect (SATE), targeted maximum likelihood estimation (TMLE)},
	pages = {3717--3732},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/WPTNMNJ7/Balzer et al. - 2016 - Targeted estimation and inference for the sample a.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/RTDM3ZXH/sim.html:text/html},
}

@article{cox_biometrics_1982,
	title = {A {Biometrics} {Invited} {Paper} with {Discussion}. {Some} {Aspects} of {Analysis} of {Covariance}},
	volume = {38},
	issn = {0006-341X},
	url = {https://www.jstor.org/stable/2530040},
	doi = {10.2307/2530040},
	abstract = {An account from first principles is given of a number of aspects of analysis of covariance. Six different meanings of analysis of covariance are outlined and the history of this technique is sketched briefly. The development of the key formulae from the method of least squares is described, and generalizations to other distributions in the exponential family are mentioned. Special problems of application in randomized experiments and in observational studies are discussed. Finally, the decomposition of regression relations is considered along with components of covariance.},
	number = {3},
	urldate = {2020-02-07},
	journal = {Biometrics},
	author = {Cox, D. R. and McCullagh, P.},
	year = {1982},
	pages = {541--561},
	file = {JSTOR Full Text PDF:/home/imbroglio/Zotero/storage/65DWVHEB/Cox and McCullagh - 1982 - A Biometrics Invited Paper with Discussion. Some A.pdf:application/pdf},
}

@article{leon_semiparametric_2003,
	title = {Semiparametric {Estimation} of {Treatment} {Effect} in a {Pretest}-{Posttest} {Study}},
	volume = {59},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2003.00120.x},
	doi = {10.1111/j.0006-341X.2003.00120.x},
	abstract = {Summary. Inference on treatment effects in a pretest-posttest study is a routine objective in medicine, public health, and other fields. A number of approaches have been advocated. We take a semiparametric perspective, making no assumptions about the distributions of baseline and posttest responses. By representing the situation in terms of counterfactual random variables, we exploit recent developments in the literature on missing data and causal inference, to derive the class of all consistent treatment effect estimators, identify the most efficient such estimator, and outline strategies for implementation of estimators that may improve on popular methods. We demonstrate the methods and their properties via simulation and by application to a data set from an HIV clinical trial.},
	language = {en},
	number = {4},
	urldate = {2020-02-07},
	journal = {Biometrics},
	author = {Leon, Selene and Tsiatis, Anastasios A. and Davidian, Marie},
	year = {2003},
	keywords = {Analysis of covariance, Counterfactuals, Influence function, Inverse probability weighting, Semiparametric model, t-test},
	pages = {1046--1055},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/3W974BA6/Leon et al. - 2003 - Semiparametric Estimation of Treatment Effect in a.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/WRBHHX8N/j.0006-341X.2003.00120.html:text/html},
}

@article{yang_efficiency_2001,
	title = {Efficiency {Study} of {Estimators} for a {Treatment} {Effect} in a {Pretest}–{Posttest} {Trial}},
	volume = {55},
	issn = {0003-1305},
	url = {https://doi.org/10.1198/000313001753272466},
	doi = {10.1198/000313001753272466},
	abstract = {Several possible methods used to evaluate treatment effects in a randomized pretest–posttest trial with two treatment groups are the two-sample t test, the paired t test, analysis of covariance I (ANCOVA I), the analysis of covariance II (ANCOVA II), and generalized estimating equations (GEE). The ANCOVA I includes treatment and baseline response as covariates in a linear model and ANCOVA II additionally includes an interaction term between the baseline response and treatment indicator as a covariate. The parameters in the ANCOVAI and ANCOVAII models are generally estimated using ordinary least squares. In this article, a semiparametric model, which makes no assumptions about the response distributions, is used. The asymptotic properties of the estimators derived from these five methods and their relative efficiencies are discussed under this semiparametric model. We show that all these methods yield consistent estimators for the treatment effect which have asymptotically normal distributions under the semiparametric model. The GEE and the ANCOVA II estimators are asymptotically equivalent and the most efficient. The estimators from other three methods are less efficient except under some special conditions which are outlined in the article.},
	number = {4},
	urldate = {2020-02-07},
	journal = {The American Statistician},
	author = {Yang, Li and Tsiatis, Anastasios A.},
	month = nov,
	year = {2001},
	keywords = {Analysis of covariance, Semiparametric model, GEE model, Paired t test, Two-sample t test},
	pages = {314--321},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/FFBEQMUD/Yang and Tsiatis - 2001 - Efficiency Study of Estimators for a Treatment Eff.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/AUIBTBEX/000313001753272466.html:text/html},
}

@article{hogben_biological_nodate,
	title = {{BIOLOGICAL} {MONOGRAPHS} {AND} {MANUALS}},
	language = {en},
	author = {Hogben, L T and Ponder, E},
	pages = {336},
	file = {Hogben and Ponder - BIOLOGICAL MONOGRAPHS AND MANUALS.pdf:/home/imbroglio/Zotero/storage/QSPFANXX/Hogben and Ponder - BIOLOGICAL MONOGRAPHS AND MANUALS.pdf:application/pdf},
}

@article{zhang_improving_2008,
	title = {Improving {Efficiency} of {Inferences} in {Randomized} {Clinical} {Trials} {Using} {Auxiliary} {Covariates}},
	volume = {64},
	copyright = {© 2008, The International Biometric Society},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00976.x},
	doi = {10.1111/j.1541-0420.2007.00976.x},
	abstract = {The primary goal of a randomized clinical trial is to make comparisons among two or more treatments. For example, in a two-arm trial with continuous response, the focus may be on the difference in treatment means; with more than two treatments, the comparison may be based on pairwise differences. With binary outcomes, pairwise odds ratios or log odds ratios may be used. In general, comparisons may be based on meaningful parameters in a relevant statistical model. Standard analyses for estimation and testing in this context typically are based on the data collected on response and treatment assignment only. In many trials, auxiliary baseline covariate information may also be available, and it is of interest to exploit these data to improve the efficiency of inferences. Taking a semiparametric theory perspective, we propose a broadly applicable approach to adjustment for auxiliary covariates to achieve more efficient estimators and tests for treatment parameters in the analysis of randomized clinical trials. Simulations and applications demonstrate the performance of the methods.},
	language = {en},
	number = {3},
	urldate = {2020-02-07},
	journal = {Biometrics},
	author = {Zhang, Min and Tsiatis, Anastasios A. and Davidian, Marie},
	year = {2008},
	keywords = {Longitudinal data, Covariate adjustment, Hypothesis test, k-arm trial, Kruskal–Wallis test, Log odds ratio, Semiparametric theory},
	pages = {707--715},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/BWNZEDUK/Zhang et al. - 2008 - Improving Efficiency of Inferences in Randomized C.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/SDDG7YCC/j.1541-0420.2007.00976.html:text/html},
}

@article{tsiatis_covariate_2008,
	title = {Covariate adjustment for two-sample treatment comparisons in randomized clinical trials: {A} principled yet flexible approach},
	volume = {27},
	issn = {1097-0258},
	shorttitle = {Covariate adjustment for two-sample treatment comparisons in randomized clinical trials},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3113},
	doi = {10.1002/sim.3113},
	abstract = {There is considerable debate regarding whether and how covariate-adjusted analyses should be used in the comparison of treatments in randomized clinical trials. Substantial baseline covariate information is routinely collected in such trials, and one goal of adjustment is to exploit covariates associated with outcome to increase precision of estimation of the treatment effect. However, concerns are routinely raised over the potential for bias when the covariates used are selected post hoc and the potential for adjustment based on a model of the relationship between outcome, covariates, and treatment to invite a ‘fishing expedition’ for that leading to the most dramatic effect estimate. By appealing to the theory of semiparametrics, we are led naturally to a characterization of all treatment effect estimators and to principled, practically feasible methods for covariate adjustment that yield the desired gains in efficiency and that allow covariate relationships to be identified and exploited while circumventing the usual concerns. The methods and strategies for their implementation in practice are presented. Simulation studies and an application to data from an HIV clinical trial demonstrate the performance of the techniques relative to the existing methods. Copyright © 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {23},
	urldate = {2020-02-07},
	journal = {Statistics in Medicine},
	author = {Tsiatis, Anastasios A. and Davidian, Marie and Zhang, Min and Lu, Xiaomin},
	year = {2008},
	keywords = {variable selection, baseline variables, clinical trials, covariate adjustment, efficiency, semiparametric theory},
	pages = {4658--4677},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/2JNMVL7F/Tsiatis et al. - 2008 - Covariate adjustment for two-sample treatment comp.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/6MGMJMZH/sim.html:text/html},
}

@article{rosenblum_simple_2010,
	title = {Simple, efficient estimators of treatment effects in randomized trials using generalized linear models to leverage baseline variables},
	volume = {6},
	issn = {1557-4679},
	url = {https://jhu.pure.elsevier.com/en/publications/simple-efficient-estimators-of-treatment-effects-in-randomized-tr-4},
	doi = {10.2202/1557-4679.1138},
	language = {English (US)},
	number = {1},
	urldate = {2020-02-07},
	journal = {The international journal of biostatistics},
	author = {Rosenblum, Michael Aaron and Laan, Mark J. Van Der},
	year = {2010},
	pmid = {20628636},
	pages = {13},
	file = {Full Text:/home/imbroglio/Zotero/storage/N566RTMA/Rosenblum and Laan - 2010 - Simple, efficient estimators of treatment effects .pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/QXKZZVHY/simple-efficient-estimators-of-treatment-effects-in-randomized-tr-4.html:text/html},
}

@article{pocock_subgroup_2002,
	title = {Subgroup analysis, covariate adjustment and baseline comparisons in clinical trial reporting: current practiceand problems},
	volume = {21},
	issn = {1097-0258},
	shorttitle = {Subgroup analysis, covariate adjustment and baseline comparisons in clinical trial reporting},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1296},
	doi = {10.1002/sim.1296},
	abstract = {Clinical trial investigators often record a great deal of baseline data on each patient at randomization. When reporting the trial's findings such baseline data can be used for (i) subgroup analyses which explore whether there is evidence that the treatment difference depends on certain patient characteristics, (ii) covariate-adjusted analyses which aim to refine the analysis of the overall treatment difference by taking account of the fact that some baseline characteristics are related to outcome and may be unbalanced between treatment groups, and (iii) baseline comparisons which compare the baseline characteristics of patients in each treatment group for any possible (unlucky) differences. This paper examines how these issues are currently tackled in the medical journals, based on a recent survey of 50 trial reports in four major journals. The statistical ramifications are explored, major problems are highlighted and recommendations for future practice are proposed. Key issues include: the overuse and overinterpretation of subgroup analyses; the underuse of appropriate statistical tests for interaction; inconsistencies in the use of covariate-adjustment; the lack of clear guidelines on covariate selection; the overuse of baseline comparisons in some studies; the misuses of significance tests for baseline comparability, and the need for trials to have a predefined statistical analysis plan for all these uses of baseline data. Copyright © 2002 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {19},
	urldate = {2020-02-07},
	journal = {Statistics in Medicine},
	author = {Pocock, Stuart J. and Assmann, Susan E. and Enos, Laura E. and Kasten, Linda E.},
	year = {2002},
	keywords = {clinical trials, covariate adjustment, baseline comparisons, medical journals, subgroup analysis},
	pages = {2917--2930},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/DFK5XYWH/Pocock et al. - 2002 - Subgroup analysis, covariate adjustment and baseli.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/Y3J7DLEP/sim.html:text/html},
}

@article{kahan_risks_2014,
	title = {The risks and rewards of covariate adjustment in randomized trials: an assessment of 12 outcomes from 8 studies},
	volume = {15},
	issn = {1745-6215},
	shorttitle = {The risks and rewards of covariate adjustment in randomized trials},
	url = {https://doi.org/10.1186/1745-6215-15-139},
	doi = {10.1186/1745-6215-15-139},
	abstract = {Adjustment for prognostic covariates can lead to increased power in the analysis of randomized trials. However, adjusted analyses are not often performed in practice.},
	number = {1},
	urldate = {2020-02-07},
	journal = {Trials},
	author = {Kahan, Brennan C. and Jairath, Vipul and Doré, Caroline J. and Morris, Tim P.},
	month = apr,
	year = {2014},
	pages = {139},
	file = {Full Text:/home/imbroglio/Zotero/storage/DGZ3V9F2/Kahan et al. - 2014 - The risks and rewards of covariate adjustment in r.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/DN8IBY4F/1745-6215-15-139.html:text/html},
}

@article{hernandez_covariate_2004,
	title = {Covariate adjustment in randomized controlled trials with dichotomous outcomes increases statistical power and reduces sample size requirements},
	volume = {57},
	issn = {0895-4356},
	url = {http://www.sciencedirect.com/science/article/pii/S0895435603003792},
	doi = {10.1016/j.jclinepi.2003.09.014},
	abstract = {Objective
Randomized controlled trials (RCTs) with dichotomous outcomes may be analyzed with or without adjustment for baseline characteristics (covariates). We studied type I error, power, and potential reduction in sample size with several covariate adjustment strategies.
Study Design and Setting
Logistic regression analysis was applied to simulated data sets (n=360) with different treatment effects, covariate effects, outcome incidences, and covariate prevalences. Treatment effects were estimated with or without adjustment for a single dichotomous covariate. Strategies included always adjusting for the covariate (“prespecified”), or only when the covariate was predictive or imbalanced.
Results
We found that the type I error was generally at the nominal level. The power was highest with prespecified adjustment. The potential reduction in sample size was higher with stronger covariate effects (from 3 to 46\%, at 50\% outcome incidence and covariate prevalence) and independent of the treatment effect. At lower outcome incidences and/or covariate prevalences, the reduction was lower.
Conclusion
We conclude that adjustment for a predictive baseline characteristic may lead to a potentially important increase in power of analyses of treatment effect. Adjusted analysis should, hence, be considered more often for RCTs with dichotomous outcomes.},
	language = {en},
	number = {5},
	urldate = {2020-02-07},
	journal = {Journal of Clinical Epidemiology},
	author = {Hernández, Adrián V and Steyerberg, Ewout W and Habbema, J. Dik F},
	month = may,
	year = {2004},
	keywords = {Covariate adjustment, Logistic regression, Randomized controlled trials, Sample size, Statistical power, Type I error},
	pages = {454--460},
	file = {ScienceDirect Full Text PDF:/home/imbroglio/Zotero/storage/Q7374UPR/Hernández et al. - 2004 - Covariate adjustment in randomized controlled tria.pdf:application/pdf;ScienceDirect Snapshot:/home/imbroglio/Zotero/storage/69V78BZD/S0895435603003792.html:text/html},
}

@article{colantuoni_leveraging_2015,
	title = {Leveraging prognostic baseline variables to gain precision in randomized trials},
	volume = {34},
	copyright = {Copyright © 2015 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6507},
	doi = {10.1002/sim.6507},
	abstract = {We focus on estimating the average treatment effect in a randomized trial. If baseline variables are correlated with the outcome, then appropriately adjusting for these variables can improve precision. An example is the analysis of covariance (ANCOVA) estimator, which applies when the outcome is continuous, the quantity of interest is the difference in mean outcomes comparing treatment versus control, and a linear model with only main effects is used. ANCOVA is guaranteed to be at least as precise as the standard unadjusted estimator, asymptotically, under no parametric model assumptions and also is locally semiparametric efficient. Recently, several estimators have been developed that extend these desirable properties to more general settings that allow any real-valued outcome (e.g., binary or count), contrasts other than the difference in mean outcomes (such as the relative risk), and estimators based on a large class of generalized linear models (including logistic regression). To the best of our knowledge, we give the first simulation study in the context of randomized trials that compares these estimators. Furthermore, our simulations are not based on parametric models; instead, our simulations are based on resampling data from completed randomized trials in stroke and HIV in order to assess estimator performance in realistic scenarios. We provide practical guidance on when these estimators are likely to provide substantial precision gains and describe a quick assessment method that allows clinical investigators to determine whether these estimators could be useful in their specific trial contexts. Copyright © 2015 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {18},
	urldate = {2020-02-07},
	journal = {Statistics in Medicine},
	author = {Colantuoni, Elizabeth and Rosenblum, Michael},
	year = {2015},
	keywords = {prognostic variables, randomized trial, relative efficiency},
	pages = {2602--2617},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/99JW6DSW/Colantuoni and Rosenblum - 2015 - Leveraging prognostic baseline variables to gain p.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/H3UUC68F/sim.html:text/html},
}

@article{balzer_adaptive_2016,
	title = {Adaptive pre-specification in randomized trials with and without pair-matching},
	volume = {35},
	copyright = {Copyright © 2016 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7023},
	doi = {10.1002/sim.7023},
	abstract = {In randomized trials, adjustment for measured covariates during the analysis can reduce variance and increase power. To avoid misleading inference, the analysis plan must be pre-specified. However, it is often unclear a priori which baseline covariates (if any) should be adjusted for in the analysis. Consider, for example, the Sustainable East Africa Research in Community Health (SEARCH) trial for HIV prevention and treatment. There are 16 matched pairs of communities and many potential adjustment variables, including region, HIV prevalence, male circumcision coverage, and measures of community-level viral load. In this paper, we propose a rigorous procedure to data-adaptively select the adjustment set, which maximizes the efficiency of the analysis. Specifically, we use cross-validation to select from a pre-specified library the candidate targeted maximum likelihood estimator (TMLE) that minimizes the estimated variance. For further gains in precision, we also propose a collaborative procedure for estimating the known exposure mechanism. Our small sample simulations demonstrate the promise of the methodology to maximize study power, while maintaining nominal confidence interval coverage. We show how our procedure can be tailored to the scientific question (intervention effect for the study sample vs. for the target population) and study design (pair-matched or not). Copyright © 2016 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {25},
	urldate = {2020-02-07},
	journal = {Statistics in Medicine},
	author = {Balzer, Laura B. and Laan, Mark J. van der and Petersen, Maya L.},
	year = {2016},
	keywords = {causal inference, targeted maximum likelihood estimation (TMLE), covariate selection, data-adaptive, pair-matched, randomized trials},
	pages = {4528--4545},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/2HJ5BMLI/Balzer et al. - 2016 - Adaptive pre-specification in randomized trials wi.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/97DUI86Z/sim.html:text/html},
}

@article{austin_substantial_2010,
	title = {A substantial and confusing variation exists in handling of baseline covariates in randomized controlled trials: a review of trials published in leading medical journals},
	volume = {63},
	issn = {0895-4356},
	shorttitle = {A substantial and confusing variation exists in handling of baseline covariates in randomized controlled trials},
	url = {http://www.sciencedirect.com/science/article/pii/S0895435609001747},
	doi = {10.1016/j.jclinepi.2009.06.002},
	abstract = {Objective
Statisticians have criticized the use of significance testing to compare the distribution of baseline covariates between treatment groups in randomized controlled trials (RCTs). Furthermore, some have advocated for the use of regression adjustment to estimate the effect of treatment after adjusting for potential imbalances in prognostically important baseline covariates between treatment groups.
Study Design and Setting
We examined 114 RCTs published in the New England Journal of Medicine, the Journal of the American Medical Association, The Lancet, and the British Medical Journal between January 1, 2007 and June 30, 2007.
Results
Significance testing was used to compare baseline characteristics between treatment arms in 38\% of the studies. The practice was very rare in British journals and more common in the U.S. journals. In 29\% of the studies, the primary outcome was continuous, whereas in 65\% of the studies, the primary outcome was either dichotomous or time-to-event in nature. Adjustment for baseline covariates was reported when estimating the treatment effect in 34\% of the studies.
Conclusions
Our findings suggest the need for greater editorial consistency across journals in the reporting of RCTs. Furthermore, there is a need for greater debate about the relative merits of unadjusted vs. adjusted estimates of treatment effect.},
	language = {en},
	number = {2},
	urldate = {2020-02-07},
	journal = {Journal of Clinical Epidemiology},
	author = {Austin, Peter C. and Manca, Andrea and Zwarenstein, Merrick and Juurlink, David N. and Stanbrook, Matthew B.},
	month = feb,
	year = {2010},
	keywords = {Analysis of covariance, Baseline covariates, Clinical trial, CONSORT statement, Randomized controlled trial, Regression adjustment, Significance testing},
	pages = {142--153},
	file = {ScienceDirect Full Text PDF:/home/imbroglio/Zotero/storage/D2UCHT2M/Austin et al. - 2010 - A substantial and confusing variation exists in ha.pdf:application/pdf;ScienceDirect Snapshot:/home/imbroglio/Zotero/storage/NMQETR4I/S0895435609001747.html:text/html},
}

@article{assmann_subgroup_2000,
	title = {Subgroup analysis and other (mis)uses of baseline data in clinical trials},
	volume = {355},
	issn = {0140-6736},
	url = {http://www.sciencedirect.com/science/article/pii/S0140673600020390},
	doi = {10.1016/S0140-6736(00)02039-0},
	abstract = {Background
Baseline data collected on each patient at randomisation in controlled clinical trials can be used to describe the population of patients, to assess comparability of treatment groups, to achieve balanced randomisation, to adjust treatment comparisons for prognostic factors, and to undertake subgroup analyses. We assessed the extent and quality of such practices in major clinical trial reports.
Methods
A sample of 50 consecutive clinical-trial reports was obtained from four major medical journals during July to September, 1997. We tabulated the detailed information on uses of baseline data by use of a standard form.
Findings
Most trials presented baseline comparability in a table. These tables were often unduly large, and about half the trials inappropriately used significance tests for baseline comparison. Methods of randomisation, including possible stratification, were often poorly described. There was little consistency over whether to use covariate adjustment and the criteria for selecting baseline factors for which to adjust were often unclear. Most trials emphasised the simple unadjusted results and covariate adjustment usually made negligible difference. Two-thirds of the reports presented subgroup findings, but mostly without appropriate statistical tests for interaction. Many reports put too much emphasis on subgroup analyses that commonly lacked statistical power.
Interpretation
Clinical trials need a predefined statistical analysis plan for uses of baseline data, especially covariate-adjusted analyses and subgroup analyses. Investigators and journals need to adopt improved standards of statistical reporting, and exercise caution when drawing conclusions from subgroup findings.},
	language = {en},
	number = {9209},
	urldate = {2020-02-07},
	journal = {The Lancet},
	author = {Assmann, Susan F and Pocock, Stuart J and Enos, Laura E and Kasten, Linda E},
	month = mar,
	year = {2000},
	pages = {1064--1069},
	file = {ScienceDirect Full Text PDF:/home/imbroglio/Zotero/storage/CJDAM5E9/Assmann et al. - 2000 - Subgroup analysis and other (mis)uses of baseline .pdf:application/pdf;ScienceDirect Snapshot:/home/imbroglio/Zotero/storage/34VZCZKJ/S0140673600020390.html:text/html},
}

@article{duvernoy_cortical_1981,
	title = {Cortical blood vessels of the human brain},
	volume = {7},
	issn = {0361-9230},
	url = {http://www.sciencedirect.com/science/article/pii/0361923081900071},
	doi = {10.1016/0361-9230(81)90007-1},
	abstract = {The study is divided into two parts, (a) Superficial or pial vessels: Arterioles and venules at the gyrus surface as well as their mode of penetration into or emergence from nervous tissue is described. The absence of pial capillaries is noted. Arterial and venous anastomoses are described whereas arteriovenous anastomoses were not encountered. In particular, the relationship of superficial vessels to the arachnoid was studied, (b) Intracortical vessels: Arteries and veins were divided into 5 groups according to their degree of cortical penetration. Considering its density, the vascular network of the cortex was divided into 4 vascular layers. A correlation between these layers and the cellular layers was established. Problems in distinguishing between arteries and veins, the geometric disposition of cortical vessels, different types of anastomoses and particular vascular features whose significance remains unclear, are discussed.
Résumé
Ce travail est divisé en deux chapitres. (a) Les vaisseaux superficiels ou pie-mériens: Les artérioles et les veinules de la surface des gyri sont décrites ainsi que leur mode de pénétration ou d'émergence dans le tissu nerveux; on constate I'absence de capillaires pie-mériens. Des anastomoses artérielles et veineuses ont été décrites tandis que des anastomoses artérioveineuses n'ont pas été recontrées. Les rapports des vaisseaux superficiels vis-à-vis de I'arachnoïde ont été particuliérement étudiés. (b) Les vaisseaux intracorticaux: Les arteres et les veines ont ete divisees en 5 groupes suivant leur degré de pénetration dans le cortex. La trame vasculaire du cortex, en tenant compte de sa densité, est répartie en 4 couches vasculaires dont la correlation avec les couches nerveuses a été établie. On a discuté ensuite les problèmes de distinction entre artères et veines, de repartition géométrique des vaisseaux dans le cortex, des différents types d'anastomoses et des images particulières dont la signification est encore obscure.},
	number = {5},
	urldate = {2019-02-12},
	journal = {Brain Research Bulletin},
	author = {Duvernoy, H. M. and Delon, S. and Vannson, J. L.},
	month = nov,
	year = {1981},
	keywords = {Cerebral cortex Cerebral vascularization Human brain Microcirculation, Cortex cérébral Vascularisation cérébrale Cerveau humain Microcirculation},
	pages = {519--579},
	file = {ScienceDirect Full Text PDF:/home/imbroglio/Zotero/storage/MJJS69BN/Duvernoy et al. - 1981 - Cortical blood vessels of the human brain.pdf:application/pdf},
}

@article{rosenbluth_analyzing_2008,
	title = {Analyzing cell mechanics in hematologic diseases with microfluidic biophysical flow cytometry},
	volume = {8},
	issn = {1473-0189},
	url = {https://pubs.rsc.org/en/content/articlelanding/2008/lc/b802931h},
	doi = {10.1039/B802931H},
	abstract = {Pathological processes in hematologic diseases originate at the single-cell level, often making measurements on individual cells more clinically relevant than population averages from bulk analysis. For this reason, flow cytometry has been an effective tool for single-cell analysis of properties using light scattering and fluorescence labeling. However, conventional flow cytometry cannot measure cell mechanical properties, alterations of which contribute to the pathophysiology of hematologic diseases such as sepsis, diabetic retinopathy, and sickle cell anemia. Here we present a high-throughput microfluidics-based ‘biophysical’ flow cytometry technique that measures single-cell transit times of blood cell populations passing through in vitro capillary networks. To demonstrate clinical relevance, we use this technique to characterize biophysical changes in two model disease states in which mechanical properties of cells are thought to lead to microvascular obstruction: (i) sepsis, a process in which inflammatory mediators in the bloodstream activate neutrophils and (ii) leukostasis, an often fatal and poorly understood complication of acute leukemia. Using patient samples, we show that cell transit time through and occlusion of microfluidic channels is increased for both disease states compared to control samples, and we find that mechanical heterogeneity of blood cell populations is a better predictor of microvascular obstruction than average properties. Inflammatory mediators involved in sepsis were observed to significantly affect the shape and magnitude of the neutrophil transit time population distribution. Altered properties of leukemia cell subpopulations, rather than of the population as a whole, were found to correlate with symptoms of leukostasis in patients—a new result that may be useful for guiding leukemia therapy. By treating cells with drugs that affect the cytoskeleton, we also demonstrate that their transit times could be significantly reduced. Biophysical flow cytometry offers a low-cost and high-throughput diagnostic and drug discovery platform for hematologic diseases that affect microcirculatory flow.},
	language = {en},
	number = {7},
	urldate = {2019-02-12},
	journal = {Lab on a Chip},
	author = {Rosenbluth, Michael J. and Lam, Wilbur A. and Fletcher, Daniel A.},
	month = jun,
	year = {2008},
	pages = {1062--1070},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/BYEV9P9V/Rosenbluth et al. - 2008 - Analyzing cell mechanics in hematologic diseases w.pdf:application/pdf},
}

@article{tsai_vitro_2012,
	title = {In vitro modeling of the microvascular occlusion and thrombosis that occur in hematologic diseases using microfluidic technology},
	volume = {122},
	issn = {0021-9738},
	url = {https://www.jci.org/articles/view/58753},
	doi = {10.1172/JCI58753},
	language = {en},
	number = {1},
	urldate = {2019-02-12},
	journal = {The Journal of Clinical Investigation},
	author = {Tsai, Michelle and Kita, Ashley and Leach, Joseph and Rounsevell, Ross and Huang, James N. and Moake, Joel and Ware, Russell E. and Fletcher, Daniel A. and Lam, Wilbur A.},
	month = jan,
	year = {2012},
	pmid = {22156199},
	pages = {408--418},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/YCLXXS4Z/Tsai et al. - 2012 - In vitro modeling of the microvascular occlusion a.pdf:application/pdf},
}

@article{wong_microfluidic_2012,
	title = {Microfluidic {Models} of {Vascular} {Functions}},
	volume = {14},
	url = {https://doi.org/10.1146/annurev-bioeng-071811-150052},
	doi = {10.1146/annurev-bioeng-071811-150052},
	abstract = {In vitro studies of vascular physiology have traditionally relied on cultures of endothelial cells, smooth muscle cells, and pericytes grown on centimeter-scale plates, filters, and flow chambers. The introduction of microfluidic tools has revolutionized the study of vascular physiology by allowing researchers to create physiologically relevant culture models, at the same time greatly reducing the consumption of expensive reagents. By taking advantage of the small dimensions and laminar flow inherent in microfluidic systems, recent studies have created in vitro models that reproduce many features of the in vivo vascular microenvironment with fine spatial and temporal resolution. In this review, we highlight the advantages of microfluidics in four areas: the investigation of hemodynamics on a capillary length scale, the modulation of fluid streams over vascular cells, angiogenesis induced by the exposure of vascular cells to well-defined gradients in growth factors or pressure, and the growth of microvascular networks in biomaterials. Such unique capabilities at the microscale are rapidly advancing the understanding of microcirculatory dynamics, shear responses, and angiogenesis in health and disease as well as the ability to create in vivo–like blood vessels in vitro.},
	number = {1},
	urldate = {2019-02-12},
	journal = {Annual Review of Biomedical Engineering},
	author = {Wong, Keith H.K. and Chan, Juliana M. and Kamm, Roger D. and Tien, Joe},
	year = {2012},
	pmid = {22540941},
	pages = {205--230},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/UKHRY66Y/Wong et al. - 2012 - Microfluidic Models of Vascular Functions.pdf:application/pdf},
}

@article{helms_vitro_2016,
	title = {In vitro models of the blood–brain barrier: {An} overview of commonly used brain endothelial cell culture models and guidelines for their use},
	volume = {36},
	issn = {0271-678X},
	shorttitle = {In vitro models of the blood–brain barrier},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4853841/},
	doi = {10.1177/0271678X16630991},
	abstract = {The endothelial cells lining the brain capillaries separate the blood from the brain parenchyma. The endothelial monolayer of the brain capillaries serves both as a crucial interface for exchange of nutrients, gases, and metabolites between blood and brain, and as a barrier for neurotoxic components of plasma and xenobiotics. This “blood-brain barrier” function is a major hindrance for drug uptake into the brain parenchyma. Cell culture models, based on either primary cells or immortalized brain endothelial cell lines, have been developed, in order to facilitate in vitro studies of drug transport to the brain and studies of endothelial cell biology and pathophysiology. In this review, we aim to give an overview of established in vitro blood–brain barrier models with a focus on their validation regarding a set of well-established blood–brain barrier characteristics. As an ideal cell culture model of the blood–brain barrier is yet to be developed, we also aim to give an overview of the advantages and drawbacks of the different models described.},
	number = {5},
	urldate = {2019-02-12},
	journal = {Journal of Cerebral Blood Flow \& Metabolism},
	author = {Helms, Hans C and Abbott, N Joan and Burek, Malgorzata and Cecchelli, Romeo and Couraud, Pierre-Olivier and Deli, Maria A and Förster, Carola and Galla, Hans J and Romero, Ignacio A and Shusta, Eric V and Stebbins, Matthew J and Vandenhaute, Elodie and Weksler, Babette and Brodin, Birger},
	month = may,
	year = {2016},
	pmid = {26868179},
	pmcid = {PMC4853841},
	pages = {862--890},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/NTJL4ZDZ/Helms et al. - 2016 - In vitro models of the blood–brain barrier An ove.pdf:application/pdf},
}

@article{shih_two-photon_2012,
	title = {Two-photon microscopy as a tool to study blood flow and neurovascular coupling in the rodent brain},
	volume = {32},
	issn = {0271-678X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3390800/},
	doi = {10.1038/jcbfm.2011.196},
	abstract = {The cerebral vascular system services the constant demand for energy during neuronal activity in the brain. Attempts to delineate the logic of neurovascular coupling have been greatly aided by the advent of two-photon laser scanning microscopy to image both blood flow and the activity of individual cells below the surface of the brain. Here we provide a technical guide to imaging cerebral blood flow in rodents. We describe in detail the surgical procedures required to generate cranial windows for optical access to the cortex of both rats and mice and the use of two-photon microscopy to accurately measure blood flow in individual cortical vessels concurrent with local cellular activity. We further provide examples on how these techniques can be applied to the study of local blood flow regulation and vascular pathologies such as small-scale stroke.},
	number = {7},
	urldate = {2019-02-12},
	journal = {Journal of Cerebral Blood Flow \& Metabolism},
	author = {Shih, Andy Y and Driscoll, Jonathan D and Drew, Patrick J and Nishimura, Nozomi and Schaffer, Chris B and Kleinfeld, David},
	month = jul,
	year = {2012},
	pmid = {22293983},
	pmcid = {PMC3390800},
	pages = {1277--1309},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/VZNV5MMB/Shih et al. - 2012 - Two-photon microscopy as a tool to study blood flo.pdf:application/pdf},
}

@article{shih_smallest_2013,
	title = {The smallest stroke: occlusion of one penetrating vessel leads to infarction and a cognitive deficit},
	volume = {16},
	copyright = {2012 Nature Publishing Group},
	issn = {1546-1726},
	shorttitle = {The smallest stroke},
	url = {https://www.nature.com/articles/nn.3278},
	doi = {10.1038/nn.3278},
	abstract = {Microinfarctions are present in the aged and injured human brain. Their clinical relevance is controversial, with postulated sequelae ranging from cognitive sparing to vascular dementia. To address the consequences of microinfarcts, we used controlled optical methods to create occlusions of individual penetrating arterioles or venules in rat cortex. Single microinfarcts, targeted to encompass all or part of a cortical column, impaired performance in a macrovibrissa-based behavioral task. Furthermore, the targeting of multiple vessels resulted in tissue damage that coalesced across cortex, even though the intervening penetrating vessels were acutely patent. Post-occlusion administration of memantine, a glutamate receptor antagonist that reduces cognitive decline in Alzheimer's disease, ameliorated tissue damage and perceptual deficits. Collectively, these data imply that microinfarcts likely contribute to cognitive decline. Strategies that have received limited success in the treatment of ischemic injury, which include therapeutics against excitotoxicity, may be successful against the progressive nature of vascular dementia.},
	language = {en},
	number = {1},
	urldate = {2019-02-12},
	journal = {Nature Neuroscience},
	author = {Shih, Andy Y. and Blinder, Pablo and Tsai, Philbert S. and Friedman, Beth and Stanley, Geoffrey and Lyden, Patrick D. and Kleinfeld, David},
	month = jan,
	year = {2013},
	pages = {55--63},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/H9S94H5F/Shih et al. - 2013 - The smallest stroke occlusion of one penetrating .pdf:application/pdf},
}

@article{fisher_capsular_1979,
	title = {Capsular {Infarcts}: {The} {Underlying} {Vascular} {Lesions}},
	volume = {36},
	issn = {0003-9942},
	shorttitle = {Capsular {Infarcts}},
	url = {https://jamanetwork.com/journals/jamaneurology/fullarticle/577205},
	doi = {10.1001/archneur.1979.00500380035003},
	abstract = {{\textless}p{\textgreater}• In ten patients, 11 infarcts involving mainly the internal capsule have been examined pathologically. Serial sections of the involved basal ganglia were studied in ten infarcts and only a gross dissection was made in the other. The implicated penetrating arteries were traced throughout their length and obstructive vascular lesions were found in nine instances. In two of the nine there was an atheromatous plaque with a superimposed thrombus, in four an atheromatous plaque had caused severe stenosis, in one a destructive arterial process lipohyalinosis had occurred, in one case the nature of the obstruction remained "uncertain," and in one the penetrating arteries were obstructed at their orifices by an atheroma in the superior division of the middle cerebral artery. In two cases the vessels were patent, suggesting embolism. The atheromas consisted almost exclusively of a conglomerate of fat-filled macrophages.{\textless}/p{\textgreater}{\textless}p{\textgreater}The clinical correlate was a pure motor hemiplegia or hemiparesis involving the face, arm, and leg without sensory deficit, homonymous hemianopia, receptive aphasia, or apractognosia. Confusion was prominent in one patient.{\textless}/p{\textgreater}},
	language = {en},
	number = {2},
	urldate = {2019-02-09},
	journal = {Archives of Neurology},
	author = {Fisher, C. Miller},
	month = feb,
	year = {1979},
	pages = {65--73},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/MFDDR46Y/Fisher - 1979 - Capsular Infarcts The Underlying Vascular Lesions.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/SZUE5HQ8/577205.html:text/html},
}

@article{reed_effects_2004,
	title = {Effects of {White} {Matter} {Lesions} and {Lacunes} on {Cortical} {Function}},
	volume = {61},
	issn = {0003-9942},
	url = {https://dx.doi.org/10.1001/archneur.61.10.1545},
	doi = {10.1001/archneur.61.10.1545},
	abstract = {Subcortical ischemic vascular dementia has been ascribed to prominent frontal lobe dysfunction secondary to ischemic lesions in frontothalamiccircuits.Whether small-vessel disease in fact predominantly affects the frontal lobes is not well documented.To investigate the effects of subcortical lesions (lacunes and white matter lesions [WML]) on cortical function, as reflected in glucose metabolism and cognitive function, in elderly individuals.Cross-sectional analyses of case series.Multicenter, university-based study of subcortical vascular dementia.Persons with normal cognition, mild cognitive impairment, or dementia and with and without lacunes on magnetic resonance images.Regional cerebral glucose metabolism, normalized regional metabolic activity, and neuropsychological test scores. Major hypotheses were that volume of lacunes and WML correlate selectively with hypometabolism of prefrontal cortex and failure of executive cognitive ability.Lacunes correlated with metabolic rates in dorsolateral frontal cortex (DLF); WML substantially reduced metabolic rates throughout cortex, most strongly so in DLF. When regional metabolic activity was normalized to whole brain activity, lacunes remained correlated with DLF activity, whereas the WML effect was no longer found, probably because of its general distribution. Regional cerebral glucose metabolism and normalized activity in DLF also correlated with cortical atrophy. Metabolic activity in DLF correlated with executive function, memory, and global cognitive function, while activity in middle temporal gyrus correlated with memory and global function but not executive function.The metabolic effects of lacunes and WML are most apparent in DLF, but the effects of WML are generalized and frontal hypometabolism correlates with memory and global impairment, cognitive as well as executive function. The effects of subcortical cerebrovascular disease appear to converge on the frontal lobes but are diffuse, complex, and of modest magnitude.Arch Neurol. 2004;61:1545-1550--{\textgreater}},
	number = {10},
	urldate = {2019-02-10},
	journal = {Archives of Neurology},
	author = {Reed, Bruce R. and Eberling, Jamie L. and Mungas, Dan and Weiner, Michael and Kramer, Joel H. and Jagust, William J.},
	month = oct,
	year = {2004},
	pages = {1545--1550},
	file = {Full Text:/home/imbroglio/Zotero/storage/ABIHFQNT/Reed et al. - 2004 - Effects of White Matter Lesions and Lacunes on Cor.pdf:application/pdf},
}

@article{koga_cognitive_2009,
	title = {Cognitive {Consequences} of {Multiple} {Lacunes} and {Leukoaraiosis} as {Vascular} {Cognitive} {Impairment} in {Community}-{Dwelling} {Elderly} {Individuals}},
	volume = {18},
	issn = {1052-3057},
	url = {http://www.sciencedirect.com/science/article/pii/S1052305708001808},
	doi = {https://doi.org/10.1016/j.jstrokecerebrovasdis.2008.07.010},
	abstract = {The aim of our study was to investigate the effects of silent brain lesions on cognitive function of community-dwelling elderly individuals. Brain magnetic resonance imaging and other medical examinations were performed on 350 nondemented elderly individuals (121 male and 229 female, average age 72.4 years) who resided in the rural community of Sefuri Village, Saga, Japan. The mini mental state examination and modified Stroop test (MST) were used to identify cognitive impairment. White matter lesions (WMLs) and cerebral atrophy on magnetic resonance imaging were measured quantitatively. Multivariate analyses were done using a logistic regression model with a software package. Cognitive impairment defined by mini mental state examination score less than 24 was present in 55 individuals (15.7\%). They had a lower educational level, significantly larger quantity of WMLs, and more remarkable cerebral atrophy. Frontal lobe dysfunction was detected in 52 individuals (14.9\%) through prolonged MST score ({\textgreater}36 seconds). Impaired frontal lobe function was related to number of silent lacunar infarcts, larger WMLs, and more prominent cerebral atrophy. MST score in individuals with two or more infarcts was significantly more prolonged compared with MST score in those without infarction. These results suggest that WMLs may cause rather diffuse cognitive decline, whereas multiple lacunar infarcts are specifically involved in frontal lobe dysfunction. Silent ischemic lesions in apparently healthy elderly individuals seem to form a distinctive group of people with vascular cognitive impairment without dementia. This group should be the primary target of prevention of vascular dementia.},
	number = {1},
	journal = {Journal of Stroke and Cerebrovascular Diseases},
	author = {Koga, Hiroshi and Takashima, Yuki and Murakawa, Ryo and Uchino, Akira and Yuzuriha, Takefumi and Yao, Hiroshi},
	year = {2009},
	keywords = {Asymptomatic stroke, lacunar infarction, magnetic resonance imaging, neuropsychology, vascular dementia, white matter lesions},
	pages = {32 -- 37},
}

@article{liang_association_2017,
	title = {Association of {Cerebral} {Small} {Vessel} {Disease} {Burden} and {Health}-{Related} {Quality} of {Life} after {Acute} {Ischemic} {Stroke}},
	volume = {9},
	issn = {1663-4365},
	url = {http://journal.frontiersin.org/article/10.3389/fnagi.2017.00372/full},
	doi = {10.3389/fnagi.2017.00372},
	language = {en},
	urldate = {2019-02-09},
	journal = {Frontiers in Aging Neuroscience},
	author = {Liang, Yan and Chen, Yang-Kun and Deng, Min and Mok, Vincent C. T. and Wang, De-Feng and Ungvari, Gabor S. and Chu, Chiu-wing W. and Kamiya, Akane and Tang, Wai-Kwong},
	month = nov,
	year = {2017},
	file = {Liang et al. - 2017 - Association of Cerebral Small Vessel Disease Burde.pdf:/home/imbroglio/microfluidics/00_literature/Liang et al. - 2017 - Association of Cerebral Small Vessel Disease Burde.pdf:application/pdf},
}

@article{dhamoon_periventricular_2018,
	title = {Periventricular {White} {Matter} {Hyperintensities} and {Functional} {Decline}},
	volume = {66},
	issn = {00028614},
	url = {http://doi.wiley.com/10.1111/jgs.15149},
	doi = {10.1111/jgs.15149},
	abstract = {Background/Objectives—We previously showed that global brain white matter hyperintensity volume (WMHV) was associated with accelerated long-term functional decline. We hypothesized that WMHV in particular brain regions are more predictive of functional decline.},
	language = {en},
	number = {1},
	urldate = {2019-02-09},
	journal = {Journal of the American Geriatrics Society},
	author = {Dhamoon, Mandip S. and Cheung, Ying-Kuen and Bagci, Ahmet and Alperin, Noam and Sacco, Ralph L. and Elkind, Mitchell S. V. and Wright, Clinton B.},
	month = jan,
	year = {2018},
	pages = {113--119},
	file = {Dhamoon et al. - 2018 - Periventricular White Matter Hyperintensities and .pdf:/home/imbroglio/microfluidics/00_literature/Dhamoon et al. - 2018 - Periventricular White Matter Hyperintensities and .pdf:application/pdf},
}

@article{jokinen_incident_2011,
	title = {Incident lacunes influence cognitive decline},
	volume = {76},
	issn = {0028-3878},
	url = {http://n.neurology.org/content/76/22/1872},
	doi = {10.1212/WNL.0b013e31821d752f},
	abstract = {Background: In cerebral small vessel disease, the core MRI findings include white matter lesions (WML) and lacunar infarcts. While the clinical significance of WML is better understood, the contribution of lacunes to the rate of cognitive decline has not been established. This study investigated whether incident lacunes on MRI determine longitudinal cognitive change in elderly subjects with WML. Methods: Within the Leukoaraiosis and Disability Study (LADIS), 387 subjects were evaluated with repeated MRI and neuropsychological assessment at baseline and after 3 years. Predictors of change in global cognitive function and specific cognitive domains over time were analyzed with multivariate linear regression. Results: After controlling for demographic factors, baseline cognitive performance, baseline lacunar and WML lesion load, and WML progression, the number of new lacunes was related to subtle decrease in compound scores for executive functions (p = 0.021) and speed and motor control (p = 0.045), but not for memory or global cognitive function. Irrespective of lacunes, WML progression was associated with decrease in executive functions score (p = 0.016). Conclusion: Incident lacunes on MRI parallel a steeper rate of decline in executive functions and psychomotor speed. Accordingly, in addition to WML, lacunes determine longitudinal cognitive impairment in small vessel disease. Although the individual contribution of lacunes on cognition was modest, they cannot be considered benign findings, but indicate a risk of progressive cognitive impairment.},
	number = {22},
	journal = {Neurology},
	author = {Jokinen, H. and Gouw, A.A. and Madureira, S. and Ylikoski, R. and van Straaten, E.C.W. and van der Flier, W.M. and Barkhof, F. and Scheltens, P. and Fazekas, F. and Schmidt, R. and Verdelho, A. and Ferro, J.M. and Pantoni, L. and Inzitari, D. and Erkinjuntti, T.},
	year = {2011},
	pages = {1872--1878},
}

@article{sbalzarini_feature_2005,
	title = {Feature point tracking and trajectory analysis for video imaging in cell biology},
	volume = {151},
	issn = {10478477},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047847705001267},
	doi = {10.1016/j.jsb.2005.06.002},
	abstract = {This paper presents a computationally eﬃcient, two-dimensional, feature point tracking algorithm for the automated detection and quantitative analysis of particle trajectories as recorded by video imaging in cell biology. The tracking process requires no a priori mathematical modeling of the motion, it is self-initializing, it discriminates spurious detections, and it can handle temporary occlusion as well as particle appearance and disappearance from the image region. The eﬃciency of the algorithm is validated on synthetic video data where it is compared to existing methods and its accuracy and precision are assessed for a wide range of signalto-noise ratios. The algorithm is well suited for video imaging in cell biology relying on low-intensity ﬂuorescence microscopy. Its applicability is demonstrated in three case studies involving transport of low-density lipoproteins in endosomes, motion of ﬂuorescently labeled Adenovirus-2 particles along microtubules, and tracking of quantum dots on the plasma membrane of live cells. The present automated tracking process enables the quantiﬁcation of dispersive processes in cell biology using techniques such as moment scaling spectra.},
	language = {en},
	number = {2},
	urldate = {2019-02-09},
	journal = {Journal of Structural Biology},
	author = {Sbalzarini, I.F. and Koumoutsakos, P.},
	month = aug,
	year = {2005},
	pages = {182--195},
	file = {Sbalzarini and Koumoutsakos - 2005 - Feature point tracking and trajectory analysis for.pdf:/home/imbroglio/microfluidics/00_literature/Sbalzarini and Koumoutsakos - 2005 - Feature point tracking and trajectory analysis for.pdf:application/pdf},
}

@article{zilberman-rudenko_utility_2017,
	title = {Utility of microfluidic devices to study the platelet–endothelium interface},
	volume = {28},
	issn = {0953-7104, 1369-1635},
	url = {https://www.tandfonline.com/doi/full/10.1080/09537104.2017.1280600},
	doi = {10.1080/09537104.2017.1280600},
	abstract = {The integration of biomaterials and understanding of vascular biology has led to the development of perfusable endothelialized flow models, which have been used as valuable tools to study the platelet–endothelium interface under shear. In these models, the parameters of geometry, compliance, biorheology, and cellular complexity are varied to recapitulate the physical biology of platelet recruitment and activation under physiologically relevant conditions of blood flow. In this review, we summarize the mechanistic insights learned from perfusable microvessel models and discuss the potential utility as well as challenges of endothelialized microfluidic devices to study platelet function in the bloodstream in vitro.},
	language = {en},
	number = {5},
	urldate = {2019-02-09},
	journal = {Platelets},
	author = {Zilberman-Rudenko, Jevgenia and Sylman, Joanna L. and Garland, Kathleen S. and Puy, Cristina and Wong, Andrew D. and Searson, Peter C. and McCarty, Owen J. T.},
	month = jul,
	year = {2017},
	pages = {449--456},
	file = {Zilberman-Rudenko et al. - 2017 - Utility of microfluidic devices to study the plate.pdf:/home/imbroglio/microfluidics/00_literature/Zilberman-Rudenko et al. - 2017 - Utility of microfluidic devices to study the plate.pdf:application/pdf},
}

@article{osaki_vitro_2018,
	title = {In {Vitro} {Microfluidic} {Models} for {Neurodegenerative} {Disorders}},
	volume = {7},
	issn = {21922640},
	url = {http://doi.wiley.com/10.1002/adhm.201700489},
	doi = {10.1002/adhm.201700489},
	language = {en},
	number = {2},
	urldate = {2019-02-09},
	journal = {Advanced Healthcare Materials},
	author = {Osaki, Tatsuya and Shin, Yoojin and Sivathanu, Vivek and Campisi, Marco and Kamm, Roger D.},
	month = jan,
	year = {2018},
	pages = {1700489},
	file = {Osaki et al. - 2018 - In Vitro Microfluidic Models for Neurodegenerative.pdf:/home/imbroglio/microfluidics/00_literature/Osaki et al. - 2018 - In Vitro Microfluidic Models for Neurodegenerative.pdf:application/pdf},
}

@article{mannino_endothelial_2018,
	title = {Endothelial cell culture in microfluidic devices for investigating microvascular processes},
	volume = {12},
	issn = {1932-1058},
	url = {http://aip.scitation.org/doi/10.1063/1.5024901},
	doi = {10.1063/1.5024901},
	language = {en},
	number = {4},
	urldate = {2019-02-09},
	journal = {Biomicrofluidics},
	author = {Mannino, Robert G. and Qiu, Yongzhi and Lam, Wilbur A.},
	month = jul,
	year = {2018},
	pages = {042203},
	file = {Mannino et al. - 2018 - Endothelial cell culture in microfluidic devices f.pdf:/home/imbroglio/microfluidics/00_literature/Mannino et al. - 2018 - Endothelial cell culture in microfluidic devices f.pdf:application/pdf},
}

@article{benkeser_online_2018,
	title = {Online cross-validation-based ensemble learning},
	volume = {37},
	issn = {1097-0258},
	doi = {10.1002/sim.7320},
	abstract = {Online estimators update a current estimate with a new incoming batch of data without having to revisit past data thereby providing streaming estimates that are scalable to big data. We develop flexible, ensemble-based online estimators of an infinite-dimensional target parameter, such as a regression function, in the setting where data are generated sequentially by a common conditional data distribution given summary measures of the past. This setting encompasses a wide range of time-series models and, as special case, models for independent and identically distributed data. Our estimator considers a large library of candidate online estimators and uses online cross-validation to identify the algorithm with the best performance. We show that by basing estimates on the cross-validation-selected algorithm, we are asymptotically guaranteed to perform as well as the true, unknown best-performing algorithm. We provide extensions of this approach including online estimation of the optimal ensemble of candidate online estimators. We illustrate excellent performance of our methods using simulations and a real data example where we make streaming predictions of infectious disease incidence using data from a large database. Copyright © 2017 John Wiley \& Sons, Ltd.},
	language = {eng},
	number = {2},
	journal = {Statistics in Medicine},
	author = {Benkeser, David and Ju, Cheng and Lendle, Sam and van der Laan, Mark},
	month = jan,
	year = {2018},
	pmid = {28474419},
	pmcid = {PMC5671383},
	keywords = {Humans, cross-validation, machine learning, Algorithms, Biostatistics, Communicable Diseases, Computer Simulation, Databases, Factual, dependent data ensemble learning, Incidence, Likelihood Functions, Machine Learning, Models, Statistical, online estimation, Online Systems, Regression Analysis, stochastic gradient descent, Stochastic Processes, time-series},
	pages = {249--260},
	file = {Full Text:/home/imbroglio/Zotero/storage/HZHU7H6X/Benkeser et al. - 2018 - Online cross-validation-based ensemble learning.pdf:application/pdf},
}

@article{cox_regression_1972,
	title = {Regression {Models} and {Life}-{Tables}},
	volume = {34},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2985181},
	abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
	number = {2},
	urldate = {2020-08-13},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Cox, D. R.},
	year = {1972},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {187--220},
	file = {cox1972paper.pdf:/home/imbroglio/Zotero/storage/87QX3MJE/cox1972paper.pdf:application/pdf},
}

@book{jewell_statistics_2003,
	address = {Boca Raton},
	edition = {1st edition},
	title = {Statistics for {Epidemiology}},
	isbn = {978-1-58488-433-0},
	language = {English},
	publisher = {Routledge},
	author = {Jewell, Nicholas P.},
	month = aug,
	year = {2003},
}

@article{koller_competing_2012,
	title = {Competing risks and the clinical community: irrelevance or ignorance?},
	volume = {31},
	issn = {1097-0258},
	shorttitle = {Competing risks and the clinical community},
	doi = {10.1002/sim.4384},
	abstract = {Life expectancy has dramatically increased in industrialized nations over the last 200 hundred years. The aging of populations carries over to clinical research and leads to an increasing representation of elderly and multimorbid individuals in study populations. Clinical research in these populations is complicated by the fact that individuals are likely to experience several potential disease endpoints that prevent some disease-specific endpoint of interest from occurrence. Large developments in competing risks methodology have been achieved over the last decades, but we assume that recognition of competing risks in the clinical community is still marginal. It is the aim of this article to address translational aspects of competing risks to the clinical community. We describe clinical populations where competing risks issues may arise. We then discuss the importance of agreement between the competing risks methodology and the study aim, in particular the distinction between etiologic and prognostic research questions. In a review of 50 clinical studies performed in individuals susceptible to competing risks published in high-impact clinical journals, we found competing risks issues in 70\% of all articles. Better recognition of issues related to competing risks and of statistical methods that deal with competing risks in accordance with the aim of the study is needed.},
	language = {eng},
	number = {11-12},
	journal = {Statistics in Medicine},
	author = {Koller, Michael T. and Raatz, Heike and Steyerberg, Ewout W. and Wolbers, Marcel},
	month = may,
	year = {2012},
	pmid = {21953401},
	pmcid = {PMC3575691},
	keywords = {Aged, Aged, 80 and over, Female, Humans, Male, Models, Statistical, Aging, Biomedical Research, Data Interpretation, Statistical, Life Expectancy, Risk},
	pages = {1089--1097},
	file = {Full Text:/home/imbroglio/Zotero/storage/SI8L3I89/Koller et al. - 2012 - Competing risks and the clinical community irrele.pdf:application/pdf;PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/PX5DJDQ3/Koller et al. - 2012 - Competing risks and the clinical community irrele.pdf:application/pdf},
}

@article{scheike_flexible_2008,
	title = {Flexible competing risks regression modeling and goodness-of-fit},
	volume = {14},
	issn = {1380-7870},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2715961/},
	doi = {10.1007/s10985-008-9094-0},
	abstract = {In this paper we consider different approaches for estimation and assessment of covariate effects for the cumulative incidence curve in the competing risks model. The classic approach is to model all cause-specific hazards and then estimate the cumulative incidence curve based on these cause-specific hazards. Another recent approach is to directly model the cumulative incidence by a proportional model (), and then obtain direct estimates of how covariates influences the cumulative incidence curve. We consider a simple and flexible class of regression models that is easy to fit and contains the Fine-Gray model as a special case. One advantage of this approach is that our regression modeling allows for non-proportional hazards. This leads to a new simple goodness-of-fit procedure for the proportional subdistribution hazards assumption that is very easy to use. The test is constructive in the sense that it shows exactly where non-proportionality is present. We illustrate our methods to a bone marrow transplant data from the Center for International Blood and Marrow Transplant Research (CIBMTR). Through this data example we demonstrate the use of the flexible regression models to analyze competing risks data when non-proportionality is present in the data.},
	number = {4},
	urldate = {2021-10-01},
	journal = {Lifetime data analysis},
	author = {Scheike, Thomas H. and Zhang, Mei-Jie},
	month = dec,
	year = {2008},
	pmid = {18752067},
	pmcid = {PMC2715961},
	pages = {464--483},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/8XQBZCFQ/Scheike and Zhang - 2008 - Flexible competing risks regression modeling and g.pdf:application/pdf},
}

@article{martinussen_subtleties_2018,
	title = {Subtleties in the interpretation of hazard ratios},
	url = {http://arxiv.org/abs/1810.09192},
	abstract = {The hazard ratio is one of the most commonly reported measures of treatment effect in randomised trials, yet the source of much misinterpretation. This point was made clear by (Hernan, 2010) in commentary, which emphasised that the hazard ratio contrasts populations of treated and untreated individuals who survived a given period of time, populations that will typically fail to be comparable - even in a randomised trial - as a result of different pressures or intensities acting on both populations. The commentary has been very influential, but also a source of surprise and confusion. In this note, we aim to provide more insight into the subtle interpretation of hazard ratios and differences, by investigating in particular what can be learned about treatment effect from the hazard ratio becoming 1 after a certain period of time. Throughout, we will focus on the analysis of randomised experiments, but our results have immediate implications for the interpretation of hazard ratios in observational studies.},
	urldate = {2021-09-14},
	journal = {arXiv:1810.09192 [math, stat]},
	author = {Martinussen, Torben and Vansteelandt, Stijn and Andersen, Per Kragh},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.09192},
	keywords = {Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/422S3SFT/Martinussen et al. - 2018 - Subtleties in the interpretation of hazard ratios.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/LMDRDB57/1810.html:text/html},
}

@article{didelez_logic_nodate,
	title = {On the logic of collapsibility for causal effect measures},
	volume = {n/a},
	issn = {1521-4036},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.202000305},
	doi = {10.1002/bimj.202000305},
	language = {en},
	number = {n/a},
	urldate = {2021-09-14},
	journal = {Biometrical Journal},
	author = {Didelez, Vanessa and Stensrud, Mats Julius},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.202000305},
	keywords = {causal inference, confounding, collapsibility, survival analysis},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/RTBQVCST/Didelez and Stensrud - On the logic of collapsibility for causal effect m.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/BYVANCLK/bimj.html:text/html},
}

@article{daniel_making_2021,
	title = {Making apples from oranges: {Comparing} noncollapsible effect estimators and their standard errors after adjustment for different covariate sets},
	volume = {63},
	issn = {1521-4036},
	shorttitle = {Making apples from oranges},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.201900297},
	doi = {10.1002/bimj.201900297},
	abstract = {We revisit the well-known but often misunderstood issue of (non)collapsibility of effect measures in regression models for binary and time-to-event outcomes. We describe an existing simple but largely ignored procedure for marginalizing estimates of conditional odds ratios and propose a similar procedure for marginalizing estimates of conditional hazard ratios (allowing for right censoring), demonstrating its performance in simulation studies and in a reanalysis of data from a small randomized trial in primary biliary cirrhosis patients. In addition, we aim to provide an educational summary of issues surrounding (non)collapsibility from a causal inference perspective and to promote the idea that the words conditional and adjusted (likewise marginal and unadjusted) should not be used interchangeably.},
	language = {en},
	number = {3},
	urldate = {2021-09-14},
	journal = {Biometrical Journal},
	author = {Daniel, Rhian and Zhang, Jingjing and Farewell, Daniel},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201900297},
	keywords = {covariate adjustment, Cox proportional hazards regression, logistic regression, noncollapsibility},
	pages = {528--557},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/IE77RHR6/Daniel et al. - 2021 - Making apples from oranges Comparing noncollapsib.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/A4279F5V/bimj.html:text/html},
}

@article{martinussen_collapsibility_2013,
	title = {On collapsibility and confounding bias in {Cox} and {Aalen} regression models},
	volume = {19},
	issn = {1572-9249},
	url = {https://doi.org/10.1007/s10985-013-9242-z},
	doi = {10.1007/s10985-013-9242-z},
	abstract = {We study the situation where it is of interest to estimate the effect of an exposure variable \$\$X\$\$on a survival time response \$\$T\$\$in the presence of confounding by measured variables \$\$Z\$\$. Quantifying the amount of confounding is complicated by the non-collapsibility or non-linearity of typical effect measures in survival analysis: survival analyses with or without adjustment for \$\$Z\$\$typically infer different effect estimands of a different magnitude, even when \$\$Z\$\$is not associated with the exposure, and henceforth not a confounder of the association between exposure and survival time. We show that, interestingly, the exposure coefficient indexing the Aalen additive hazards model is not subject to such non-collapsibility, unlike the corresponding coefficient indexing the Cox model, so that simple measures of the amount of confounding bias are obtainable for the Aalen hazards model, but not for the Cox model. We argue that various other desirable properties can be ascribed to the Aalen model as a result of this collapsibility. This work generalizes recent work by Janes et al. (Biostatistics 11:572–582, 2010).},
	language = {en},
	number = {3},
	urldate = {2021-09-12},
	journal = {Lifetime Data Analysis},
	author = {Martinussen, Torben and Vansteelandt, Stijn},
	month = jul,
	year = {2013},
	pages = {279--296},
	file = {Springer Full Text PDF:/home/imbroglio/Zotero/storage/FBGJNIQL/Martinussen and Vansteelandt - 2013 - On collapsibility and confounding bias in Cox and .pdf:application/pdf},
}

@article{laan_estimating_2013,
	title = {Estimating the {Effect} of a {Community}-{Based} {Intervention} with {Two} {Communities}},
	volume = {1},
	issn = {2193-3685},
	url = {https://www.degruyter.com/document/doi/10.1515/jci-2012-0011/html},
	doi = {10.1515/jci-2012-0011},
	abstract = {Due to the need to evaluate the effectiveness of community-based programs in practice, there is substantial interest in methods to estimate the causal effects of community-level treatments or exposures on individual level outcomes. The challenge one is confronted with is that different communities have different environmental factors affecting the individual outcomes, and all individuals in a community share the same environment and intervention. In practice, data are often available from only a small number of communities, making it difficult if not impossible to adjust for these environmental confounders. In this paper we consider an extreme version of this dilemma, in which two communities each receives a different level of the intervention, and covariates and outcomes are measured on a random sample of independent individuals from each of the two populations; the results presented can be straightforwardly generalized to settings in which more than two communities are sampled. We address the question of what conditions are needed to estimate the causal effect of the intervention, defined in terms of an ideal experiment in which the exposed level of the intervention is assigned to both communities and individual outcomes are measured in the combined population, and then the clock is turned back and a control level of the intervention is assigned to both communities and individual outcomes are measured in the combined population. We refer to the difference in the expectation of these outcomes as the marginal (overall) treatment effect. We also discuss conditions needed for estimation of the treatment effect on the treated community. We apply a nonparametric structural equation model to define these causal effects and to establish conditions under which they are identified. These identifiability conditions provide guidance for the design of studies to investigate community level causal effects and for assessing the validity of causal interpretations when data are only available from a few communities. When the identifiability conditions fail to hold, the proposed statistical parameters still provide nonparametric treatment effect measures (albeit non-causal) whose statistical interpretations do not depend on model specifications. In addition, we study the use of a matched cohort sampling design in which the units of different communities are matched on individual factors. Finally, we provide semiparametric efficient and doubly robust targeted MLE estimators of the community level causal effect based on i.i.d. sampling and matched cohort sampling.},
	language = {en},
	number = {1},
	urldate = {2021-09-08},
	journal = {Journal of Causal Inference},
	author = {Laan, Mark J. van der and Petersen, Maya and Zheng, Wenjing},
	month = may,
	year = {2013},
	note = {Publisher: De Gruyter},
	keywords = {causal effect, efficient influence curve, causal effect among the treated, community-based intervention, environmental confounding},
	pages = {83--106},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/8KTPG9P2/Laan et al. - 2013 - Estimating the Effect of a Community-Based Interve.pdf:application/pdf},
}

@article{zhang_tmlecommunity_2020,
	title = {{tmleCommunity}: {A} {R} {Package} {Implementing} {Target} {Maximum} {Likelihood} {Estimation} for {Community}-level {Data}},
	shorttitle = {{tmleCommunity}},
	url = {http://arxiv.org/abs/2006.08553},
	abstract = {Over the past years, many applications aim to assess the causal effect of treatments assigned at the community level, while data are still collected at the individual level among individuals of the community. In many cases, one wants to evaluate the effect of a stochastic intervention on the community, where all communities in the target population receive probabilistically assigned treatments based on a known specified mechanism (e.g., implementing a community-level intervention policy that target stochastic changes in the behavior of a target population of communities). The tmleCommunity package is recently developed to implement targeted minimum loss-based estimation (TMLE) of the effect of community-level intervention(s) at a single time point on an individual-based outcome of interest, including the average causal effect. Implementations of the inverse-probability-of-treatment-weighting (IPTW) and the G-computation formula (GCOMP) are also available. The package supports multivariate arbitrary (i.e., static, dynamic or stochastic) interventions with a binary or continuous outcome. Besides, it allows user-specified data-adaptive machine learning algorithms through SuperLearner, sl3 and h2oEnsemble packages. The usage of the tmleCommunity package, along with a few examples, will be described in this paper.},
	urldate = {2021-09-08},
	journal = {arXiv:2006.08553 [stat]},
	author = {Zhang, Chi and Ahern, Jennifer and van der Laan, Mark J. and Sofrygin, Oleg},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.08553},
	keywords = {Statistics - Applications, Statistics - Computation},
	annote = {Comment: 42 pages},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/JA4B3QP3/Zhang et al. - 2020 - tmleCommunity A R Package Implementing Target Max.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/HU98BGPH/2006.html:text/html},
}

@article{diggle_informative_1994,
	title = {Informative {Drop}-{Out} in {Longitudinal} {Data} {Analysis}},
	volume = {43},
	issn = {0035-9254},
	url = {https://www.jstor.org/stable/2986113},
	doi = {10.2307/2986113},
	abstract = {A model is proposed for continuous longitudinal data with non-ignorable or informative drop-out (ID). The model combines a multivariate linear model for the underlying response with a logistic regression model for the drop-out process. The latter incorporates dependence of the probability of drop-out on unobserved, or missing, observations. Parameters in the model are estimated by using maximum likelihood (ML) and inferences drawn through conventional likelihood procedures. In particular, likelihood ratio tests can be used to assess the informativeness of the drop-out process through comparison of the full model with reduced models corresponding to random drop-out (RD) and completely random processes. A simulation study is used to assess the procedure in two settings: the comparison of time trends under a linear regression model with autocorrelated errors and the estimation of period means and treatment differences from a four-period four-treatment crossover trial. It is seen in both settings that, when data are generated under an ID process, the ML estimators from the ID model do not suffer from the bias that is present in the ordinary least squares and RD ML estimators. The approach is then applied to three examples. These derive from a milk protein trial involving three groups of cows, milk yield data from a study of mastitis in dairy cattle and data from a multicentre clinical trial on the study of depression. All three examples provide evidence of an underlying ID process, two with some strength. It is seen that the assumption of an ID rather than an RD process has practical implications for the interpretation of the data.},
	number = {1},
	urldate = {2021-09-07},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Diggle, P. and Kenward, M. G.},
	year = {1994},
	note = {Publisher: [Wiley, Royal Statistical Society]},
	pages = {49--93},
}

@article{ishwaran_random_2008,
	title = {Random survival forests},
	volume = {2},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-3/Random-survival-forests/10.1214/08-AOAS169.full},
	doi = {10.1214/08-AOAS169},
	abstract = {We introduce random survival forests, a random forests method for the analysis of right-censored survival data. New survival splitting rules for growing survival trees are introduced, as is a new missing data algorithm for imputing missing data. A conservation-of-events principle for survival forests is introduced and used to define ensemble mortality, a simple interpretable measure of mortality that can be used as a predicted outcome. Several illustrative examples are given, including a case study of the prognostic implications of body mass for individuals with coronary artery disease. Computations for all examples were implemented using the freely available R-software package, randomSurvivalForest.},
	number = {3},
	urldate = {2021-09-03},
	journal = {The Annals of Applied Statistics},
	author = {Ishwaran, Hemant and Kogalur, Udaya B. and Blackstone, Eugene H. and Lauer, Michael S.},
	month = sep,
	year = {2008},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Conservation of events, cumulative hazard function, ensemble, out-of-bag, prediction error, survival tree},
	pages = {841--860},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/TSV3ZCG8/Ishwaran et al. - 2008 - Random survival forests.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/GRYW97X2/08-AOAS169.html:text/html},
}

@article{stitelman_targeted_2011,
	title = {Targeted {Maximum} {Likelihood} {Estimation} of {Effect} {Modification} {Parameters} in {Survival} {Analysis}},
	volume = {7},
	issn = {1557-4679},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3083138/},
	doi = {10.2202/1557-4679.1307},
	abstract = {The Cox proportional hazards model or its discrete time analogue, the logistic failure time model, posit highly restrictive parametric models and attempt to estimate parameters which are specific to the model proposed. These methods are typically implemented when assessing effect modification in survival analyses despite their flaws. The targeted maximum likelihood estimation (TMLE) methodology is more robust than the methods typically implemented and allows practitioners to estimate parameters that directly answer the question of interest. TMLE will be used in this paper to estimate two newly proposed parameters of interest that quantify effect modification in the time to event setting. These methods are then applied to the Tshepo study to assess if either gender or baseline CD4 level modify the effect of two cART therapies of interest, efavirenz (EFV) and nevirapine (NVP), on the progression of HIV. The results show that women tend to have more favorable outcomes using EFV while males tend to have more favorable outcomes with NVP. Furthermore, EFV tends to be favorable compared to NVP for individuals at high CD4 levels.},
	number = {1},
	urldate = {2021-09-03},
	journal = {The International Journal of Biostatistics},
	author = {Stitelman, Ori M and Wester, C. William and De Gruttola, Victor and van der Laan, Mark J.},
	month = mar,
	year = {2011},
	pmid = {21556287},
	pmcid = {PMC3083138},
	pages = {19},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/EZ3QNXN3/Stitelman et al. - 2011 - Targeted Maximum Likelihood Estimation of Effect M.pdf:application/pdf},
}

@article{austin_practical_2017,
	title = {Practical recommendations for reporting {Fine}-{Gray} model analyses for competing risk data},
	volume = {36},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7501},
	doi = {10.1002/sim.7501},
	abstract = {In survival analysis, a competing risk is an event whose occurrence precludes the occurrence of the primary event of interest. Outcomes in medical research are frequently subject to competing risks. In survival analysis, there are 2 key questions that can be addressed using competing risk regression models: first, which covariates affect the rate at which events occur, and second, which covariates affect the probability of an event occurring over time. The cause-specific hazard model estimates the effect of covariates on the rate at which events occur in subjects who are currently event-free. Subdistribution hazard ratios obtained from the Fine-Gray model describe the relative effect of covariates on the subdistribution hazard function. Hence, the covariates in this model can also be interpreted as having an effect on the cumulative incidence function or on the probability of events occurring over time. We conducted a review of the use and interpretation of the Fine-Gray subdistribution hazard model in articles published in the medical literature in 2015. We found that many authors provided an unclear or incorrect interpretation of the regression coefficients associated with this model. An incorrect and inconsistent interpretation of regression coefficients may lead to confusion when comparing results across different studies. Furthermore, an incorrect interpretation of estimated regression coefficients can result in an incorrect understanding about the magnitude of the association between exposure and the incidence of the outcome. The objective of this article is to clarify how these regression coefficients should be reported and to propose suggestions for interpreting these coefficients.},
	language = {en},
	number = {27},
	urldate = {2021-08-25},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C. and Fine, Jason P.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7501},
	keywords = {survival analysis, competing risks, cumulative incidence function, subdistribution hazard model},
	pages = {4391--4400},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/PGUF7YRC/Austin and Fine - 2017 - Practical recommendations for reporting Fine-Gray .pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/P2Y77HPN/sim.html:text/html},
}

@article{austin_accounting_2017,
	title = {Accounting for competing risks in randomized controlled trials: a review and recommendations for improvement},
	volume = {36},
	issn = {1097-0258},
	shorttitle = {Accounting for competing risks in randomized controlled trials},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7215},
	doi = {10.1002/sim.7215},
	abstract = {In studies with survival or time-to-event outcomes, a competing risk is an event whose occurrence precludes the occurrence of the primary event of interest. Specialized statistical methods must be used to analyze survival data in the presence of competing risks. We conducted a review of randomized controlled trials with survival outcomes that were published in high-impact general medical journals. Of 40 studies that we identified, 31 (77.5\%) were potentially susceptible to competing risks. However, in the majority of these studies, the potential presence of competing risks was not accounted for in the statistical analyses that were described. Of the 31 studies potentially susceptible to competing risks, 24 (77.4\%) reported the results of a Kaplan–Meier survival analysis, while only five (16.1\%) reported using cumulative incidence functions to estimate the incidence of the outcome over time in the presence of competing risks. The former approach will tend to result in an overestimate of the incidence of the outcome over time, while the latter approach will result in unbiased estimation of the incidence of the primary outcome over time. We provide recommendations on the analysis and reporting of randomized controlled trials with survival outcomes in the presence of competing risks. © 2017 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
	language = {en},
	number = {8},
	urldate = {2021-08-25},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C. and Fine, Jason P.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7215},
	keywords = {survival analysis, competing risks, randomized controlled trial, RCT, systematic review},
	pages = {1203--1209},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/CE3LR5KC/Austin and Fine - 2017 - Accounting for competing risks in randomized contr.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/EY7P76X9/sim.html:text/html},
}

@article{sparapani_nonparametric_2016,
	title = {Nonparametric survival analysis using {Bayesian} {Additive} {Regression} {Trees} ({BART})},
	volume = {35},
	issn = {0277-6715},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4899272/},
	doi = {10.1002/sim.6893},
	abstract = {Bayesian additive regression trees (BART) provide a framework for flexible nonparametric modeling of relationships of covariates to outcomes. Recently, BART models have been shown to provide excellent predictive performance, for both continuous and binary outcomes, and exceeding that of its competitors. Software is also readily available for such outcomes. In this article we introduce modeling that extends the usefulness of BART in medical applications by addressing needs arising in survival analysis. Simulation studies of one-sample and two-sample scenarios, in comparison with long-standing traditional methods, establish face validity of the new approach. We then demonstrate the model’s ability to accommodate data from complex regression models with a simulation study of a nonproportional hazards scenario with crossing survival functions, and survival function estimation in a scenario where hazards are multiplicatively modified by a highly nonlinear function of the covariates. Using data from a recently published study of patients undergoing hematopoietic stem cell transplantation, we illustrate the use and some advantages of the proposed method in medical investigations.},
	number = {16},
	urldate = {2021-08-25},
	journal = {Statistics in medicine},
	author = {Sparapani, Rodney A. and Logan, Brent R. and McCulloch, Robert E. and Laud, Purushottam W.},
	month = jul,
	year = {2016},
	pmid = {26854022},
	pmcid = {PMC4899272},
	pages = {2741--2753},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/QU4M3MWE/Sparapani et al. - 2016 - Nonparametric survival analysis using Bayesian Add.pdf:application/pdf},
}

@article{stitelman_general_2012,
	title = {A {General} {Implementation} of {TMLE} for {Longitudinal} {Data} {Applied} to {Causal} {Inference} in {Survival} {Analysis}},
	volume = {8},
	issn = {1557-4679},
	url = {https://www.degruyter.com/view/j/ijb.2012.8.issue-1/1557-4679.1334/1557-4679.1334.xml},
	doi = {10.1515/1557-4679.1334},
	abstract = {In many randomized controlled trials the outcome of interest is a time to event, and one measures on each subject baseline covariates and time-dependent covariates until the subject either drops-out, the time to event is observed, or the end of study is reached. The goal of such a study is to assess the causal effect of the treatment on the survival curve. We present a targeted maximum likelihood estimator of the causal effect of treatment on survival fully utilizing all the available covariate information, resulting in a double robust locally efficient substitution estimator that will be consistent and asymptotically linear if either the censoring mechanism is consistently estimated, or if the maximum likelihood based estimator is already consistent. In particular, under the independent censoring assumption assumed by current methods, this TMLE is always consistent and asymptotically linear so that it provides valid confidence intervals and tests. Furthermore, we show that when both the censoring mechanism and the initial maximum likelihood based estimator are mis-specified, and thus inconsistent, the TMLE exhibits stability when inverse probability weighted estimators and double robust estimating equation based methods break down The TMLE is used to analyze the Tshepo study, a study designed to evaluate the efficacy, tolerability, and development of drug resistance of six different first-line antiretroviral therapies. Most importantly this paper presents a general algorithm that may be used to create targeted maximum likelihood estimators of a large class of parameters of interest for general longitudinal data structures.},
	number = {1},
	urldate = {2020-02-14},
	journal = {The International Journal of Biostatistics},
	author = {Stitelman, Ori M. and De, Gruttola Victor and van, der Laan Mark J.},
	year = {2012},
	keywords = {causal inference, targeted maximum likelihood estimation, survival analysis, double robust, informative censoring, time-dependent covariates},
	file = {Full Text:/home/imbroglio/Zotero/storage/FRQDGN47/Stitelman et al. - 2012 - A General Implementation of TMLE for Longitudinal .pdf:application/pdf;Full Text PDF:/home/imbroglio/Zotero/storage/RCJHUQ7P/E9-R1_Step4_Guideline_2019_1203.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/F7RGMDNE/html.html:text/html},
}

@article{diaz_improved_2019,
	title = {Improved precision in the analysis of randomized trials with survival outcomes, without assuming proportional hazards},
	volume = {25},
	issn = {1380-7870, 1572-9249},
	url = {http://link.springer.com/10.1007/s10985-018-9428-5},
	doi = {10.1007/s10985-018-9428-5},
	language = {en},
	number = {3},
	urldate = {2021-08-22},
	journal = {Lifetime Data Analysis},
	author = {Díaz, Iván and Colantuoni, Elizabeth and Hanley, Daniel F. and Rosenblum, Michael},
	month = jul,
	year = {2019},
	pages = {439--468},
	file = {Submitted Version:/home/imbroglio/Zotero/storage/2HQIGVQQ/Díaz et al. - 2019 - Improved precision in the analysis of randomized t.pdf:application/pdf;Submitted Version:/home/imbroglio/Zotero/storage/XZ828VML/Díaz et al. - 2019 - Improved precision in the analysis of randomized t.pdf:application/pdf},
}

@article{zhu_targeted_2020,
	title = {Targeted estimation of heterogeneous treatment effect in observational survival analysis},
	volume = {107},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046420301039},
	doi = {10.1016/j.jbi.2020.103474},
	abstract = {The aim of clinical effectiveness research using repositories of electronic health records is to identify what health interventions ‘work best’ in real-world settings. Since there are several reasons why the net benefit of intervention may differ across patients, current comparative effectiveness literature focuses on investigating heterogeneous treatment effect and predicting whether an individual might benefit from an intervention. The majority of this literature has concentrated on the estimation of the effect of treatment on binary outcomes. However, many medical interventions are evaluated in terms of their effect on future events, which are subject to loss to follow-up. In this study, we describe a framework for the estimation of heterogeneous treatment effect in terms of differences in time-to-event (survival) probabilities. We divide the problem into three phases: (1) estimation of treatment effect conditioned on unique sets of the covariate vector; (2) identification of features important for heterogeneity using non-parametric variable importance methods; and (3) estimation of treatment effect on the reference classes defined by the previously selected features, using one-step Targeted Maximum Likelihood Estimation. We conducted a series of simulation studies and found that this method performs well when either sample size or event rate is high enough and the number of covariates contributing to the effect heterogeneity is moderate. An application of this method to a clinical case study was conducted by estimating the effect of oral anticoagulants on newly diagnosed non-valvular atrial fibrillation patients using data from the UK Clinical Practice Research Datalink.},
	language = {en},
	urldate = {2021-08-25},
	journal = {Journal of Biomedical Informatics},
	author = {Zhu, Jie and Gallego, Blanca},
	month = jul,
	year = {2020},
	keywords = {Survival analysis, Heterogeneous treatment effect, Machine learning, Oral anticoagulants, Targeted maximum likelihood estimation},
	pages = {103474},
	file = {ScienceDirect Full Text PDF:/home/imbroglio/Zotero/storage/2T4PFNNZ/Zhu and Gallego - 2020 - Targeted estimation of heterogeneous treatment eff.pdf:application/pdf},
}

@article{torres_attributable_2021,
	title = {Attributable mortality of acute respiratory distress syndrome: a systematic review, meta-analysis and survival analysis using targeted minimum loss-based estimation},
	copyright = {© Author(s) (or their employer(s)) 2021. No commercial re-use. See rights and permissions. Published by BMJ.},
	issn = {0040-6376, 1468-3296},
	shorttitle = {Attributable mortality of acute respiratory distress syndrome},
	url = {https://thorax.bmj.com/content/early/2021/04/15/thoraxjnl-2020-215950},
	doi = {10.1136/thoraxjnl-2020-215950},
	abstract = {Background Although acute respiratory distress syndrome (ARDS) is associated with high mortality, its direct causal link with death is unclear. Clarifying this link is important to justify costly research on prevention of ARDS.
Objective To estimate the attributable mortality, if any, of ARDS.
Design First, we performed a systematic review and meta-analysis of observational studies reporting mortality of critically ill patients with and without ARDS matched for underlying risk factor. Next, we conducted a survival analysis of prospectively collected patient-level data from subjects enrolled in three intensive care unit (ICU) cohorts to estimate the attributable mortality of critically ill septic patients with and without ARDS using a novel causal inference method.
Results In the meta-analysis, 44 studies (47 cohorts) involving 56 081 critically ill patients were included. Mortality was higher in patients with versus without ARDS (risk ratio 2.48, 95\% CI 1.86 to 3.30; p{\textless}0.001) with a numerically stronger association between ARDS and mortality in trauma than sepsis. In the survival analysis of three ICU cohorts enrolling 1203 critically ill patients, 658 septic patients were included. After controlling for confounders, ARDS was found to increase the mortality rate by 15\% (95\% CI 3\% to 26\%; p=0.015). Significant increases in mortality were seen for severe (23\%, 95\% CI 3\% to 44\%; p=0.028) and moderate (16\%, 95\% CI 2\% to 31\%; p=0.031), but not for mild ARDS.
Conclusions ARDS has a direct causal link with mortality. Our findings provide information about the extent to which continued funding of ARDS prevention trials has potential to impart survival benefit.
PROSPERO Registration Number CRD42017078313},
	language = {en},
	urldate = {2021-08-25},
	journal = {Thorax},
	author = {Torres, Lisa K. and Hoffman, Katherine L. and Oromendia, Clara and Diaz, Ivan and Harrington, John S. and Schenck, Edward J. and Price, David R. and Gomez-Escobar, Luis and Higuera, Angelica and Vera, Mayra Pinilla and Baron, Rebecca M. and Fredenburgh, Laura E. and Huh, Jin-Won and Choi, Augustine M. K. and Siempos, Ilias I.},
	month = apr,
	year = {2021},
	pmid = {33863829},
	note = {Publisher: BMJ Publishing Group Ltd
Section: Critical care},
	keywords = {ARDS, clinical epidemiology, critical care},
	file = {Snapshot:/home/imbroglio/Zotero/storage/MRCQKNGK/thoraxjnl-2020-215950.html:text/html},
}

@article{murray_causal_2021,
	title = {Causal survival analysis: {A} guide to estimating intention-to-treat and per-protocol effects from randomized clinical trials with non-adherence},
	volume = {2},
	issn = {2632-0843},
	shorttitle = {Causal survival analysis},
	url = {https://doi.org/10.1177/2632084320961043},
	doi = {10.1177/2632084320961043},
	abstract = {When reporting results from randomized experiments, researchers often choose to present a per-protocol effect in addition to an intention-to-treat effect. However, these per-protocol effects are often described retrospectively, for example, comparing outcomes among individuals who adhered to their assigned treatment strategy throughout the study. This retrospective definition of a per-protocol effect is often confounded and cannot be interpreted causally because it encounters treatment-confounder feedback loops, where past confounders affect future treatment, and current treatment affects future confounders. Per-protocol effects estimated using this method are highly susceptible to the placebo paradox, also called the “healthy adherers” bias, where individuals who adhere to placebo appear to have better survival than those who don’t. This result is generally not due to a benefit of placebo, but rather is most often the result of uncontrolled confounding. Here, we aim to provide an overview to causal inference for survival outcomes with time-varying exposures for static interventions using inverse probability weighting. The basic concepts described here can also apply to other types of exposure strategies, although these may require additional design or analytic considerations. We provide a workshop guide with solutions manual, fully reproducible R, SAS, and Stata code, and a simulated dataset on a GitHub repository for the reader to explore.},
	language = {en},
	number = {1},
	urldate = {2021-08-25},
	journal = {Research Methods in Medicine \& Health Sciences},
	author = {Murray, Eleanor J and Caniglia, Ellen C and Petito, Lucia C},
	month = jan,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Causal inference, clinical trials, intention-to-treat effects, per-protocol effects},
	pages = {39--49},
	file = {SAGE PDF Full Text:/home/imbroglio/Zotero/storage/ENCSJIRK/Murray et al. - 2021 - Causal survival analysis A guide to estimating in.pdf:application/pdf},
}

@article{kerschberger_impact_2021,
	title = {The {Impact} of {Same}-{Day} {Antiretroviral} {Therapy} {Initiation} {Under} the {World} {Health} {Organization} {Treat}-{All} {Policy}},
	volume = {190},
	issn = {0002-9262},
	url = {https://academic.oup.com/aje/article/190/8/1519/6133987},
	doi = {10.1093/aje/kwab032},
	abstract = {Abstract. Rapid initiation of antiretroviral therapy (ART) is recommended for people living with human immunodeficiency virus (HIV), with the option to start tr},
	language = {en},
	number = {8},
	urldate = {2021-08-25},
	journal = {American Journal of Epidemiology},
	author = {Kerschberger, Bernhard and Boulle, Andrew and Kuwengwa, Rudo and Ciglenecki, Iza and Schomaker, Michael},
	month = aug,
	year = {2021},
	note = {Publisher: Oxford Academic},
	pages = {1519--1532},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/NEPHKD5K/Kerschberger et al. - 2021 - The Impact of Same-Day Antiretroviral Therapy Init.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/62GU8BUV/6133987.html:text/html},
}

@article{hu_estimating_2021,
	title = {Estimating heterogeneous survival treatment effect in observational data using machine learning},
	volume = {40},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9090},
	doi = {10.1002/sim.9090},
	abstract = {Methods for estimating heterogeneous treatment effect in observational data have largely focused on continuous or binary outcomes, and have been relatively less vetted with survival outcomes. Using flexible machine learning methods in the counterfactual framework is a promising approach to address challenges due to complex individual characteristics, to which treatments need to be tailored. To evaluate the operating characteristics of recent survival machine learning methods for the estimation of treatment effect heterogeneity and inform better practice, we carry out a comprehensive simulation study presenting a wide range of settings describing confounded heterogeneous survival treatment effects and varying degrees of covariate overlap. Our results suggest that the nonparametric Bayesian Additive Regression Trees within the framework of accelerated failure time model (AFT-BART-NP) consistently yields the best performance, in terms of bias, precision, and expected regret. Moreover, the credible interval estimators from AFT-BART-NP provide close to nominal frequentist coverage for the individual survival treatment effect when the covariate overlap is at least moderate. Including a nonparametrically estimated propensity score as an additional fixed covariate in the AFT-BART-NP model formulation can further improve its efficiency and frequentist coverage. Finally, we demonstrate the application of flexible causal machine learning estimators through a comprehensive case study examining the heterogeneous survival effects of two radiotherapy approaches for localized high-risk prostate cancer.},
	language = {en},
	number = {21},
	urldate = {2021-08-25},
	journal = {Statistics in Medicine},
	author = {Hu, Liangyuan and Ji, Jiayi and Li, Fan},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.9090},
	keywords = {causal inference, machine learning, Bayesian additive regression trees, observational studies, survival treatment effect heterogeneity},
	pages = {4691--4713},
	file = {Snapshot:/home/imbroglio/Zotero/storage/UK7A626P/sim.html:text/html;Submitted Version:/home/imbroglio/Zotero/storage/Y8AZXNTF/Hu et al. - 2021 - Estimating heterogeneous survival treatment effect.pdf:application/pdf},
}

@article{cao_time--event_2019,
	title = {Time-to-event estimation by re-defining time},
	volume = {100},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S153204641930245X},
	doi = {10.1016/j.jbi.2019.103326},
	abstract = {The primary goal of a time-to-event estimation model is to accurately infer the occurrence time of a target event. Most existing studies focus on developing new models to effectively utilize the information in the censored observations. In this paper, we propose a model to tackle the time-to-event estimation problem from a completely different perspective. Our model relaxes a fundamental constraint that the target variable, time, is a univariate number which satisfies a partial order. Instead, the proposed model interprets each event occurrence time as a time concept with a vector representation. We hypothesize that the model will be more accurate and interpretable by capturing (1) the relationships between features and time concept vectors and (2) the relationships among time concept vectors. We also propose a scalable framework to simultaneously learn the model parameters and time concept vectors. Rigorous experiments and analysis have been conducted in medical event prediction task on seven gene expression datasets. The results demonstrate the efficiency and effectiveness of the proposed model. Furthermore, similarity information among time concept vectors helped in identifying time regimes, thus leading to a potential knowledge discovery related to the human cancer considered in our experiments.},
	language = {en},
	urldate = {2021-08-25},
	journal = {Journal of Biomedical Informatics},
	author = {Cao, Xi Hang and Han, Chao and Glass, Lucas M. and Kindman, Allen and Obradovic, Zoran},
	month = dec,
	year = {2019},
	keywords = {Survival analysis, Concept embedding, Knowledge discovery, Regime identification, Representation learning, Time-to-event estimation},
	pages = {103326},
	file = {ScienceDirect Full Text PDF:/home/imbroglio/Zotero/storage/ASMIA4SS/Cao et al. - 2019 - Time-to-event estimation by re-defining time.pdf:application/pdf},
}

@techreport{gallego_time--event_2021,
	type = {preprint},
	title = {Time-to-event comparative effectiveness of {NOACs} vs {VKAs} in newly diagnosed non-valvular atrial fibrillation patients},
	url = {http://medrxiv.org/lookup/doi/10.1101/2021.08.06.21261092},
	abstract = {Objective To investigate the difference in the time-to-event probabilities of ischaemic events, major bleeding and death of NOAC vs VKAs in newly diagnosed non-valvular atrial ﬁbrillation patients. Design Retrospective observational cohort study. Setting UK’s Clinical Practice Research Data linked to the Hospital Episode Statistics inpatient and outpatient data, mortality data and the Patient Level Index of Multiple Deprivation. Participants Patients over 18 years of age, with an initial diagnosis of atrial ﬁbrillation between 1st-Mar-2011 and 31-July-2017, without a record for a valve condition, prosthesis or procedure previous to initial diagnosis, and without a record of oral anticoagulant treatment in the previous year. Intervention Oral anticoagulant treatment with either vitamin K antagonists (VKAs) or the newer target-speciﬁc oral anticoagulants (NOACs). Main Outcome Measures Ischaemic event, major bleeding event and death from 15 days from initial prescription up to two years follow-up. Statistical Analysis Treatment effect was deﬁned as the difference in time-to-event probability between NOAC and VKA treatment groups. Treatment and outcomes were modelled using an ensemble of parametric and non-parametric models, and the average and conditional average treatment effects were estimated using one-step Targeted Maximum Likelihood Estimation (TMLE). Heterogeneity of treatment effect was examined using variable importance methods in Bayesian Additive Regression Trees (BART).
Results The average treatment effect of NOAC vs VKA was consistently close to zero across all times, with a temporal average of 0.00[95\%0.00, 0.00] for ischaemic event, 0.00\%[95\% − 0.01, 0.01] for major bleeding and 0.00[95\% − 0.01, 0.01] for death. Only history of major bleeding was found to inﬂuence the distribution of treatment effect for major bleeding, but its impact on the associated conditional average treatment effect was not signiﬁcant.
Conclusions This study found no statistically signiﬁcant difference between NOAC and VKA users up to two years of medication use for the prevention of ischaemic events, major bleeding or death.},
	language = {en},
	urldate = {2021-08-25},
	institution = {Epidemiology},
	author = {Gallego, Blanca and Zhu, Jie},
	month = aug,
	year = {2021},
	doi = {10.1101/2021.08.06.21261092},
	file = {Gallego and Zhu - 2021 - Time-to-event comparative effectiveness of NOACs v.pdf:/home/imbroglio/Zotero/storage/ACR26BNC/Gallego and Zhu - 2021 - Time-to-event comparative effectiveness of NOACs v.pdf:application/pdf},
}

@article{brooks_targeted_2013,
	title = {Targeted {Minimum} {Loss}-{Based} {Estimation} of {Causal} {Effects} in {Right}-{Censored} {Survival} {Data} with {Time}-{Dependent} {Covariates}: {Warfarin}, {Stroke}, and {Death} in {Atrial} {Fibrillation}},
	volume = {1},
	issn = {2193-3685},
	shorttitle = {Targeted {Minimum} {Loss}-{Based} {Estimation} of {Causal} {Effects} in {Right}-{Censored} {Survival} {Data} with {Time}-{Dependent} {Covariates}},
	url = {https://www.degruyter.com/document/doi/10.1515/jci-2013-0001/html},
	doi = {10.1515/jci-2013-0001},
	abstract = {Causal effects in right-censored survival data can be formally defined as the difference in the marginal cumulative event probabilities under particular interventions. Conventional estimators, such as the Kaplan-Meier (KM), fail to consistently estimate these marginal parameters under dependent treatment assignment or dependent censoring. Several modern estimators have been developed that reduce bias under both dependent treatment assignment and dependent censoring by incorporating information from baseline and time-dependent covariates. In the present article we describe a recently developed targeted minimum loss-based estimation (TMLE) algorithm for general longitudinal data structures and present in detail its application in right-censored survival data with time-dependent covariates. The treatment-specific marginal cumulative event probability is defined via a series of iterated conditional expectations in a time-dependent counting process framework. The TMLE involves an initial estimator of each conditional expectation and sequentially updates these such that the resulting estimator solves the efficient influence curve estimating equation in the nonparametric statistical model. We describe the assumptions required for consistent estimation of statistical parameters and additional assumptions required for consistent estimation of the causal effect parameter. Using simulated right-censored survival data, the mean squared error, bias, and 95\% confidence interval coverage probability of the TMLE is compared with those of the conventional KM and the inverse probability of censoring weight estimating equation, conventional maximum likelihood substitution estimator, and the double robustaugmented inverse probability of censoring weighted estimating equation. We conclude the article with estimation of the causal effect of warfarin medical therapy on the probability of “stroke or death” within a 1-year time frame using data from the ATRIA-1 observational cohort of persons with atrial fibrillation. Our results suggest that a fixed policy of warfarin treatment for all patients would result in 2\% fewer deaths or strokes within 1-year as compared with a policy of withholding warfarin from all patients.},
	language = {en},
	number = {2},
	urldate = {2021-08-25},
	journal = {Journal of Causal Inference},
	author = {Brooks, Jordan C. and Laan, Mark J. van der and Singer, Daniel E. and Go, Alan S.},
	month = nov,
	year = {2013},
	note = {Publisher: De Gruyter
Section: Journal of Causal Inference},
	pages = {235--254},
	file = {Snapshot:/home/imbroglio/Zotero/storage/5KL8RA8S/html.html:text/html},
}

@article{osman_nonparametric_2012,
	title = {Nonparametric regression models for right-censored data using {Bernstein} polynomials},
	volume = {56},
	issn = {0167-9473},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947311003185},
	doi = {10.1016/j.csda.2011.08.019},
	abstract = {In some applications of survival analysis with covariates, the commonly used semiparametric assumptions (e.g., proportional hazards) may turn out to be stringent and unrealistic, particularly when there is scientific background to believe that survival curves under different covariate combinations will cross during the study period. We present a new nonparametric regression model for the conditional hazard rate using a suitable sieve of Bernstein polynomials. The proposed nonparametric methodology has three key features: (i) the smooth estimator of the conditional hazard rate is shown to be a unique solution of a strictly convex optimization problem for a wide range of applications; making it computationally attractive, (ii) the model is shown to encompass a proportional hazards structure, and (iii) large sample properties including consistency and convergence rates are established under a set of mild regularity conditions. Empirical results based on several simulated data scenarios indicate that the proposed model has reasonably robust performance compared to other semiparametric models particularly when such semiparametric modeling assumptions are violated. The proposed method is further illustrated on the gastric cancer data and the Veterans Administration lung cancer data.},
	language = {en},
	number = {3},
	urldate = {2021-08-24},
	journal = {Computational Statistics \& Data Analysis},
	author = {Osman, Muhtarjan and Ghosh, Sujit K.},
	month = mar,
	year = {2012},
	keywords = {Nonproportional hazards, Bernstein polynomials, Censored data, Nonparametric regression, Sieve},
	pages = {559--573},
	file = {Osman and Ghosh - 2012 - Nonparametric regression models for right-censored.pdf:/home/imbroglio/Zotero/storage/VT2U4F4S/Osman and Ghosh - 2012 - Nonparametric regression models for right-censored.pdf:application/pdf;ScienceDirect Snapshot:/home/imbroglio/Zotero/storage/2Y6TNY6B/S0167947311003185.html:text/html;ScienceDirect Snapshot:/home/imbroglio/Zotero/storage/Q47Q4CW8/S0167947311003185.html:text/html},
}

@article{rytgaard_continuous-time_2021,
	title = {Continuous-time targeted minimum loss-based estimation of intervention-specific mean outcomes},
	url = {http://arxiv.org/abs/2105.02088},
	abstract = {This paper studies the generalization of the targeted minimum loss-based estimation (TMLE) framework to estimation of effects of time-varying interventions in settings where both interventions, covariates, and outcome can happen at subject-specific time-points on an arbitrarily fine time-scale. TMLE is a general template for constructing asymptotically linear substitution estimators for smooth low-dimensional parameters in infinite-dimensional models. Existing longitudinal TMLE methods are developed for data where observations are made on a discrete time-grid. We consider a continuous-time counting process model where intensity measures track the monitoring of subjects, and focus on a low-dimensional target parameter defined as the intervention-specific mean outcome at the end of follow-up. To construct our TMLE algorithm for the given statistical estimation problem we derive an expression for the efficient influence curve and represent the target parameter as a functional of intensities and conditional expectations. The high-dimensional nuisance parameters of our model are estimated and updated in an iterative manner according to separate targeting steps for the involved intensities and conditional expectations. The resulting estimator solves the efficient influence curve equation. We state a general efficiency theorem and describe a highly adaptive lasso estimator for nuisance parameters that allows us to establish asymptotic linearity and efficiency of our estimator under minimal conditions on the underlying statistical model.},
	urldate = {2021-08-24},
	journal = {arXiv:2105.02088 [math, stat]},
	author = {Rytgaard, Helene C. and Gerds, Thomas A. and van der Laan, Mark J.},
	month = may,
	year = {2021},
	note = {arXiv: 2105.02088},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory},
	annote = {Comment: 27 pages (excluding supplementary material), 1 figures},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/CBANU96G/Rytgaard et al. - 2021 - Continuous-time targeted minimum loss-based estima.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/IDR6CKI9/2105.html:text/html},
}

@article{han_semiparametric_nodate,
	title = {Semiparametric estimation of the nonmixture cure model with auxiliary survival information},
	volume = {n/a},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13450},
	doi = {10.1111/biom.13450},
	abstract = {With rapidly increasing data sources, statistical methods that make use of external information are gradually becoming popular tools in medical research. In this article, we efficiently synthesize the auxiliary survival information and propose a semiparametric estimation method for the combined empirical likelihood in the framework of the nonmixture cure model, to enhance inference about the associations between exposures and disease outcomes. The auxiliary survival probabilities from external sources are first summarized as unbiased estimation equations, which help produce more efficient estimates of the effects of interest and improve the prediction accuracy for the risk of the event. Then we develop a Bernstein-based sieve empirical likelihood method to estimate the parametric and nonparametric components simultaneously. Such an estimation procedure allows us to reduce the computation burden while preserving the shape constraint on the baseline distribution function. The resulting estimators for the true associations are strongly consistent and asymptotically normal. Instead of collecting substantial exposure data, the auxiliary survival information at multiple time points is incorporated, which further reduces the mean squared error of the estimators. This contributes to biomarker evaluation and treatment effect analysis within smaller studies. We show how to choose the number of auxiliary survival probabilities appropriately and provide a guideline for practical applications. Simulation studies demonstrate that the estimators enjoy large gains in efficiency. A melanoma dataset is analyzed for illustrating the methodology.},
	language = {en},
	number = {n/a},
	urldate = {2021-08-24},
	journal = {Biometrics},
	author = {Han, Bo and Keilegom, Ingrid Van and Wang, Xiaoguang},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13450},
	keywords = {biomarker evaluation, empirical likelihood, information synthesis, nonture cure model, sieve method},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/796DA7SP/Han et al. - Semiparametric estimation of the nonmixture cure m.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/K7SJ7WLA/biom.html:text/html},
}

@article{sparapani_nonparametric_2021,
	title = {Nonparametric {Machine} {Learning} and {Efficient} {Computation} with {Bayesian} {Additive} {Regression} {Trees}: {The} {BART} {R} {Package}},
	volume = {97},
	copyright = {Copyright (c) 2021 Rodney Sparapani, Charles Spanbauer, Robert McCulloch},
	issn = {1548-7660},
	shorttitle = {Nonparametric {Machine} {Learning} and {Efficient} {Computation} with {Bayesian} {Additive} {Regression} {Trees}},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v097i01},
	doi = {10.18637/jss.v097.i01},
	abstract = {In this article, we introduce the BART R package which is an acronym for Bayesian additive regression trees. BART is a Bayesian nonparametric, machine learning, ensemble predictive modeling method for continuous, binary, categorical and time-to-event outcomes. Furthermore, BART is a tree-based, black-box method which fits the outcome to an arbitrary random function, f , of the covariates. The BART technique is relatively computationally efficient as compared to its competitors, but large sample sizes can be demanding. Therefore, the BART package includes efficient state-of-the-art implementations for continuous, binary, categorical and time-to-event outcomes that can take advantage of modern off-the-shelf hardware and software multi-threading technology. The BART package is written in C++ for both programmer and execution efficiency. The BART package takes advantage of multi-threading via forking as provided by the parallel package and OpenMP when available and supported by the platform. The ensemble of binary trees produced by a BART fit can be stored and re-used later via the R predict function. In addition to being an R package, the installed BART routines can be called directly from C++. The BART package provides the tools for your BART toolbox.},
	language = {en},
	number = {1},
	urldate = {2021-08-24},
	journal = {Journal of Statistical Software},
	author = {Sparapani, Rodney and Spanbauer, Charles and McCulloch, Robert},
	month = jan,
	year = {2021},
	note = {Number: 1},
	keywords = {survival analysis, competing risks, binary trees, black-box, categorical, continuous, ensemble predictive model, forking, multi-threading, multinomial, OpenMP, recurrent events},
	pages = {1--66},
	file = {Full Text:/home/imbroglio/Zotero/storage/P6MXST8R/Sparapani et al. - 2021 - Nonparametric Machine Learning and Efficient Compu.pdf:application/pdf},
}

@article{race_semi-parametric_2021,
	title = {Semi-parametric survival analysis via {Dirichlet} process mixtures of the {First} {Hitting} {Time} model},
	volume = {27},
	issn = {1572-9249},
	url = {https://doi.org/10.1007/s10985-020-09514-0},
	doi = {10.1007/s10985-020-09514-0},
	abstract = {Time-to-event data often violate the proportional hazards assumption inherent in the popular Cox regression model. Such violations are especially common in the sphere of biological and medical data where latent heterogeneity due to unmeasured covariates or time varying effects are common. A variety of parametric survival models have been proposed in the literature which make more appropriate assumptions on the hazard function, at least for certain applications. One such model is derived from the First Hitting Time (FHT) paradigm which assumes that a subject’s event time is determined by a latent stochastic process reaching a threshold value. Several random effects specifications of the FHT model have also been proposed which allow for better modeling of data with unmeasured covariates. While often appropriate, these methods often display limited flexibility due to their inability to model a wide range of heterogeneities. To address this issue, we propose a Bayesian model which loosens assumptions on the mixing distribution inherent in the random effects FHT models currently in use. We demonstrate via simulation study that the proposed model greatly improves both survival and parameter estimation in the presence of latent heterogeneity. We also apply the proposed methodology to data from a toxicology/carcinogenicity study which exhibits nonproportional hazards and contrast the results with both the Cox model and two popular FHT models.},
	language = {en},
	number = {1},
	urldate = {2021-08-24},
	journal = {Lifetime Data Analysis},
	author = {Race, Jonathan A. and Pennell, Michael L.},
	month = jan,
	year = {2021},
	pages = {177--194},
	file = {Submitted Version:/home/imbroglio/Zotero/storage/W8PYCP4I/Race and Pennell - 2021 - Semi-parametric survival analysis via Dirichlet pr.pdf:application/pdf},
}

@article{xu_semi-parametric_2020,
	title = {Semi-{Parametric} {Joint} {Modeling} of {Survival} and {Longitudinal} {Data}: {The} {R} {Package} {JSM}},
	volume = {93},
	copyright = {Copyright (c) 2020 Cong Xu, Pantelis Z. Hadjipantelis, Jane-Ling Wang},
	issn = {1548-7660},
	shorttitle = {Semi-{Parametric} {Joint} {Modeling} of {Survival} and {Longitudinal} {Data}},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v093i02},
	doi = {10.18637/jss.v093.i02},
	abstract = {This paper is devoted to the R package JSM which performs joint statistical modeling of survival and longitudinal data. In biomedical studies it has been increasingly common to collect both baseline and longitudinal covariates along with a possibly censored survival time. Instead of analyzing the survival and longitudinal outcomes separately, joint modeling approaches have attracted substantive attention in the recent literature and have been shown to correct biases from separate modeling approaches and enhance information. Most existing approaches adopt a linear mixed effects model for the longitudinal component and the Cox proportional hazards model for the survival component. We extend the Cox model to a more general class of transformation models for the survival process, where the baseline hazard function is completely unspecified leading to semiparametric survival models. We also offer a non-parametric multiplicative random effects model for the longitudinal process in JSM in addition to the linear mixed effects model. In this paper, we present the joint modeling framework that is implemented in JSM, as well as the standard error estimation methods, and illustrate the package with two real data examples: a liver cirrhosis data and a Mayo Clinic primary biliary cirrhosis data.},
	language = {en},
	number = {1},
	urldate = {2021-08-24},
	journal = {Journal of Statistical Software},
	author = {Xu, Cong and Hadjipantelis, Pantelis Z. and Wang, Jane-Ling},
	month = apr,
	year = {2020},
	note = {Number: 1},
	keywords = {B-splines, EM algorithm, multiplicative random effects, semi-parametric models, transformation model},
	pages = {1--29},
	file = {Full Text:/home/imbroglio/Zotero/storage/JNNQJ2KL/Xu et al. - 2020 - Semi-Parametric Joint Modeling of Survival and Lon.pdf:application/pdf},
}

@article{selingerova_comparison_2021,
	title = {Comparison of parametric and semiparametric survival regression models with kernel estimation},
	volume = {91},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949655.2021.1906875},
	doi = {10.1080/00949655.2021.1906875},
	abstract = {The modelling of censored survival data is based on different estimations of the conditional hazard function. When survival time follows a known distribution, parametric models are useful. This strong assumption is replaced by a weaker in the case of semiparametric models. For instance, the frequently used model suggested by Cox is based on the proportionality of hazards. These models use non-parametric methods to estimate some baseline hazard and parametric methods to estimate the influence of a covariate. An alternative approach is to use smoothing that is more flexible. In this paper, two types of kernel smoothing and some bandwidth selection techniques are introduced. Application to real data shows different interpretations for each approach. The extensive simulation study is aimed at comparing different approaches and assessing their benefits. Kernel estimation is demonstrated to be very helpful for verifying assumptions of parametric or semiparametric models and is able to capture changes in the hazard function in both time and covariate directions.},
	number = {13},
	urldate = {2021-08-24},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Selingerova, Iveta and Katina, Stanislav and Horova, Ivanka},
	month = sep,
	year = {2021},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00949655.2021.1906875},
	keywords = {Survival analysis, Cox model, hazard function, Kernel estimation, simulations},
	pages = {2717--2739},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/W228Q3L5/Selingerova et al. - 2021 - Comparison of parametric and semiparametric surviv.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/8ZDP27KT/00949655.2021.html:text/html},
}

@article{sinha_semiparametric_1997,
	title = {Semiparametric {Bayesian} {Analysis} of {Survival} {Data}},
	volume = {92},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1997.10474077},
	doi = {10.1080/01621459.1997.10474077},
	abstract = {This review paper investigates the potential of the semiparametric Bayes methods for the analysis of survival data. The nonparametric part of every semiparametric model is assumed to be a realization of a stochastic process. The parametric part, which may include a regression parameter or a parameter quantifying the heterogeneity of a population, is assumed to have a prior distribution with possibly unknown hyperparameters. Careful applications of some recently popular computational tools, such as posterior likelihood method and the Markov Chain Monte Carlo algorithms such as Gibbs sampling (Geman and Geman, 1984), are used to nd posterior estimates of several quantities of interest even when we are dealing with complex models and unusual data structures. The methodologies developed in this paper are motivated and aimed at analyzing some common types of survival data from di erent medical studies. We will center our attention to the following topics.},
	language = {en},
	number = {439},
	urldate = {2021-08-24},
	journal = {Journal of the American Statistical Association},
	author = {Sinha, Debajyoti and Dey, Dipak K.},
	month = sep,
	year = {1997},
	pages = {1195--1212},
	file = {Sinha and Dey - 1997 - Semiparametric Bayesian Analysis of Survival Data.pdf:/home/imbroglio/Zotero/storage/9G3LZG97/Sinha and Dey - 1997 - Semiparametric Bayesian Analysis of Survival Data.pdf:application/pdf},
}

@article{cai_onestep_2020,
	title = {One‐step targeted maximum likelihood estimation for time‐to‐event outcomes},
	volume = {76},
	issn = {0006-341X, 1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/biom.13172},
	doi = {10.1111/biom.13172},
	language = {en},
	number = {3},
	urldate = {2021-08-23},
	journal = {Biometrics},
	author = {Cai, Weixin and Laan, Mark J.},
	month = sep,
	year = {2020},
	keywords = {causal inference, machine learning, targeted maximum likelihood estimation, survival analysis, censored data, survival curve},
	pages = {722--733},
	file = {Cai and Laan - 2019 - One‐step targeted maximum likelihood estimation fo.pdf:/home/imbroglio/Zotero/storage/VQ4BRJUW/Cai and Laan - 2019 - One‐step targeted maximum likelihood estimation fo.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/HCE67UQW/biom.html:text/html},
}

@article{chen_causal_2001,
	title = {Causal {Inference} on the {Difference} of the {Restricted} {Mean} {Lifetime} {Between} {Two} {Groups}},
	volume = {57},
	issn = {0006341X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.0006-341X.2001.01030.x},
	doi = {10.1111/j.0006-341X.2001.01030.x},
	language = {en},
	number = {4},
	urldate = {2021-08-22},
	journal = {Biometrics},
	author = {Chen, Pei-Yun and Tsiatis, Anastasios A.},
	month = dec,
	year = {2001},
	pages = {1030--1038},
}

@article{zhang_robust_2015,
	title = {Robust methods to improve efficiency and reduce bias in estimating survival curves in randomized clinical trials},
	volume = {21},
	issn = {1380-7870, 1572-9249},
	url = {http://link.springer.com/10.1007/s10985-014-9291-y},
	doi = {10.1007/s10985-014-9291-y},
	language = {en},
	number = {1},
	urldate = {2021-08-22},
	journal = {Lifetime Data Analysis},
	author = {Zhang, Min},
	month = jan,
	year = {2015},
	pages = {119--137},
}

@article{lu_semiparametric_2011,
	title = {Semiparametric {Estimation} of {Treatment} {Effect} with {Time}-{Lagged} {Response} in the {Presence} of {Informative} {Censoring}},
	volume = {17},
	issn = {1380-7870},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3217309/},
	doi = {10.1007/s10985-011-9199-8},
	abstract = {In many randomized clinical trials, the primary response variable, for example, the survival time, is not observed directly after the patients enroll in the study but rather observed after some period of time (lag time). It is often the case that such a response variable is missing for some patients due to censoring that occurs when the study ends before the patient’s response is observed or when the patients drop out of the study. It is often assumed that censoring occurs at random which is referred to as noninformative censoring; however, in many cases such an assumption may not be reasonable. If the missing data are not analyzed properly, the estimator or test for the treatment effect may be biased. In this paper, we use semiparametric theory to derive a class of consistent and asymptotically normal estimators for the treatment effect parameter which are applicable when the response variable is right censored. The baseline auxiliary covariates and post-treatment auxiliary covariates, which may be time-dependent, are also considered in our semiparametric model. These auxiliary covariates are used to derive estimators that both account for informative censoring and are more efficient then the estimators which do not consider the auxiliary covariates.},
	number = {4},
	urldate = {2021-08-22},
	journal = {Lifetime data analysis},
	author = {Lu, Xiaomin and Tsiatis, Anastasios A.},
	month = oct,
	year = {2011},
	pmid = {21706378},
	pmcid = {PMC3217309},
	pages = {566--593},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/I67WKAXA/Lu and Tsiatis - 2011 - Semiparametric Estimation of Treatment Effect with.pdf:application/pdf},
}

@article{joffe_extended_2008,
	title = {Extended {Instrumental} {Variables} {Estimation} for {Overall} {Effects}},
	volume = {4},
	issn = {1557-4679},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2669310/},
	abstract = {We consider a method for extending instrumental variables methods in order to estimate the overall effect of a treatment or exposure. The approach is designed for settings in which the instrument influences both the treatment of interest and a secondary treatment also influenced by the primary treatment. We demonstrate that, while instrumental variables methods may be used to estimate the joint effects of the primary and secondary treatments, they cannot by themselves be used to estimate the overall effect of the primary treatment. However, instrumental variables methods may be used in conjunction with approaches for estimating the effect of the primary on the secondary treatment to estimate the overall effect of the primary treatment. We consider extending the proposed methods to deal with confounding of the effect of the instrument, mediation of the effect of the instrument by other variables, failure-time outcomes, and time-varying secondary treatments. We motivate our discussion by considering estimation of the overall effect of the type of vascular access among hemodialysis patients.},
	number = {1},
	urldate = {2021-08-22},
	journal = {The international journal of biostatistics},
	author = {Joffe, Marshall M. and Small, Dylan and Have, Thomas Ten and Brunelli, Steve and Feldman, Harold I.},
	year = {2008},
	pmid = {19381345},
	pmcid = {PMC2669310},
	pages = {artno4},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/KZBW3VXH/Joffe et al. - 2008 - Extended Instrumental Variables Estimation for Ove.pdf:application/pdf},
}

@article{kaplan_nonparametric_1958,
	title = {Nonparametric {Estimation} from {Incomplete} {Observations}},
	volume = {53},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1958.10501452},
	doi = {10.1080/01621459.1958.10501452},
	language = {en},
	number = {282},
	urldate = {2021-08-22},
	journal = {Journal of the American Statistical Association},
	author = {Kaplan, E. L. and Meier, Paul},
	month = jun,
	year = {1958},
	pages = {457--481},
	file = {Full Text:/home/imbroglio/Zotero/storage/I3NRKBKB/Kaplan and Meier - 1958 - Nonparametric Estimation from Incomplete Observati.pdf:application/pdf},
}

@article{bibaut_fast_2019,
	title = {Fast rates for empirical risk minimization over c{\textbackslash}`adl{\textbackslash}`ag functions with bounded sectional variation norm},
	url = {http://arxiv.org/abs/1907.09244},
	abstract = {Empirical risk minimization over classes functions that are bounded for some version of the variation norm has a long history, starting with Total Variation Denoising (Rudin et al., 1992), and has been considered by several recent articles, in particular Fang et al., 2019 and van der Laan, 2015. In this article, we consider empirical risk minimization over the class \${\textbackslash}mathcal\{F\}\_d\$ of c{\textbackslash}`adl{\textbackslash}`ag functions over \$[0,1]{\textasciicircum}d\$ with bounded sectional variation norm (also called Hardy-Krause variation). We show how a certain representation of functions in \${\textbackslash}mathcal\{F\}\_d\$ allows to bound the bracketing entropy of sieves of \${\textbackslash}mathcal\{F\}\_d\$, and therefore derive rates of convergence in nonparametric function estimation. Specifically, for sieves whose growth is controlled by some rate \$a\_n\$, we show that the empirical risk minimizer has rate of convergence \$O\_P(n{\textasciicircum}\{-1/3\} ({\textbackslash}log n){\textasciicircum}\{2(d-1)/3\} a\_n)\$. Remarkably, the dimension only affects the rate in \$n\$ through the logarithmic factor, making this method especially appropriate for high dimensional problems. In particular, we show that in the case of nonparametric regression over sieves of c{\textbackslash}`adl{\textbackslash}`ag functions with bounded sectional variation norm, this upper bound on the rate of convergence holds for least-squares estimators, under the random design, sub-exponential errors setting.},
	urldate = {2021-08-01},
	journal = {arXiv:1907.09244 [math, stat]},
	author = {Bibaut, Aurélien F. and van der Laan, Mark J.},
	month = aug,
	year = {2019},
	note = {arXiv: 1907.09244
version: 2},
	keywords = {Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/UIWVZPJ4/Bibaut and van der Laan - 2019 - Fast rates for empirical risk minimization over c.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/5LTI2L6B/1907.html:text/html},
}

@article{pearl_causal_2009,
	title = {Causal inference in statistics: {An} overview},
	volume = {3},
	issn = {1935-7516},
	shorttitle = {Causal inference in statistics},
	url = {https://projecteuclid.org/journals/statistics-surveys/volume-3/issue-none/Causal-inference-in-statistics-An-overview/10.1214/09-SS057.full},
	doi = {10.1214/09-SS057},
	abstract = {This review presents empirical researchers with recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and uniﬁes other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the eﬀects of potential interventions, (also called “causal eﬀects” or “policy evaluation”) (2) queries about probabilities of counterfactuals, (including assessment of “regret,” “attribution” or “causes of eﬀects”) and (3) queries about direct and indirect eﬀects (also known as “mediation”). Finally, the paper deﬁnes the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
	language = {en},
	number = {none},
	urldate = {2021-07-23},
	journal = {Statistics Surveys},
	author = {Pearl, Judea},
	month = jan,
	year = {2009},
	file = {Pearl - 2009 - Causal inference in statistics An overview.pdf:/home/imbroglio/Zotero/storage/HITKTQIX/Pearl - 2009 - Causal inference in statistics An overview.pdf:application/pdf},
}

@book{pearl_causality_2000,
	address = {Cambridge, U.K. ; New York},
	edition = {1st Edition},
	title = {Causality: {Models}, {Reasoning}, and {Inference}},
	isbn = {978-0-521-77362-1},
	shorttitle = {Causality},
	abstract = {Written by one of the pre-eminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, philosophy, cognitive science, and the health and social sciences. Pearl presents a unified account of the probabilistic, manipulative, counterfactual and structural approaches to causation, and devises simple mathematical tools for analyzing the relationships between causal connections, statistical associations, actions and observations. The book will open the way for including causal analysis in the standard curriculum of statistics, artifical intelligence, business, epidemiology, social science and economics. Students in these areas will find natural models, simple identification procedures, and precise mathematical definitions of causal concepts that traditional texts have tended to evade or make unduly complicated. This book will be of interest to professionals and students in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable. Professor of Computer Science at the UCLA, Judea Pearl is the winner of the 2008 Benjamin Franklin Award in Computers and Cognitive Science.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	month = mar,
	year = {2000},
}

@article{laan_targeted_2006,
	title = {Targeted {Maximum} {Likelihood} {Learning}},
	volume = {2},
	issn = {1557-4679},
	doi = {10.2202/1557-4679.1043},
	language = {en},
	number = {1},
	journal = {The International Journal of Biostatistics},
	author = {Laan, Mark J. van der and Rubin, Daniel},
	month = dec,
	year = {2006},
	note = {Publisher: De Gruyter
Section: The International Journal of Biostatistics},
	keywords = {cross-validation, causal effect, efficient influence curve, loss function, targeted maximum likelihood estimation, maximum likelihood estimation, estimating function, locally efficient estimation, sieve, variable importance},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/HW2CYHB3/van and Rubin - 2006 - Targeted Maximum Likelihood Learning.pdf:application/pdf;Full Text PDF:/home/imbroglio/Zotero/storage/7TYKLA68/Laan and Rubin - 2006 - Targeted Maximum Likelihood Learning.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/AW9TSIUM/article-ijb.2006.2.1.1043.xml.html:text/html},
}

@book{laan_targeted_2011,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	isbn = {978-1-4419-9781-4},
	shorttitle = {Targeted {Learning}},
	abstract = {The statistics profession is at a unique point in history. The need for valid statistical tools is greater than ever; data sets are massive, often measuring hundreds of thousands of measurements for a single subject. The field is ready to move towards clear objective benchmarks under which tools can be evaluated. Targeted learning allows (1) the full generalization and utilization of cross-validation as an estimator selection tool so that the subjective choices made by humans are now made by the machine, and (2) targeting the fitting of the probability distribution of the data toward the target parameter representing the scientific question of interest. This book is aimed at both statisticians and applied researchers interested in causal inference and general effect estimation for observational and experimental data. Part I is an accessible introduction to super learning and the targeted maximum likelihood estimator, including related concepts necessary to understand and apply these methods. Parts II-IX handle complex data structures and topics applied researchers will immediately recognize from their own research, including time-to-event outcomes, direct and indirect effects, positivity violations, case-control studies, censored data, longitudinal data, and genomic studies."Targeted Learning, by Mark J. van der Laan and Sherri Rose, fills a much needed gap in statistical and causal inference. It protects us from wasting computational, analytical, and data resources on irrelevant aspects of a problem and teaches us how to focus on what is relevant – answering questions that researchers truly care about."-Judea Pearl, Computer Science Department, University of California, Los Angeles"In summary, this book should be on the shelf of every investigator who conducts observational research and randomized controlled trials. The concepts and methodology are foundational for causal inference and at the same time stay true to what the data at hand can say about the questions that motivate their collection."-Ira B. Tager, Division of Epidemiology, University of California, Berkeley},
	language = {en},
	publisher = {Springer-Verlag},
	author = {Laan, Mark J. van der and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1},
	file = {Snapshot:/home/imbroglio/Zotero/storage/FSIGDFF3/9781441997814.html:text/html;van der Laan and Rose - 2011 - Targeted Learning.pdf:/home/imbroglio/Zotero/storage/PY8TRAXJ/van der Laan and Rose - 2011 - Targeted Learning.pdf:application/pdf},
}

@book{laan_targeted_2018,
	series = {Springer {Series} in {Statistics}},
	title = {Targeted {Learning} in {Data} {Science}: {Causal} {Inference} for {Complex} {Longitudinal} {Studies}},
	isbn = {978-3-319-65303-7},
	shorttitle = {Targeted {Learning} in {Data} {Science}},
	abstract = {This textbook for graduate students in statistics, data science, and public health deals with the practical challenges that come with big, complex, and dynamic data. It presents a scientific roadmap to translate real-world data science applications into formal statistical estimation problems by using the general template of targeted maximum likelihood estimators. These targeted machine learning algorithms estimate quantities of interest while still providing valid inference. Targeted learning methods within data science area critical component for solving scientific problems in the modern age. The techniques can answer complex questions including optimal rules for assigning treatment based on longitudinal data with time-dependent confounding, as well as other estimands in dependent data structures, such as networks. Included in Targeted Learning in Data Science are demonstrations with soft ware packages and real data sets that present a case that targeted learning is crucial for the next generation of statisticians and data scientists. Th is book is a sequel to the first textbook on machine learning for causal inference, Targeted Learning, published in 2011.Mark van der Laan, PhD, is Jiann-Ping Hsu/Karl E. Peace Professor of Biostatistics and Statistics at UC Berkeley. His research interests include statistical methods in genomics, survival analysis, censored data, machine learning, semiparametric models, causal inference, and targeted learning. Dr. van der Laan received the 2004 Mortimer Spiegelman Award, the 2005 Van Dantzig Award, the 2005 COPSS Snedecor Award, the 2005 COPSS Presidential Award, and has graduated over 40 PhD students in biostatistics and statistics.Sherri Rose, PhD, is Associate Professor of Health Care Policy (Biostatistics) at Harvard Medical School. Her work is centered on developing and integrating innovative statistical approaches to advance human health. Dr. Rose’s methodological research focuses on nonparametric machine learning for causal inference and prediction. She co-leads the Health Policy Data Science Lab and currently serves as an associate editor for the Journal of the American Statistical Association and Biostatistics.},
	language = {en},
	publisher = {Springer International Publishing},
	author = {Laan, Mark J. van der and Rose, Sherri},
	year = {2018},
	doi = {10.1007/978-3-319-65304-4},
	file = {Snapshot:/home/imbroglio/Zotero/storage/DNGAFAFR/9783319653037.html:text/html;Snapshot:/home/imbroglio/Zotero/storage/RYXHVNK8/9783319653037.html:text/html},
}

@article{laan_super_2007,
	title = {Super {Learner}},
	volume = {6},
	issn = {1544-6115, 2194-6302},
	doi = {10.2202/1544-6115.1309},
	language = {en},
	number = {1},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Laan, Mark J. van der and Polley, Eric C. and Hubbard, Alan E.},
	month = sep,
	year = {2007},
	keywords = {cross-validation, loss-based estimation, machine learning, prediction},
	file = {:/home/imbroglio/Zotero/storage/UAUGVHQ8/paper222.html:text/html;Full Text PDF:/home/imbroglio/Zotero/storage/GAN8VE4A/van et al. - 2007 - Super Learner.pdf:application/pdf;Full Text PDF:/home/imbroglio/Zotero/storage/ENKNI7VY/Laan et al. - 2007 - Super Learner.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/RT2KNC4V/article-sagmb.2007.6.1.1309.xml.html:text/html},
}

@article{efron_logistic_1988,
	title = {Logistic {Regression}, {Survival} {Analysis}, and the {Kaplan}-{Meier} {Curve}},
	volume = {83},
	issn = {0162-1459},
	doi = {10.1080/01621459.1988.10478612},
	abstract = {We discuss the use of standard logistic regression techniques to estimate hazard rates and survival curves from censored data. These techniques allow the statistician to use parametric regression modeling on censored data in a flexible way that provides both estimates and standard errors. An example is given that demonstrates the increased structure that can be seen in a parametric analysis, as compared with the nonparametric Kaplan-Meier survival curves. In fact, the logistic regression estimates are closely related to Kaplan-Meier curves, and approach the Kaplan-Meier estimate as the number of parameters grows large.},
	number = {402},
	journal = {Journal of the American Statistical Association},
	author = {Efron, Bradley},
	month = jun,
	year = {1988},
	keywords = {Hazard-rate estimates, Parametric smoothing, Parametric survival curves, Partial logistic regression, Semi-parametric smoothing},
	pages = {414--425},
	file = {Efron - 2020 - Logistic Regression, Survival Analysis, and the Ka.pdf:/home/imbroglio/Zotero/storage/FTKC76GG/Efron - 2020 - Logistic Regression, Survival Analysis, and the Ka.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/T8VHNAVZ/01621459.1988.html:text/html;Snapshot:/home/imbroglio/Zotero/storage/Q9IIKBGU/01621459.1988.html:text/html},
}

@article{myers_endothelialized_2012,
	title = {Endothelialized {Microfluidics} for {Studying} {Microvascular} {Interactions} in {Hematologic} {Diseases}},
	issn = {1940-087X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3471282/},
	doi = {10.3791/3958},
	abstract = {Advances in microfabrication techniques have enabled the production of inexpensive and reproducible microfluidic systems for conducting biological and biochemical experiments at the micro- and nanoscales 1,2. In addition, microfluidics have also been specifically used to quantitatively analyze hematologic and microvascular processes, because of their ability to easily control the dynamic fluidic environment and biological conditions3-6. As such, researchers have more recently used microfluidic systems to study blood cell deformability, blood cell aggregation, microvascular blood flow, and blood cell-endothelial cell interactions6-13.However, these microfluidic systems either did not include cultured endothelial cells or were larger than the sizescale relevant to microvascular pathologic processes. A microfluidic platform with cultured endothelial cells that accurately recapitulates the cellular, physical, and hemodynamic environment of the microcirculation is needed to further our understanding of the underlying biophysical pathophysiology of hematologic diseases that involve the microvasculature., Here, we report a method to create an "endothelialized" in vitro model of the microvasculature, using a simple, single mask microfabrication process in conjunction with standard endothelial cell culture techniques, to study pathologic biophysical microvascular interactions that occur in hematologic disease. This "microvasculature-on-a-chip" provides the researcher with a robust assay that tightly controls biological as well as biophysical conditions and is operated using a standard syringe pump and brightfield/fluorescence microscopy. Parameters such as microcirculatory hemodynamic conditions, endothelial cell type, blood cell type(s) and concentration(s), drug/inhibitory concentration etc., can all be easily controlled. As such, our microsystem provides a method to quantitatively investigate disease processes in which microvascular flow is impaired due to alterations in cell adhesion, aggregation, and deformability, a capability unavailable with existing assays.},
	number = {64},
	urldate = {2019-02-12},
	journal = {Journal of Visualized Experiments : JoVE},
	author = {Myers, David R. and Sakurai, Yumiko and Tran, Reginald and Ahn, Byungwook and Hardy, Elaissa Trybus and Mannino, Robert and Kita, Ashley and Tsai, Michelle and Lam, Wilbur A.},
	month = jun,
	year = {2012},
	pmid = {22760254},
	pmcid = {PMC3471282},
	file = {Myers et al. - 2012 - Endothelialized Microfluidics for Studying Microva.pdf:/home/imbroglio/microfluidics/00_literature/Myers et al. - 2012 - Endothelialized Microfluidics for Studying Microva.pdf:application/pdf;PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/IPAJEECU/Myers et al. - 2012 - Endothelialized Microfluidics for Studying Microva.pdf:application/pdf},
}

@article{moore_covariate_2009,
	title = {Covariate adjustment in randomized trials with binary outcomes: {Targeted} maximum likelihood estimation},
	volume = {28},
	issn = {0277-6715},
	shorttitle = {Covariate adjustment in randomized trials with binary outcomes},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2857590/},
	doi = {10.1002/sim.3445},
	abstract = {Covariate adjustment using linear models for continuous outcomes in randomized trials has been shown to increase efficiency and power over the unadjusted method in estimating the marginal effect of treatment. However, for binary outcomes, investigators generally rely on the unadjusted estimate as the literature indicates that covariate-adjusted estimates based on the logistic regression models are less efficient. The crucial step that has been missing when adjusting for covariates is that one must integrate/average the adjusted estimate over those covariates in order to obtain the marginal effect. We apply the method of targeted maximum likelihood estimation (tMLE) to obtain estimators for the marginal effect using covariate adjustment for binary outcomes. We show that the covariate adjustment in randomized trials using the logistic regression models can be mapped, by averaging over the covariate(s), to obtain a fully robust and efficient estimator of the marginal effect, which equals a targeted maximum likelihood estimator. This tMLE is obtained by simply adding a clever covariate to a fixed initial regression. We present simulation studies that demonstrate that this tMLE increases efficiency and power over the unadjusted method, particularly for smaller sample sizes, even when the regression model is mis-specified.},
	number = {1},
	urldate = {2020-08-11},
	journal = {Statistics in medicine},
	author = {Moore, K. L. and Laan, M. J. van der},
	month = jan,
	year = {2009},
	pmid = {18985634},
	pmcid = {PMC2857590},
	keywords = {variable selection, covariate adjustment, efficiency, clinical trails},
	pages = {39--64},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/A4QK7Z3E/Moore and Laan - 2009 - Covariate adjustment in randomized trials with bin.pdf:application/pdf;PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/96Y56YVX/Moore and van der Laan - 2009 - Covariate adjustment in randomized trials with bin.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/JHJMUCH3/sim.html:text/html},
}

@article{laan_generally_2017,
	title = {A {Generally} {Efficient} {Targeted} {Minimum} {Loss} {Based} {Estimator} based on the {Highly} {Adaptive} {Lasso}},
	volume = {13},
	doi = {10.1515/ijb-2015-0097},
	language = {en},
	number = {2},
	journal = {The International Journal of Biostatistics},
	author = {Laan, Mark J. van der},
	month = oct,
	year = {2017},
	keywords = {efficient influence curve, influence curve, asymptotic linear estimator, canonical gradient, cross-validated targeted minimum loss estimation (CV-TMLE), Donsker class, efficient estimator, empirical process, entropy, highly adaptive Lasso, one-step TMLE, super-learning, targeted minimum loss estimation (TMLE)},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/JDTQGV9M/van - 2017 - A Generally Efficient Targeted Minimum Loss Based .pdf:application/pdf;Full Text PDF:/home/imbroglio/Zotero/storage/XKL3MEPA/Laan - 2017 - A Generally Efficient Targeted Minimum Loss Based .pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/XS5VA3KS/article-20150097.html:text/html},
}

@article{greenwood_natural_1926,
	title = {The {Natural} {Duration} of {Cancer}},
	volume = {33},
	journal = {Reports of Public Health and Related Subjects},
	author = {Greenwood, Major},
	year = {1926},
	note = {London: Her Majesty's Stationery Office},
	pages = {1--26},
}

@article{laan_unified_2003,
	title = {Unified {Cross}-{Validation} {Methodology} {For} {Selection} {Among} {Estimators} and a {General} {Cross}-{Validated} {Adaptive} {Epsilon}-{Net} {Estimator}: {Finite} {Sample} {Oracle} {Inequalities} and {Examples}},
	shorttitle = {Unified {Cross}-{Validation} {Methodology} {For} {Selection} {Among} {Estimators} and a {General} {Cross}-{Validated} {Adaptive} {Epsilon}-{Net} {Estimator}},
	journal = {U.C. Berkeley Division of Biostatistics Working Paper Series},
	author = {Laan, Mark J. van der and Dudoit, Sandrine},
	month = nov,
	year = {2003},
	file = {"Unified Cross-Validation Methodology For Selection Among Estimators an" by Mark J. van der Laan and Sandrine Dudoit:/home/imbroglio/Zotero/storage/2LM57J99/paper130.html:text/html},
}

@article{benkeser_improving_2020,
	title = {Improving precision and power in randomized trials for {COVID}-19 treatments using covariate adjustment, for binary, ordinal, and time-to-event outcomes},
	issn = {1541-0420},
	doi = {10.1111/biom.13377},
	abstract = {Time is of the essence in evaluating potential drugs and biologics for the treatment and prevention of COVID-19. There are currently 876 randomized clinical trials (phase 2 and 3) of treatments for COVID-19 registered on clinicaltrials.gov. Covariate adjustment is a statistical analysis method with potential to improve precision and reduce the required sample size for a substantial number of these trials. Though covariate adjustment is recommended by the U.S. Food and Drug Administration and the European Medicines Agency, it is underutilized, especially for the types of outcomes (binary, ordinal, and time-to-event) that are common in COVID-19 trials. To demonstrate the potential value added by covariate adjustment in this context, we simulated two-arm, randomized trials comparing a hypothetical COVID-19 treatment versus standard of care, where the primary outcome is binary, ordinal, or time-to-event. Our simulated distributions are derived from two sources: longitudinal data on over 500 patients hospitalized at Weill Cornell Medicine New York Presbyterian Hospital and a Centers for Disease Control and Prevention preliminary description of 2449 cases. In simulated trials with sample sizes ranging from 100 to 1000 participants, we found substantial precision gains from using covariate adjustment-equivalent to 4-18\% reductions in the required sample size to achieve a desired power. This was the case for a variety of estimands (targets of inference). From these simulations, we conclude that covariate adjustment is a low-risk, high-reward approach to streamlining COVID-19 treatment trials. We provide an R package and practical recommendations for implementation.},
	language = {eng},
	journal = {Biometrics},
	author = {Benkeser, David and Díaz, Iván and Luedtke, Alex and Segal, Jodi and Scharfstein, Daniel and Rosenblum, Michael},
	month = sep,
	year = {2020},
	keywords = {covariate adjustment, randomized trial, survival analysis, COVID-19, ordinal outcomes},
	file = {Full Text:/home/imbroglio/Zotero/storage/CS6TJ6Q6/Benkeser et al. - 2020 - Improving precision and power in randomized trials.pdf:application/pdf},
}

@incollection{zheng_cross-validated_2011,
	address = {New York},
	title = {Cross-validated targeted minimum loss based estimation},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Studies}},
	publisher = {Springer},
	author = {Zheng, Wenjing and Laan, M. J. van der},
	editor = {Laan, M. J. van der and Rose, Sherri},
	year = {2011},
	file = {Citeseer - Full Text PDF:/home/imbroglio/Zotero/storage/IYF9ZYNF/Zheng et al. - Asymptotic Theory for Cross-validated Targeted Max.pdf:application/pdf;Citeseer - Snapshot:/home/imbroglio/Zotero/storage/IQV3NGAT/download.html:text/html},
}

@incollection{van_der_laan_generally_2018,
	address = {Cham},
	series = {Springer {Series} in {Statistics}},
	title = {A {Generally} {Efficient} {HAL}-{TMLE}},
	isbn = {978-3-319-65304-4},
	url = {https://doi.org/10.1007/978-3-319-65304-4_7},
	abstract = {We will present a TMLE of ψ0 that is asymptotically efficient at any P∈MP∈MP {\textbackslash}in {\textbackslash}mathcal\{M\}. This is a remarkable statement since we only assume strong positivity, some global bounds, and a finite variation norm of Q¯0,G¯0Q¯0,G¯0{\textbackslash}bar\{Q\}\_\{0\},{\textbackslash}bar\{G\}\_\{0\}. This estimation problem for the treatment specific mean will be our key example to demonstrate a general one-step TMLE that is guaranteed to be asymptotically efficient for any model and pathwise differentiable target parameter, essentially only assuming a positivity assumption, also guaranteeing strong identifiability of the target parameter.},
	language = {en},
	urldate = {2020-11-05},
	booktitle = {Targeted {Learning} in {Data} {Science}: {Causal} {Inference} for {Complex} {Longitudinal} {Studies}},
	publisher = {Springer International Publishing},
	author = {van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2018},
	doi = {10.1007/978-3-319-65304-4_7},
	pages = {95--102},
}

@book{bickel_efficient_1998,
	address = {New York},
	title = {Efficient and {Adaptive} {Estimation} for {Semiparametric} {Models}},
	isbn = {978-0-387-98473-5},
	abstract = {This book is about estimation in situations where we believe we have enough knowledge to model some features of the data parametrically, but are unwilling to assume anything for other features. Such models have arisen in a wide variety of contexts in recent years, particularly in economics, epidemiology, and astronomy. The complicated structure of these models typically requires us to consider nonlinear estimation procedures which often can only be implemented algorithmically. The theory of these procedures is necessarily based on asymptotic approximations.},
	language = {en},
	publisher = {Springer-Verlag},
	author = {Bickel, Peter J. and Klaassen, Chris A. J. and Ritov, Ya'acov and Wellner, Jon A.},
	year = {1998},
	file = {Snapshot:/home/imbroglio/Zotero/storage/ZRB54V8R/9780387984735.html:text/html},
}

@book{laan_unified_2003-1,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Unified {Methods} for {Censored} {Longitudinal} {Data} and {Causality}},
	isbn = {978-0-387-95556-8},
	abstract = {During the last decades, there has been an explosion in computation and information technology. This development comes with an expansion of complex observational studies and clinical trials in a variety of fields such as medicine, biology, epidemiology, sociology, and economics among many others, which involve collection of large amounts of data on subjects or organisms over time. The goal of such studies can be formulated as estimation of a finite dimensional parameter of the population distribution corresponding to the observed time- dependent process. Such estimation problems arise in survival analysis, causal inference and regression analysis. This book provides a fundamental statistical framework for the analysis of complex longitudinal data. It provides the first comprehensive description of optimal estimation techniques based on time-dependent data structures subject to informative censoring and treatment assignment in so called semiparametric models. Semiparametric models are particularly attractive since they allow the presence of large unmodeled nuisance parameters. These techniques include estimation of regression parameters in the familiar (multivariate) generalized linear regression and multiplicative intensity models. They go beyond standard statistical approaches by incorporating all the observed data to allow for informative censoring, to obtain maximal efficiency, and by developing estimators of causal effects. It can be used to teach masters and Ph.D. students in biostatistics and statistics and is suitable for researchers in statistics with a strong interest in the analysis of complex longitudinal data.},
	language = {en},
	publisher = {Springer-Verlag},
	author = {Laan, Mark J. van der and Robins, James M.},
	year = {2003},
	file = {Snapshot:/home/imbroglio/Zotero/storage/KYJPZAEA/9780387955568.html:text/html},
}

@incollection{robins_recovery_1992,
	address = {Boston, MA},
	title = {Recovery of {Information} and {Adjustment} for {Dependent} {Censoring} {Using} {Surrogate} {Markers}},
	isbn = {978-1-4757-1229-2},
	abstract = {A class of tests and estimators for the parameters of the Cox proportional hazards model, the accelerated failure time model, and a model for the effect of treatment on the mean of a response variable of interest are proposed that use surrogate marker data to recover information lost due to independent censoring and to adjust for bias due to dependent censoring in randomized clinical trials. We construct an adaptive test that (i) is asymptotically distribution free under the null hypothesis of no treatment effect on survival, (ii) incorporates surrogate marker data, and (iii) is guaranteed to be locally more powerful than the ordinary log-rank test against proportional hazards alternatives when the baseline failure time distribution is Weibull. The proposed test is shown to outperform the log-rank test in a series of simulation experiments. We also prove the optimal estimator within our class is semiparametric efficient by first showing that our estimation problem is a special case of the general problem of parameter estimation in an arbitrary semiparametric model with data missing at random, and then deriving a representation for the efficient score in this more general problem.},
	language = {en},
	booktitle = {{AIDS} {Epidemiology}: {Methodological} {Issues}},
	publisher = {Birkhäuser},
	author = {Robins, James M. and Rotnitzky, Andrea},
	editor = {Jewell, Nicholas P. and Dietz, Klaus and Farewell, Vernon T.},
	year = {1992},
	keywords = {Asymptotic Relative Efficiency, Efficient Score, Failure Time, Influence Function, Semiparametric Model},
	pages = {297--331},
}

@article{benkeser_improved_2018,
	title = {Improved estimation of the cumulative incidence of rare outcomes},
	volume = {37},
	issn = {02776715},
	doi = {10.1002/sim.7337},
	language = {en},
	number = {2},
	journal = {Statistics in Medicine},
	author = {Benkeser, David and Carone, Marco and Gilbert, Peter B.},
	month = jan,
	year = {2018},
	pages = {280--293},
	file = {Benkeser et al. - 2018 - Improved estimation of the cumulative incidence of.pdf:/home/imbroglio/Zotero/storage/RXIYF8YW/Benkeser et al. - 2018 - Improved estimation of the cumulative incidence of.pdf:application/pdf},
}

@article{breiman_stacked_1996,
	title = {Stacked regressions},
	volume = {24},
	issn = {1573-0565},
	doi = {10.1007/BF00117832},
	abstract = {Stacking regressions is a method for forming linear combinations of different predictors to give improved prediction accuracy. The idea is to use cross-validation data and least squares under non-negativity constraints to determine the coefficients in the combination. Its effectiveness is demonstrated in stacking regression trees of different sizes and in a simulation stacking linear subset and ridge regressions. Reasons why this method works are explored. The idea of stacking originated with Wolpert (1992).},
	language = {en},
	number = {1},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = jul,
	year = {1996},
	pages = {49--64},
	file = {Springer Full Text PDF:/home/imbroglio/Zotero/storage/ZCSHR8H2/Breiman - 1996 - Stacked regressions.pdf:application/pdf},
}

@inproceedings{robins_robust_1999,
	address = {Alexandria, VA},
	title = {Robust estimation in sequentially ignorable missing data and causal inference models},
	booktitle = {Proceedings of the {American} {Statistical} {Association}: {Section} on {Bayesian} {Statistical} {Science}},
	author = {Robins, JM},
	year = {1999},
	pages = {6--10},
	file = {jsaprocpat1.pdf:/home/imbroglio/Zotero/storage/ER9FQ9Z7/jsaprocpat1.pdf:application/pdf},
}

@article{petersen_diagnosing_2012,
	title = {Diagnosing and responding to violations in the positivity assumption},
	volume = {21},
	issn = {0962-2802, 1477-0334},
	url = {http://journals.sagepub.com/doi/10.1177/0962280210386207},
	doi = {10.1177/0962280210386207},
	language = {en},
	number = {1},
	urldate = {2020-11-03},
	journal = {Statistical Methods in Medical Research},
	author = {Petersen, Maya L and Porter, Kristin E and Gruber, Susan and Wang, Yue and van der Laan, Mark J},
	month = feb,
	year = {2012},
	pages = {31--54},
	file = {Full Text:/home/imbroglio/Zotero/storage/U7WPTWMY/Petersen et al. - 2012 - Diagnosing and responding to violations in the pos.pdf:application/pdf},
}

@article{robins_new_1986,
	title = {A new approach to causal inference in mortality studies with a sustained exposure period—application to control of the healthy worker survivor effect},
	volume = {7},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0270025586900886},
	doi = {10.1016/0270-0255(86)90088-6},
	abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave empioyment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject’s assigned treatment protocol has been erased from the data tile. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated.},
	language = {en},
	urldate = {2020-09-22},
	journal = {Mathematical Modelling},
	author = {Robins, James},
	year = {1986},
	pages = {1393--1512},
	file = {Robins - 1986 - A new approach to causal inference in mortality st.pdf:/home/imbroglio/Zotero/storage/DUYYC3W9/Robins - 1986 - A new approach to causal inference in mortality st.pdf:application/pdf},
}

@article{neyman_application_1990,
	title = {On the {Application} of {Probability} {Theory} to {Agricultural} {Experiments}. {Essay} on {Principles}. {Section} 9.},
	volume = {5},
	url = {http://www.jstor.org/stable/2245382},
	number = {4},
	journal = {Statistical Science},
	author = {Neyman, Jerzy and Dabrowska, D. M. and Speed, T. P.},
	year = {1990},
	pages = {465--472},
	file = {Splawa-Neyman et al. - 1990 - On the Application of Probability Theory to Agricu.pdf:/home/imbroglio/Zotero/storage/UIS847LX/Splawa-Neyman et al. - 1990 - On the Application of Probability Theory to Agricu.pdf:application/pdf},
}

@article{pearl_consistency_2010,
	title = {On the {Consistency} {Rule} in {Causal} {Inference}: {Axiom}, {Definition}, {Assumption}, or {Theorem}?},
	volume = {21},
	issn = {1044-3983},
	shorttitle = {On the {Consistency} {Rule} in {Causal} {Inference}},
	doi = {10.1097/EDE.0b013e3181f5d3fd},
	abstract = {In 2 recent communications, Cole and Frangakis (Epidemiology. 2009;20:3–5) and VanderWeele (Epidemiology. 2009;20:880–883) conclude that the consistency rule used in causal inference is an assumption that precludes any side-effects of treatment/exposure on the outcomes of interest. They further develop auxiliary notation to make this assumption formal and explicit. I argue that the consistency rule is a theorem in the logic of counterfactuals and need not be altered. Instead, warnings of potential side-effects should be embodied in standard modeling practices that make causal assumptions explicit and transparent.},
	language = {en-US},
	number = {6},
	journal = {Epidemiology},
	author = {Pearl, Judea},
	month = nov,
	year = {2010},
	pages = {872--875},
	file = {Snapshot:/home/imbroglio/Zotero/storage/CD8NZIDD/On_the_Consistency_Rule_in_Causal_Inference_.19.html:text/html},
}

@inproceedings{gill_coarsening_1997,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Coarsening at {Random}: {Characterizations}, {Conjectures}, {Counter}-{Examples}},
	isbn = {978-1-4684-6316-3},
	shorttitle = {Coarsening at {Random}},
	doi = {10.1007/978-1-4684-6316-3_14},
	abstract = {The notion of coarsening at random (CAR) was introduced by Heitjan and Rubin (1991) to describe the most general form of randomly grouped, censored, or missing data, for which the coarsening mechanism can be ignored when making likelihood-based inference about the parameters of the distribution of the variable of interest. The CAR assumption is popular, and applications abound. However the full implications of the assumption have not been realized. Moreover a satisfactory theory of CAR for continuously distributed data—which is needed in many applications, particularly in survival analysis—hardly exists as yet. This paper gives a detailed study of CAR. We show that grouped data from a finite sample space always fit a CAR model: a nonparametric model for the variable of interest together with the assumption of an arbitrary CAR mechanism puts no restriction at all on the distribution of the observed data. In a slogan, CAR is everything. We describe what would seem to be the most general way CAR data could occur in practice, a sequential procedure called randomized monotone coarsening. We show that CAR mechanisms exist which are not of this type. Such a coarsening mechanism uses information about the underlying data which is not revealed to the observer, without this affecting the observer’s conclusions. In a second slogan, CAR is more than it seems. This implies that if the analyst can argue from subject-matter considerations that coarsened data is CAR, he or she has knowledge about the structure of the coarsening mechanism which can be put to good use in non-likelihood-based inference procedures. We argue that this is a valuable option in multivariate survival analysis. We give a new definition of CAR in general sample spaces, criticising earlier proposals, and we establish parallel results to the discrete case. The new definition focusses on the distribution rather than the density of the data. It allows us to generalise the theory of CAR to the important situation where coarsening variables (e.g., censoring times) are partially observed as well as the variables of interest.},
	language = {en},
	booktitle = {Proceedings of the {First} {Seattle} {Symposium} in {Biostatistics}},
	publisher = {Springer US},
	author = {Gill, Richard D. and van der Laan, Mark J. and Robins, James M.},
	editor = {Lin, D. Y. and Fleming, T. R.},
	year = {1997},
	keywords = {Semiparametric Model, Conditional Distribution, Discrete Case, Marginal Distribution, Sample Space},
	pages = {255--294},
}

@article{jacobsen_coarsening_1995,
	title = {Coarsening at {Random} in {General} {Sample} {Spaces} and {Random} {Censoring} in {Continuous} {Time}},
	volume = {23},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176324622},
	doi = {10.1214/aos/1176324622},
	abstract = {Heitjan and Rubin proposed a concept, "coarsening at random," generalizing Rubin's theory of missing at random. Their analysis was done in discrete sample spaces. We propose a generalization to general sample spaces. Among Heitjan and Rubin's applications was right-censoring in survival analysis. We discuss the application of the generalized theory to various censoring patterns in continuous time and connect to the modern theory of random censoring.},
	language = {EN},
	number = {3},
	urldate = {2020-11-03},
	journal = {Annals of Statistics},
	author = {Jacobsen, Martin and Keiding, Niels},
	month = jun,
	year = {1995},
	mrnumber = {MR1345200},
	zmnumber = {0839.62001},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Missing data, survival analysis, profile likelihood},
	pages = {774--786},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/PPH6H7DH/Jacobsen and Keiding - 1995 - Coarsening at Random in General Sample Spaces and .pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/ZYNPIGS3/1176324622.html:text/html},
}

@article{heitjan_ignorability_1991,
	title = {Ignorability and {Coarse} {Data}},
	volume = {19},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176348396},
	doi = {10.1214/aos/1176348396},
	abstract = {We present a general statistical model for data coarsening, which includes as special cases rounded, heaped, censored, partially categorized and missing data. Formally, with coarse data, observations are made not in the sample space of the random variable of interest, but rather in its power set. Grouping is a special case in which the degree of coarsening is known and nonstochastic. We establish simple conditions under which the possible stochastic nature of the coarsening mechanism can be ignored when drawing Bayesian and likelihood inferences and thus the data can be validly treated as grouped data. The conditions are that the data be coarsened at random, a generalization of the condition missing at random, and that the parameters of the data and the coarsening process be distinct. Applications of the general model and the ignorability condition are illustrated in a numerical example and described briefly in a variety of special cases.},
	language = {EN},
	number = {4},
	urldate = {2020-11-03},
	journal = {Annals of Statistics},
	author = {Heitjan, Daniel F. and Rubin, Donald B.},
	month = dec,
	year = {1991},
	mrnumber = {MR1135174},
	zmnumber = {0745.62004},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {missing data, Censord data, coarsened at random, grouped data, heaped data, interval-censored data, missing at random, rounded data},
	pages = {2244--2253},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/TQ4YU9W4/Heitjan and Rubin - 1991 - Ignorability and Coarse Data.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/SDI64GQ9/1176348396.html:text/html;Snapshot:/home/imbroglio/Zotero/storage/AERP43EW/1176348396.html:text/html},
}

@article{vaart_oracle_2006,
	title = {Oracle inequalities for multi-fold cross validation},
	volume = {24},
	issn = {0721-2631},
	doi = {10.1524/stnd.2006.24.3.351},
	abstract = {We consider choosing an estimator or model from a given class by cross validation consisting of holding a nonneglible fraction of the observations out as a test set. We derive bounds that show that the risk of the resulting procedure is (up to a constant) smaller than the risk of an oracle plus an error which typically grows logarithmically with the number of estimators in the class. We extend the results to penalized cross validation in order to control unbounded loss functions. Applications include regression with squared and absolute deviation loss and classiﬁcation under Tsybakov’s condition.},
	language = {en},
	number = {3},
	journal = {Statistics \& Decisions},
	author = {Vaart, Aad W. van der and Dudoit, Sandrine and Laan, Mark J. van der},
	month = jan,
	year = {2006},
	file = {Vaart et al. - 2006 - Oracle inequalities for multi-fold cross validatio.pdf:/home/imbroglio/Zotero/storage/EK4UN6IF/Vaart et al. - 2006 - Oracle inequalities for multi-fold cross validatio.pdf:application/pdf},
}

@book{hernan_causal_2020,
	address = {Boca Raton},
	title = {Causal {Inference}: {What} {If}},
	language = {en},
	publisher = {Chapman \& Hall/CRC},
	author = {Hernán, MA and Robins, JM},
	year = {2020},
	file = {Hernán and Robins - Causal Inference What If.pdf:/home/imbroglio/Zotero/storage/I2V35226/Hernán and Robins - Causal Inference What If.pdf:application/pdf},
}

@misc{us_food_and_drug_administration_october_nodate,
	title = {October 24-25, 2018: {Meeting} of the {Endocrinologic} and {Metabolic} {Drugs} {Advisory} {Committee} {Meeting} {Announcement}. {October} 2018},
	shorttitle = {October 24-25, 2018},
	abstract = {October 24-25, 2018: Meeting of the Endocrinologic and Metabolic Drugs Advisory Committee Meeting Announcement},
	language = {en},
	author = {U.S. Food {and} Drug Administration},
	file = {Snapshot:/home/imbroglio/Zotero/storage/E7BKWFEI/october-24-25-2018-meeting-endocrinologic-and-metabolic-drugs-advisory-committee-meeting.html:text/html},
}

@article{us_food_and_drug_administration_guidance_nodate,
	title = {Guidance for {Industry} {Diabetes} {Mellitus} — {Evaluating} {Cardiovascular} {Risk} in {New} {Antidiabetic} {Therapies} to {Treat} {Type} 2 {Diabetes}. {December} 2008},
	language = {en},
	author = {{U.S. Food and Drug Administration.}},
	file = {2008 - Guidance for Industry.pdf:/home/imbroglio/Zotero/storage/36IFC9CR/2008 - Guidance for Industry.pdf:application/pdf},
}

@misc{us_food_and_drug_administration_type_nodate,
	title = {Type 2 {Diabetes} {Mellitus}: {Evaluating} the {Safety} of {New} {Drugs} for {Improving} {Glycemic} {Control}: {Guidance} for {Industry}. {March} 2020},
	language = {en},
	author = {U.S. Food {and} Drug Administration},
	file = {2020 - Type 2 Diabetes Mellitus Evaluating the Safety of.pdf:/home/imbroglio/Zotero/storage/B8XQFTMZ/2020 - Type 2 Diabetes Mellitus Evaluating the Safety of.pdf:application/pdf},
}

@article{simon_regularization_2011,
	title = {Regularization {Paths} for {Cox}’s {Proportional} {Hazards} {Model} via {Coordinate} {Descent}},
	volume = {39},
	issn = {1548-7660},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4824408/},
	doi = {10.18637/jss.v039.i05},
	abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of ℓ1 and ℓ2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
	number = {5},
	urldate = {2020-11-02},
	journal = {Journal of statistical software},
	author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
	month = mar,
	year = {2011},
	pmid = {27065756},
	pmcid = {PMC4824408},
	pages = {1--13},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/A5XSWCHS/Simon et al. - 2011 - Regularization Paths for Cox’s Proportional Hazard.pdf:application/pdf},
}

@article{stensrud_exploring_2017,
	title = {Exploring {Selection} {Bias} by {Causal} {Frailty} {Models}: {The} {Magnitude} {Matters}},
	volume = {28},
	issn = {1531-5487},
	shorttitle = {Exploring {Selection} {Bias} by {Causal} {Frailty} {Models}},
	doi = {10.1097/EDE.0000000000000621},
	abstract = {Counter-intuitive associations appear frequently in epidemiology, and these results are often debated. In particular, several scenarios are characterized by a general risk factor that appears protective in particular subpopulations, for example, individuals suffering from a specific disease. However, the associations are not necessarily representing causal effects. Selection bias due to conditioning on a collider may often be involved, and causal graphs are widely used to highlight such biases. These graphs, however, are qualitative, and they do not provide information on the real life relevance of a spurious association. Quantitative estimates of such associations can be obtained from simple statistical models. In this study, we present several paradoxical associations that occur in epidemiology, and we explore these associations in a causal, frailty framework. By using frailty models, we are able to put numbers on spurious effects that often are neglected in epidemiology. We discuss several counter-intuitive findings that have been reported in real life analyses, and we present calculations that may expand the understanding of these associations. In particular, we derive novel expressions to explain the magnitude of bias in index-event studies.},
	language = {eng},
	number = {3},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Stensrud, Mats Julius and Valberg, Morten and Røysland, Kjetil and Aalen, Odd O.},
	year = {2017},
	pmid = {28244888},
	keywords = {Humans, Models, Statistical, Bias, Causality, Proportional Hazards Models, Selection Bias},
	pages = {379--386},
}

@book{pearl_causal_2016,
	address = {Chichester, West Sussex},
	edition = {1st edition},
	title = {Causal {Inference} in {Statistics} - {A} {Primer}},
	isbn = {978-1-119-18684-7},
	abstract = {Many of the concepts and terminology surrounding modern causal inference can be quite intimidating to the novice. Judea Pearl presents a book ideal for beginners in statistics, providing a comprehensive introduction to the field of causality.  Examples from classical statistics are presented throughout to demonstrate the need for causality in resolving decision-making dilemmas posed by data. Causal methods are also compared to traditional statistical methods, whilst questions are provided at the end of each section to aid student learning.},
	language = {English},
	publisher = {Wiley},
	author = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P.},
	month = mar,
	year = {2016},
}

@misc{noauthor_causal_nodate,
	title = {Causal {Inference} in {Statistics}: {A} {Primer} {\textbar} {Wiley}},
	shorttitle = {Causal {Inference} in {Statistics}},
	url = {https://www.wiley.com/en-us/Causal+Inference+in+Statistics%3A+A+Primer-p-9781119186847},
	abstract = {Many of the concepts and terminology surrounding modern causal inference can be quite intimidating to the novice. Judea Pearl presents a book ideal for beginners in statistics, providing a comprehensive introduction to the field of causality. Examples from classical statistics are presented throughout to demonstrate the need for causality in resolving decision-making dilemmas posed by data. Causal methods are also compared to traditional statistical methods, whilst questions are provided at the end of each section to aid student learning.},
	language = {en-us},
	urldate = {2020-11-02},
	journal = {Wiley.com},
	file = {Snapshot:/home/imbroglio/Zotero/storage/KN34LFYF/Causal+Inference+in+Statistics+A+Primer-p-9781119186847.html:text/html},
}

@article{petersen_causal_2014,
	title = {Causal models and learning from data: integrating causal modeling and statistical estimation},
	volume = {25},
	issn = {1531-5487},
	shorttitle = {Causal models and learning from data},
	doi = {10.1097/EDE.0000000000000078},
	abstract = {The practice of epidemiology requires asking causal questions. Formal frameworks for causal inference developed over the past decades have the potential to improve the rigor of this process. However, the appropriate role for formal causal thinking in applied epidemiology remains a matter of debate. We argue that a formal causal framework can help in designing a statistical analysis that comes as close as possible to answering the motivating causal question, while making clear what assumptions are required to endow the resulting estimates with a causal interpretation. A systematic approach for the integration of causal modeling with statistical estimation is presented. We highlight some common points of confusion that occur when causal modeling techniques are applied in practice and provide a broad overview on the types of questions that a causal framework can help to address. Our aims are to argue for the utility of formal causal thinking, to clarify what causal models can and cannot do, and to provide an accessible introduction to the flexible and powerful tools provided by causal models.},
	language = {eng},
	number = {3},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Petersen, Maya L. and van der Laan, Mark J.},
	month = may,
	year = {2014},
	pmid = {24713881},
	pmcid = {PMC4077670},
	keywords = {Humans, Models, Statistical, Causality, Confounding Factors, Epidemiologic, Epidemiologic Methods, Sensitivity and Specificity},
	pages = {418--426},
	file = {Accepted Version:/home/imbroglio/Zotero/storage/BRIFEDS2/Petersen and van der Laan - 2014 - Causal models and learning from data integrating .pdf:application/pdf},
}

@article{chong_assessing_2020,
	title = {Assessing the {Safety} of {Glucose}-{Lowering} {Drugs} — {A} {New} {Focus} for the {FDA}},
	volume = {383},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMp2004889},
	doi = {10.1056/NEJMp2004889},
	number = {13},
	urldate = {2020-11-02},
	journal = {New England Journal of Medicine},
	author = {Chong, William H. and Yanoff, Lisa B. and Andraca-Carrera, Eugenio and Thanh Hai, Mary},
	month = sep,
	year = {2020},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/NEJMp2004889},
	pages = {1199--1202},
	file = {Snapshot:/home/imbroglio/Zotero/storage/FFJ8DRSJ/NEJMp2004889.html:text/html},
}

@article{rubin_estimating_1974,
	title = {Estimating causal effects of treatments in randomized and nonrandomized studies.},
	volume = {66},
	issn = {0022-0663},
	url = {http://content.apa.org/journals/edu/66/5/688},
	doi = {10.1037/h0037350},
	language = {en},
	number = {5},
	urldate = {2020-10-13},
	journal = {Journal of Educational Psychology},
	author = {Rubin, Donald B.},
	year = {1974},
	pages = {688--701},
	file = {Rubin - 1974 - Estimating causal effects of treatments in randomi.pdf:/home/imbroglio/Zotero/storage/KU7G6DVN/Rubin - 1974 - Estimating causal effects of treatments in randomi.pdf:application/pdf},
}

@misc{ich_harmonised_guideline_e9r1_addendum_2019,
	title = {Addendum on {Estimands} and {Sensitivity} {Analysis} in {Clinical} {Trials}},
	author = {ICH Harmonised Guideline E9(R1)},
	year = {2019},
	file = {ADDENDUM ON ESTIMANDS AND SENSITIVITY ANALYSIS IN CLINICAL TRIALS:/home/imbroglio/Zotero/storage/XUNHBVTK/ADDENDUM ON ESTIMANDS AND SENSITIVITY ANALYSIS IN CLINICAL TRIALS.pdf:application/pdf},
}

@article{verma_occurence_2019,
	title = {Occurence of {First} and {Recurrent} {Major} {Adverse} {Cardiovascular} {Events} {With} {Liraglutide} {Treatment} {Among} {Patients} {With} {Type} 2 {Diabetes} and {High} {Risk} of {Cardiovascular} {Events}: {A} {Post} {Hoc} {Analysis} of a {Randomized} {Clinical} {Trial}},
	volume = {4},
	issn = {2380-6583},
	shorttitle = {Occurence of {First} and {Recurrent} {Major} {Adverse} {Cardiovascular} {Events} {With} {Liraglutide} {Treatment} {Among} {Patients} {With} {Type} 2 {Diabetes} and {High} {Risk} of {Cardiovascular} {Events}},
	url = {https://jamanetwork.com/journals/jamacardiology/fullarticle/2754760},
	doi = {10.1001/jamacardio.2019.3080},
	language = {en},
	number = {12},
	urldate = {2020-10-02},
	journal = {JAMA Cardiology},
	author = {Verma, Subodh and Bain, Stephen C. and Buse, John B. and Idorn, Thomas and Rasmussen, Søren and Ørsted, David D. and Nauck, Michael A.},
	month = dec,
	year = {2019},
	pages = {1214},
	file = {Full Text:/home/imbroglio/Zotero/storage/PMPX6RDC/Verma et al. - 2019 - Occurence of First and Recurrent Major Adverse Car.pdf:application/pdf},
}

@article{aroda_incorporating_2019,
	title = {Incorporating and interpreting regulatory guidance on estimands in diabetes clinical trials: {The} {PIONEER} 1 randomized clinical trial as an example},
	volume = {21},
	issn = {1463-1326},
	shorttitle = {Incorporating and interpreting regulatory guidance on estimands in diabetes clinical trials},
	url = {https://dom-pubs.onlinelibrary.wiley.com/doi/abs/10.1111/dom.13804},
	doi = {10.1111/dom.13804},
	abstract = {Regulatory guidelines describe the use of estimands in designing and conducting clinical trials. Estimands ensure alignment of the objectives with the design, conduct and analysis of a trial. An estimand is defined by four inter-related attributes: the population of interest, the variable (endpoint) of interest, the way intercurrent events are handled and the population level summary. A trial may employ multiple estimands to evaluate treatment effects from different perspectives in order to address different scientific questions. As estimands may be an unfamiliar concept for many clinicians treating diabetes, this paper reviews the estimand concept and uses the PIONEER 1 phase 3a clinical trial, which investigated the efficacy and safety of oral semaglutide vs placebo, as an example of the way in which estimands can be implemented and interpreted. In the PIONEER 1 trial, two estimands were employed for each efficacy endpoint and were labelled as: (a) the treatment policy estimand, used to assess the treatment effect regardless of use of rescue medication or discontinuation of trial product, and provides a broad perspective of the treatment effect in the population of patients with type 2 diabetes in clinical practice; and (b) the trial product estimand, used to assess the treatment effect if all patients had continued to use trial product for the planned duration of the trial without rescue medication, thereby providing information on the anticipated treatment effect of the medication. Both approaches are complementary to understanding the effect of the studied treatments.},
	language = {en},
	number = {10},
	urldate = {2020-10-02},
	journal = {Diabetes, Obesity and Metabolism},
	author = {Aroda, Vanita R. and Saugstrup, Trine and Buse, John B. and Donsmark, Morten and Zacho, Jeppe and Davies, Melanie J.},
	year = {2019},
	note = {\_eprint: https://dom-pubs.onlinelibrary.wiley.com/doi/pdf/10.1111/dom.13804},
	keywords = {Diabetes, estimand, GLP-1 receptor agonists, oral semaglutide, PIONEER, regulatory guidance},
	pages = {2203--2210},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/5LA3VNZK/Aroda et al. - 2019 - Incorporating and interpreting regulatory guidance.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/BKEUJ94S/dom.html:text/html},
}

@book{rubin_matched_2006,
	address = {Cambridge},
	title = {Matched {Sampling} for {Causal} {Effects}},
	isbn = {978-0-521-85762-8},
	url = {https://www.cambridge.org/core/books/matched-sampling-for-causal-effects/0A7F262A9D8A8C4F4F745444ED4F50AC},
	abstract = {Matched sampling is often used to help assess the causal effect of some exposure or intervention, typically when randomized experiments are not available or cannot be conducted. This book presents a selection of Donald B. Rubin's research articles on matched sampling, from the early 1970s, when the author was one of the major researchers involved in establishing the field, to recent contributions to this now extremely active area. The articles include fundamental theoretical studies that have become classics, important extensions, and real applications that range from breast cancer treatments to tobacco litigation to studies of criminal tendencies. They are organized into seven parts, each with an introduction by the author that provides historical and personal context and discusses the relevance of the work today. A concluding essay offers advice to investigators designing observational studies. The book provides an accessible introduction to the study of matched sampling and will be an indispensable reference for students and researchers.},
	publisher = {Cambridge University Press},
	author = {Rubin, Donald B.},
	year = {2006},
	doi = {10.1017/CBO9780511810725},
	file = {Snapshot:/home/imbroglio/Zotero/storage/7GFZRPEY/0A7F262A9D8A8C4F4F745444ED4F50AC.html:text/html},
}

@article{cochran_controlling_1973,
	title = {Controlling {Bias} in {Observational} {Studies}: {A} {Review}},
	volume = {35},
	issn = {0581-572X},
	shorttitle = {Controlling {Bias} in {Observational} {Studies}},
	url = {https://www.jstor.org/stable/25049893},
	abstract = {This paper reviews work on the effectiveness of different methods of matched sampling and statistical adjustment, alone and in combination, in reducing bias due to confounding x-variables when comparing two populations. The adjustment methods were linear regression adjustment for x continuous and direct standardization for x categorical. With x continuous, the range of situations examined included linear relations between y and x, parallel and non-parallel, monotonic non-linear parallel relations, equal and unequal variances of x, and the presence of errors of measurement in x. The percent of initial bias \$E({\textbackslash}overline\{y\}\_\{1\}-{\textbackslash}overline\{y\}\_\{2\})\$ that was removed was used as the criterion. Overall, linear regression adjustment on random samples appeared superior to the matching methods, with linear regression adjustment on matched samples the most robust method. Several different approaches were suggested for the case of multivariate x, on which little or no work has been done.},
	number = {4},
	urldate = {2020-09-25},
	journal = {Sankhyā: The Indian Journal of Statistics, Series A (1961-2002)},
	author = {Cochran, William G. and Rubin, Donald B.},
	year = {1973},
	note = {Publisher: Springer},
	pages = {417--446},
}

@article{robins_probability_1989,
	title = {The {Probability} of {Causation} under a {Stochastic} {Model} for {Individual} {Risk}},
	volume = {45},
	issn = {0006-341X},
	url = {http://www.jstor.org/stable/2531765},
	doi = {10.2307/2531765},
	abstract = {In this paper we offer a mathematical definition for the probability of causation that formalizes the legal and ordinary-language meaning of the term. We show that, under this definition, even the average probability of causation among exposed cases is not identifiable from epidemiologic data. This is because the probability of causation depends both on the unknown mechanisms by which exposure affects disease risk and competing risks, and on the unknown degree of heterogeneity in the background disease risk of the exposed population. We derive the maximum and minimum values for the probability of causation consistent with the observable population quantities. We also derive the relationship of the "assigned share" (excess incidence rate as a proportion of total incidence rate) to the probability of causation.},
	number = {4},
	urldate = {2020-09-25},
	journal = {Biometrics},
	author = {Robins, James and Greenland, Sander},
	year = {1989},
	note = {Publisher: [Wiley, International Biometric Society]},
	pages = {1125--1138},
	file = {JSTOR Full Text PDF:/home/imbroglio/Zotero/storage/U7XB2BK3/Robins and Greenland - 1989 - The Probability of Causation under a Stochastic Mo.pdf:application/pdf},
}

@article{moore_increasing_2009,
	title = {Increasing {Power} in {Randomized} {Trials} with {Right} {Censored} {Outcomes} {Through} {Covariate} {Adjustment}},
	volume = {19},
	issn = {1054-3406},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2895464/},
	doi = {10.1080/10543400903243017},
	abstract = {Targeted maximum likelihood methodology is applied to provide a test that makes use of the covariate data that are commonly collected in randomized trials, and does not require assumptions beyond those of the logrank test when censoring is uninformative. Under informative censoring, the logrank test is biased, whereas the test provided in this article is consistent under consistent estimation of the censoring mechanism or the conditional hazard for survival. Two approaches based on this methodology are provided: (1) a substitution-based approach that targets treatment and time-specific survival from which the logrank parameter is estimated, and (2) directly targeting the logrank parameter.},
	number = {6},
	urldate = {2020-08-11},
	journal = {Journal of biopharmaceutical statistics},
	author = {Moore, K. L. and Laan, M. J. van der},
	month = nov,
	year = {2009},
	pmid = {20183467},
	pmcid = {PMC2895464},
	pages = {1099--1131},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/BQHJ2BZY/Moore and Laan - 2009 - INCREASING POWER IN RANDOMIZED TRIALS WITH RIGHT C.pdf:application/pdf},
}

@article{hernan_structural_2004,
	title = {A {Structural} {Approach} to {Selection} {Bias}:},
	volume = {15},
	issn = {1044-3983},
	shorttitle = {A {Structural} {Approach} to {Selection} {Bias}},
	url = {http://journals.lww.com/00001648-200409000-00020},
	doi = {10.1097/01.ede.0000135174.63482.43},
	abstract = {The term “selection bias” encompasses various biases in epidemiology. We describe examples of selection bias in casecontrol studies (eg, inappropriate selection of controls) and cohort studies (eg, informative censoring). We argue that the causal structure underlying the bias in each example is essentially the same: conditioning on a common effect of 2 variables, one of which is either exposure or a cause of exposure and the other is either the outcome or a cause of the outcome. This structure is shared by other biases (eg, adjustment for variables affected by prior exposure). A structural classiﬁcation of bias distinguishes between biases resulting from conditioning on common effects (“selection bias”) and those resulting from the existence of common causes of exposure and outcome (“confounding”). This classiﬁcation also leads to a uniﬁed approach to adjust for selection bias.},
	language = {en},
	number = {5},
	urldate = {2020-09-23},
	journal = {Epidemiology},
	author = {Hernán, Miguel A. and Hernández-Díaz, Sonia and Robins, James M.},
	month = sep,
	year = {2004},
	pages = {615--625},
	file = {Hernán et al. - 2004 - A Structural Approach to Selection Bias.pdf:/home/imbroglio/Zotero/storage/KIYZCBBF/Hernán et al. - 2004 - A Structural Approach to Selection Bias.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/6CDFZJFC/A_Structural_Approach_to_Selection_Bias.20.html:text/html},
}

@article{rajkumar_lenalidomide_2010,
	title = {Lenalidomide plus high-dose dexamethasone versus lenalidomide plus low-dose dexamethasone as initial therapy for newly diagnosed multiple myeloma: an open-label randomised controlled trial},
	volume = {11},
	issn = {1470-2045},
	shorttitle = {Lenalidomide plus high-dose dexamethasone versus lenalidomide plus low-dose dexamethasone as initial therapy for newly diagnosed multiple myeloma},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3042271/},
	doi = {10.1016/S1470-2045(09)70284-0},
	abstract = {Background
High-dose dexamethasone is a mainstay of therapy for multiple myeloma. We studied whether low-dose dexamethasone in combination with lenalidomide is non-inferior to and has lower toxicity than high-dose dexamethasone plus lenalidomide.

Methods
Patients with untreated symptomatic myeloma were randomly assigned in this open-label non-inferiority trial to lenalidomide 25 mg on days 1–21 plus dexamethasone 40 mg on days 1–4, 9–12, and 17–20 of a 28-day cycle (high dose), or lenalidomide given on the same schedule with dexamethasone 40 mg on days 1, 8, 15, and 22 of a 28-day cycle (low dose). After four cycles, patients could discontinue therapy to pursue stem-cell transplantation or continue treatment until disease progression. The primary endpoint was response rate after four cycles assessed with European Group for Blood and Bone Marrow Transplant criteria. The non-inferiority margin was an absolute difference of 15\% in response rate. Analysis was by modified intention to treat. This trial is registered with ClinicalTrials.gov, number NCT00098475.

Findings
445 patients were randomly assigned: 223 to high-dose and 222 to low-dose regimens. 169 (79\%) of 214 patients receiving high-dose therapy and 142 (68\%) of 205 patients on low-dose therapy had complete or partial response within four cycles (odds ratio 1·75, 80\% CI 1·30–2·32; p=0.008). However, at the second interim analysis at 1 year, overall survival was 96\% (95\% CI 94–99) in the low-dose dexamethasone group compared with 87\% (82–92) in the high-dose group (p=0·0002). As a result, the trial was stopped and patients on high-dose therapy were crossed over to low-dose therapy. 117 patients (52\%) on the high-dose regimen had grade three or worse toxic effects in the first 4 months, compared with 76 (35\%) of the 220 on the low-dose regimen for whom toxicity data were available (p=0·0001), 12 of 222 on high dose and one of 220 on low-dose dexamethasone died in the first 4 months (p=0·003). The three most common grade three or higher toxicities were deep-vein thrombosis, 57 (26\%) of 223 versus 27 (12\%) of 220 (p=0·0003); infections including pneumonia, 35 (16\%) of 223 versus 20 (9\%) of 220 (p=0·04), and fatigue 33 (15\%) of 223 versus 20 (9\%) of 220 (p=0·08), respectively.

Interpretation
Lenalidomide plus low-dose dexamethasone is associated with better short-term overall survival and with lower toxicity than lenalidomide plus high-dose dexamethasone in patients with newly diagnosed myeloma.},
	number = {1},
	urldate = {2020-09-21},
	journal = {The lancet oncology},
	author = {Rajkumar, SV and Greipp, P R and Jacobus, S and Callander, N S and Fonseca, R and Vesole, DH and Williams, ME and Abonour, R and Siegel, DS and Katz, M},
	month = jan,
	year = {2010},
	pmid = {19853510},
	pmcid = {PMC3042271},
	pages = {29--37},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/DBBFB5T9/Rajkumar et al. - 2010 - Lenalidomide plus high-dose dexamethasone versus l.pdf:application/pdf},
}

@article{marso_liraglutide_2016,
	title = {Liraglutide and {Cardiovascular} {Outcomes} in {Type} 2 {Diabetes}},
	volume = {375},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMoa1603827},
	doi = {10.1056/NEJMoa1603827},
	language = {en},
	number = {4},
	urldate = {2021-12-15},
	journal = {New England Journal of Medicine},
	author = {Marso, Steven P. and Daniels, Gilbert H. and Brown-Frandsen, Kirstine and Kristensen, Peter and Mann, Johannes F.E. and Nauck, Michael A. and Nissen, Steven E. and Pocock, Stuart and Poulter, Neil R. and Ravn, Lasse S. and Steinberg, William M. and Stockner, Mette and Zinman, Bernard and Bergenstal, Richard M. and Buse, John B.},
	month = jul,
	year = {2016},
	pages = {311--322},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/YZH3QBQF/Marso et al. - 2016 - Liraglutide and Cardiovascular Outcomes in Type 2 .pdf:application/pdf;Marso et al. - 2016 - Liraglutide and Cardiovascular Outcomes in Type 2 .pdf:/home/imbroglio/Zotero/storage/MGBHZ9AY/Marso et al. - 2016 - Liraglutide and Cardiovascular Outcomes in Type 2 .pdf:application/pdf;Marso et al. - 2016 - Liraglutide and Cardiovascular Outcomes in Type 2 .pdf:/home/imbroglio/Zotero/storage/ZQKJYJ6C/Marso et al. - 2016 - Liraglutide and Cardiovascular Outcomes in Type 2 .pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/C5IW24YP/NEJMoa1603827.html:text/html},
}

@incollection{kennedy_semiparametric_2016,
	address = {Cham},
	series = {{ICSA} {Book} {Series} in {Statistics}},
	title = {Semiparametric {Theory} and {Empirical} {Processes} in {Causal} {Inference}},
	isbn = {978-3-319-41259-7},
	url = {https://doi.org/10.1007/978-3-319-41259-7_8},
	abstract = {In this paper we review important aspects of semiparametric theory and empirical processes that arise in causal inference problems. We begin with a brief introduction to the general problem of causal inference, and go on to discuss estimation and inference for causal effects under semiparametric models, which allow parts of the data-generating process to be unrestricted if they are not of particular interest (i.e., nuisance functions). These models are very useful in causal problems because the outcome process is often complex and difficult to model, and there may only be information available about the treatment process (at best). Semiparametric theory gives a framework for benchmarking efficiency and constructing estimators in such settings. In the second part of the paper we discuss empirical process theory, which provides powerful tools for understanding the asymptotic behavior of semiparametric estimators that depend on flexible nonparametric estimators of nuisance functions. These tools are crucial for incorporating machine learning and other modern methods into causal inference analyses. We conclude by examining related extensions and future directions for work in semiparametric causal inference.},
	language = {en},
	urldate = {2022-10-07},
	booktitle = {Statistical {Causal} {Inferences} and {Their} {Applications} in {Public} {Health} {Research}},
	publisher = {Springer International Publishing},
	author = {Kennedy, Edward H.},
	editor = {He, Hua and Wu, Pan and Chen, Ding-Geng (Din)},
	year = {2016},
	doi = {10.1007/978-3-319-41259-7_8},
	keywords = {Influence Function, Causal Inference, Empirical Process, Propensity Score, Tangent Space},
	pages = {141--167},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/TCL2DVZH/Kennedy - 2016 - Semiparametric Theory and Empirical Processes in C.pdf:application/pdf},
}

@article{holland_statistics_1986,
	title = {Statistics and {Causal} {Inference}},
	volume = {81},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2289064},
	doi = {10.2307/2289064},
	abstract = {Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling.},
	number = {396},
	urldate = {2022-10-07},
	journal = {Journal of the American Statistical Association},
	author = {Holland, Paul W.},
	year = {1986},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {945--960},
	file = {JSTOR Full Text PDF:/home/imbroglio/Zotero/storage/4XKRHJQU/Holland - 1986 - Statistics and Causal Inference.pdf:application/pdf},
}

@article{laan_statistical_2006,
	title = {Statistical {Inference} for {Variable} {Importance}},
	volume = {2},
	issn = {1557-4679},
	url = {https://www.degruyter.com/document/doi/10.2202/1557-4679.1008/html?lang=en},
	doi = {10.2202/1557-4679.1008},
	abstract = {Many statistical problems involve the learning of an importance/effect of a variable for predicting an outcome of interest based on observing a sample of \$n\$ independent and identically distributed observations on a list of input variables and an outcome. For example, though prediction/machine learning is, in principle, concerned with learning the optimal unknown mapping from input variables to an outcome from the data, the typical reported output is a list of importance measures for each input variable. The approach in prediction has been to learn the unknown optimal predictor from the data and derive, for each of the input variables, the variable importance from the obtained fit. In this article we propose a new approach which involves for each variable separately 1) defining variable importance as a real valued parameter, 2) deriving the efficient influence curve and thereby optimal estimating function for this parameter in the assumed (possibly nonparametric) model, and 3) develop a corresponding double robust locally efficient estimator of this variable importance, obtained by substituting for the nuisance parameters in the optimal estimating function data adaptive estimators. We illustrate this methodology in the context of prediction, and obtain in this manner double robust locally optimal estimators of marginal variable importance, accompanied with p-values and confidence intervals. In addition, we present a model based and machine learning approach to estimate covariate-adjusted variable importance. Finally, we generalize this methodology to variable importance parameters for time-dependent variables.},
	language = {en},
	number = {1},
	urldate = {2022-09-25},
	journal = {The International Journal of Biostatistics},
	author = {Laan, Mark J. van der},
	month = feb,
	year = {2006},
	note = {Publisher: De Gruyter},
	keywords = {causal effect, efficient influence curve, prediction, estimating function, variable importance, adjusted-variable importance},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/XC22XPE6/Laan - 2006 - Statistical Inference for Variable Importance.pdf:application/pdf},
}

@misc{wallace_dtrreg_2020,
	title = {{DTRreg}: {DTR} {Estimation} and {Inference} via {G}-{Estimation}, {Dynamic} {WOLS}, {Q}-{Learning}, and {Dynamic} {Weighted} {Survival} {Modeling} ({DWSurv})},
	copyright = {GPL-2},
	shorttitle = {{DTRreg}},
	url = {https://CRAN.R-project.org/package=DTRreg},
	abstract = {Dynamic treatment regime estimation and inference via G-estimation, dynamic weighted ordinary least squares (dWOLS) and Q-learning. Inference via bootstrap and (for G-estimation) recursive sandwich estimation. Estimation and inference for survival outcomes via Dynamic Weighted Survival Modeling (DWSurv). Extension to continuous treatment variables (gdwols). Wallace et al. (2017) {\textless}doi:10.18637/jss.v080.i02{\textgreater}; Simoneau et al. (2020) {\textless}doi:10.1080/00949655.2020.1793341{\textgreater}.},
	urldate = {2022-09-25},
	author = {Wallace, Michael and Moodie, Erica E. M. and Stephens, David A. and Schulz, Gabrielle Simoneau {and} Juliana},
	month = sep,
	year = {2020},
	keywords = {CausalInference},
}

@misc{gerds_riskregression_2022,
	title = {{riskRegression}: {Risk} {Regression} {Models} and {Prediction} {Scores} for {Survival} {Analysis} with {Competing} {Risks}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {{riskRegression}},
	url = {https://CRAN.R-project.org/package=riskRegression},
	abstract = {Implementation of the following methods for event history analysis. Risk regression models for survival endpoints also in the presence of competing risks are fitted using binomial regression based on a time sequence of binary event status variables. A formula interface for the Fine-Gray regression model and an interface for the combination of cause-specific Cox regression models. A toolbox for assessing and comparing performance of risk predictions (risk markers and risk prediction models). Prediction performance is measured by the Brier score and the area under the ROC curve for binary possibly time-dependent outcome. Inverse probability of censoring weighting and pseudo values are used to deal with right censored data. Lists of risk markers and lists of risk models are assessed simultaneously. Cross-validation repeatedly splits the data, trains the risk prediction models on one part of each split and then summarizes and compares the performance across splits.},
	urldate = {2022-09-25},
	author = {Gerds, Thomas Alexander and Ohlendorff, Johan Sebastian and Blanche, Paul and Mortensen, Rikke and Wright, Marvin and Tollenaar, Nikolaj and Muschelli, John and Mogensen, Ulla Brasch and Ozenne, Brice},
	month = sep,
	year = {2022},
	keywords = {CausalInference, Survival},
}

@misc{polley_superlearner_2021,
	title = {{SuperLearner}: {Super} {Learner} {Prediction}},
	copyright = {GPL-3},
	shorttitle = {{SuperLearner}},
	url = {https://CRAN.R-project.org/package=SuperLearner},
	abstract = {Implements the super learner prediction method and contains a library of prediction algorithms to be used in the super learner.},
	urldate = {2022-09-25},
	author = {Polley, Eric and LeDell, Erin and Kennedy, Chris and Lendle, Sam and Laan, Mark van der},
	month = may,
	year = {2021},
	keywords = {MachineLearning},
}

@misc{benkeser_survtmle_2019,
	title = {survtmle: {Compute} {Targeted} {Minimum} {Loss}-{Based} {Estimates} in {Right}-{Censored} {Survival} {Settings}},
	copyright = {MIT + file LICENSE},
	shorttitle = {survtmle},
	url = {https://CRAN.R-project.org/package=survtmle},
	abstract = {Targeted estimates of marginal cumulative incidence in survival settings with and without competing risks, including estimators that respect bounds (Benkeser, Carone, and Gilbert. Statistics in Medicine, 2017. {\textless}doi:10.1002/sim.7337{\textgreater}).},
	urldate = {2022-09-25},
	author = {Benkeser, David and Hejazi, Nima},
	month = apr,
	year = {2019},
}

@misc{schwab_ltmle_2020,
	title = {ltmle: {Longitudinal} {Targeted} {Maximum} {Likelihood} {Estimation}},
	copyright = {GPL-2},
	shorttitle = {ltmle},
	url = {https://CRAN.R-project.org/package=ltmle},
	abstract = {Targeted Maximum Likelihood Estimation (TMLE) of treatment/censoring specific mean outcome or marginal structural model for point-treatment and longitudinal data.},
	urldate = {2022-09-25},
	author = {Schwab, Joshua and Lendle, Samuel and Petersen, Maya and Laan, Mark van der and Gruber, Susan},
	month = mar,
	year = {2020},
	keywords = {CausalInference},
}

@misc{sofrygin_stremr_2017,
	title = {stremr: {Streamlined} {Estimation} of {Survival} for {Static}, {Dynamic} and {Stochastic} {Treatment} and {Monitoring} {Regimes}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {stremr},
	url = {https://CRAN.R-project.org/package=stremr},
	abstract = {Analysis of longitudinal time-to-event or time-to-failure data. Estimates the counterfactual discrete survival curve under static, dynamic and stochastic interventions on treatment (exposure) and monitoring events over time. Estimators (IPW, MSM-IPW, GCOMP, longitudinal TMLE) adjust for measured time-varying confounding and informative right-censoring. Model fitting can be performed either with GLM or H2O-3 machine learning libraries. The exposure, monitoring and censoring variables can be coded as either binary, categorical or continuous. Each can be multivariate (e.g., can use more than one column of dummy indicators for different censoring events). The input data needs to be in long format.},
	urldate = {2022-09-25},
	author = {Sofrygin, Oleg and Laan, Mark J. van der and Neugebauer, Romain},
	month = jan,
	year = {2017},
}

@article{van_der_laan_locally_2002,
	title = {Locally {Efficient} {Estimation} of a {Multivariate} {Survival} {Function} in {Longitudinal} {Studies}},
	volume = {97},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214502760047023},
	doi = {10.1198/016214502760047023},
	abstract = {We consider estimation of the joint distribution of multivariate survival times T = (T1,…,Tk), which are subject to right-censoring by a common censoring variableC. Two estimators are proposed: an initial inverse-probability-of-censoring weighted (IPCW) estimator and a one-step estimator. Both estimators incorporate information on available time-independent and time-dependent prognostic factor (covariate) data. The IPCW estimator is consistent and asymptotically normal (CAN) under coarsening at random (CAR) and a correct specification of a model for the hazard of censoring given the past covariate and failure data. The one-step estimator is a locally efficient doubly robust estimator. That is, (i) it is CAN under the assumption of CAR and either (but not necessarily both) correct specification of a model for the hazard of censoring given the past or correct specification of a model for the conditional distribution of T given past failure and covariate information, and (ii) it is efficient when both these models are correctly specified. The proposed methodology does not require that the time variables T1,…,Tk be ordered, although our methods cover this important special case. In particular, our estimators can be used to estimate the gap time distributions associated with an ordered series of events. The proposed methodology is an improvement over currently available approaches in a number of ways. Specifically, when censoring and failure are dependent because the hazard of censoring depends on both past failure and covariate history, our one-step estimator is the only estimator with the double-robustness property. When censoring can be assumed to be independent of the failure and covariate processes, our locally efficient one-step estimator, unlike the maximum likelihood estimator (MLE) of van der Laan but like the estimators of Dabrowska, Prentice and Cai, and Bickel, does not require smoothing and so will perform well in moderate size samples even if k is large, say 7 or 8; furthermore, unlike all previous estimators, our estimator exploits the information available in past covariate as well as failure history and so will be efficient (nearly efficient) even when the components of T are highly dependent, whenever the specified model for the conditional distribution of T given past failure and covariate information is correct (nearly correct). We examine the finite sample performance of our estimators in a simulation study. Finally, we apply our estimators to data on time to wound excision and time to wound infection in a population of burn victims.},
	number = {458},
	urldate = {2022-09-21},
	journal = {Journal of the American Statistical Association},
	author = {van der Laan, Mark J and Hubbard, Alan E and Robins, James M},
	month = jun,
	year = {2002},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214502760047023},
	keywords = {Asymptotically efficient, Asymptotically linear estimator, Cox proportional-hazards model, Influence curve, Multivariate right-censored data},
	pages = {494--507},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/AB9K9QAX/van der Laan et al. - 2002 - Locally Efficient Estimation of a Multivariate Sur.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/M67ZAILZ/016214502760047023.html:text/html},
}

@misc{chen_xgboost_2022,
	title = {xgboost: {Extreme} {Gradient} {Boosting}},
	copyright = {Apache License (== 2.0) {\textbar} file LICENSE},
	shorttitle = {xgboost},
	url = {https://CRAN.R-project.org/package=xgboost},
	abstract = {Extreme Gradient Boosting, which is an efficient implementation of the gradient boosting framework from Chen \& Guestrin (2016) {\textless}doi:10.1145/2939672.2939785{\textgreater}. This package is its R interface. The package includes efficient linear model solver and tree learning algorithms. The package can automatically do parallel computation on a single machine which could be more than 10 times faster than existing gradient boosting packages. It supports various objective functions, including regression, classification and ranking. The package is made to be extensible, so that users are also allowed to define their own objectives easily.},
	urldate = {2022-06-03},
	author = {Chen, Tianqi and He, Tong and Benesty, Michael and Khotilovich, Vadim and Tang, Yuan and Cho, Hyunsu and Chen, Kailong and Mitchell, Rory and Cano, Ignacio and Zhou, Tianyi and Li, Mu and Xie, Junyuan and Lin, Min and Geng, Yifeng and Li, Yutian and Yuan, Jiaming and implementation), XGBoost contributors (base XGBoost},
	month = apr,
	year = {2022},
	keywords = {MachineLearning, HighPerformanceComputing, ModelDeployment},
}

@misc{gelman_arm_2020,
	title = {arm: {Data} {Analysis} {Using} {Regression} and {Multilevel}/{Hierarchical} {Models}},
	copyright = {GPL ({\textgreater} 2)},
	shorttitle = {arm},
	url = {https://CRAN.R-project.org/package=arm},
	abstract = {Functions to accompany A. Gelman and J. Hill, Data Analysis Using Regression and Multilevel/Hierarchical Models, Cambridge University Press, 2007.},
	urldate = {2022-06-03},
	author = {Gelman, Andrew and Su, Yu-Sung},
	year = {2020},
	keywords = {Bayesian, TeachingStatistics},
}

@article{wright_ranger_2017,
	title = {ranger: {A} {Fast} {Implementation} of {Random} {Forests} for {High} {Dimensional} {Data} in {C}++ and {R}},
	volume = {77},
	copyright = {Copyright (c) 2017 Marvin N. Wright, Andreas Ziegler},
	issn = {1548-7660},
	shorttitle = {ranger},
	url = {https://doi.org/10.18637/jss.v077.i01},
	doi = {10.18637/jss.v077.i01},
	abstract = {We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study.},
	language = {en},
	urldate = {2022-06-03},
	journal = {Journal of Statistical Software},
	author = {Wright, Marvin N. and Ziegler, Andreas},
	month = mar,
	year = {2017},
	keywords = {survival analysis},
	pages = {1--17},
	file = {Full Text:/home/imbroglio/Zotero/storage/5ZQ4CK7S/Wright and Ziegler - 2017 - ranger A Fast Implementation of Random Forests fo.pdf:application/pdf},
}

@misc{wright_ranger_2021,
	title = {\{ranger\}: {A} {Fast} {Implementation} of {Random} {Forests} for {High} {Dimensional} {Data} in \{{C}++\} and \{{R}\}},
	copyright = {GPL-3},
	shorttitle = {ranger},
	url = {https://CRAN.R-project.org/package=ranger},
	abstract = {A fast implementation of Random Forests, particularly suited for high dimensional data. Ensembles of classification, regression, survival and probability prediction trees are supported. Data from genome-wide association studies can be analyzed efficiently. In addition to data frames, datasets of class 'gwaa.data' (R package 'GenABEL') and 'dgCMatrix' (R package 'Matrix') can be directly analyzed.},
	urldate = {2022-06-03},
	author = {Wright, Marvin N. and Wager, Stefan and Probst, Philipp},
	month = jul,
	year = {2021},
	keywords = {Survival, MachineLearning},
}

@misc{noauthor_xgboost_nodate,
	title = {{XGBoost} {\textbar} {Proceedings} of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	url = {https://dl.acm.org/doi/abs/10.1145/2939672.2939785},
	language = {en},
	urldate = {2022-06-03},
	journal = {ACM Conferences},
	file = {Snapshot:/home/imbroglio/Zotero/storage/4ASYRAGD/2939672.html:text/html},
}

@article{friedman_regularization_2010,
	title = {Regularization {Paths} for {Generalized} {Linear} {Models} via {Coordinate} {Descent}},
	volume = {33},
	copyright = {Copyright (c) 2009 Jerome H. Friedman, Trevor Hastie, Rob Tibshirani},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v033.i01},
	doi = {10.18637/jss.v033.i01},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multi- nomial regression problems while the penalties include ℓ1 (the lasso), ℓ2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
	language = {en},
	urldate = {2022-06-03},
	journal = {Journal of Statistical Software},
	author = {Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Rob},
	month = feb,
	year = {2010},
	pages = {1--22},
	file = {Submitted Version:/home/imbroglio/Zotero/storage/F2AID4RN/Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf:application/pdf},
}

@article{rytgaard_one-step_2021,
	title = {One-step {TMLE} for targeting cause-specific absolute risks and survival curves},
	url = {http://arxiv.org/abs/2107.01537},
	abstract = {This paper considers one-step targeted maximum likelihood estimation method for general competing risks and survival analysis settings where event times take place on the positive real line R+ and are subject to right-censoring. Our interest is overall in the effects of baseline treatment decisions, static, dynamic or stochastic, possibly confounded by pre-treatment covariates. We point out two overall contributions of our work. First, our method can be used to obtain simultaneous inference across all absolute risks in competing risks settings. Second, we present a practical result for achieving inference for the full survival curve, or a full absolute risk curve, across time by targeting over a fine enough grid of points. The one-step procedure is based on a one-dimensional universal least favorable submodel for each cause-specific hazard that can be implemented in recursive steps along a corresponding universal least favorable submodel. We present a theorem for conditions to achieve weak convergence of the estimator for an infinite-dimensional target parameter. Our empirical study demonstrates the use of the methods.},
	urldate = {2022-04-20},
	journal = {arXiv:2107.01537 [stat]},
	author = {Rytgaard, Helene C. W. and van der Laan, Mark J.},
	month = sep,
	year = {2021},
	note = {arXiv: 2107.01537
version: 2},
	keywords = {Statistics - Methodology},
	annote = {Comment: 21 pages (including appendix), 1 figure, 5 tables},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/GYPIFDDQ/Rytgaard and van der Laan - 2021 - One-step TMLE for targeting cause-specific absolut.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/V455MJ7G/2107.html:text/html},
}

@misc{phillips_practical_2022,
	title = {Practical considerations for specifying a super learner},
	url = {http://arxiv.org/abs/2204.06139},
	abstract = {Common tasks encountered in epidemiology, including disease incidence estimation and causal inference, rely on predictive modeling. Constructing a predictive model can be thought of as learning a prediction function, i.e., a function that takes as input covariate data and outputs a predicted value. Many strategies for learning these functions from data are available, from parametric regressions to machine learning algorithms. It can be challenging to choose an approach, as it is impossible to know in advance which one is the most suitable for a particular dataset and prediction task at hand. The super learner (SL) is an algorithm that alleviates concerns over selecting the one "right" strategy while providing the freedom to consider many of them, such as those recommended by collaborators, used in related research, or specified by subject-matter experts. It is an entirely pre-specified and data-adaptive strategy for predictive modeling. To ensure the SL is well-specified for learning the prediction function, the analyst does need to make a few important choices. In this Education Corner article, we provide step-by-step guidelines for making these choices, walking the reader through each of them and providing intuition along the way. In doing so, we aim to empower the analyst to tailor the SL specification to their prediction task, thereby ensuring their SL performs as well as possible. A flowchart provides a concise, easy-to-follow summary of key suggestions and heuristics, based on our accumulated experience, and guided by theory.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Phillips, Rachael V. and van der Laan, Mark J. and Lee, Hana and Gruber, Susan},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06139 [stat]},
	keywords = {Statistics - Applications, Statistics - Methodology, Statistics - Computation},
	annote = {Comment: This article has been submitted for publication as an Education Corner article in the International Journal of Epidemiology published by Oxford University Press},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/DTWXA3IQ/Phillips et al. - 2022 - Practical considerations for specifying a super le.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/S6P5WB2H/2204.html:text/html},
}

@misc{denz_comparison_2022,
	title = {A {Comparison} of {Different} {Methods} to {Adjust} {Survival} {Curves} for {Confounders}},
	url = {http://arxiv.org/abs/2203.10002},
	doi = {10.48550/arXiv.2203.10002},
	abstract = {Treatment specific survival curves are an important tool to illustrate the treatment effect in studies with time-to-event outcomes. In non-randomized studies, unadjusted estimates can lead to biased depictions due to confounding. Multiple methods to adjust survival curves for confounders exist. However, it is currently unclear which method is the most appropriate in which situation. Our goal is to compare these methods in different scenarios with a focus on their bias and goodness-of-fit. We provide a short review of all methods and illustrate their usage by contrasting the survival of smokers and non-smokers, using data from the German Epidemiological Trial on Ankle Brachial Index. Subsequently, we compare the methods using a Monte-Carlo simulation. We consider scenarios in which correctly or incorrectly specified covariate sets for describing the treatment assignment and the time-to-event outcome are used with varying sample sizes. The bias and goodness-of-fit is determined by summary statistics which take into account the entire survival curve. When used properly, all methods showed no systematic bias in medium to large samples. Cox-Regression based methods, however, showed systematic bias in small samples. The goodness-of-fit varied greatly between different methods and scenarios. Methods utilizing an outcome model were more efficient than other techniques, while augmented estimators using an additional treatment assignment model were unbiased when either model was correct with a goodness-of-fit comparable to other methods. These "doubly-robust" methods have important advantages in every considered scenario. Pseudo-Value based methods, coupled with isotonic regression to correct for non-monotonicity, are viable alternatives to traditional methods.},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Denz, Robin and Klaaßen-Mielke, Renate and Timmesfeld, Nina},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2203.10002
arXiv:2203.10002 [stat]},
	keywords = {Statistics - Methodology},
	annote = {Comment: 26 pages, 5 figures, submitted to "Statistics in Medicine" as research article, accepted for oral presentation at the International Biometric Conference 2022},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/9JW5FYKM/Denz et al. - 2022 - A Comparison of Different Methods to Adjust Surviv.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/E8FZL9TQ/2203.html:text/html},
}

@misc{westling_inference_2021,
	title = {Inference for treatment-specific survival curves using machine learning},
	url = {http://arxiv.org/abs/2106.06602},
	abstract = {In the absence of data from a randomized trial, researchers often aim to use observational data to draw causal inference about the effect of a treatment on a time-to-event outcome. In this context, interest often focuses on the treatment-specific survival curves; that is, the survival curves were the entire population under study to be assigned to receive the treatment or not. Under certain causal conditions, including that all confounders of the treatment-outcome relationship are observed, the treatment-specific survival can be identified with a covariate-adjusted survival function. Several estimators of this function have been proposed, including estimators based on outcome regression, inverse probability weighting, and doubly robust estimators. In this article, we propose a new cross-fitted doubly-robust estimator that incorporates data-adaptive (e.g. machine learning) estimators of the conditional survival functions. We establish conditions on the nuisance estimators under which our estimator is consistent and asymptotically linear, both pointwise and uniformly in time. We also propose a novel ensemble learner for combining multiple candidate estimators of the conditional survival estimators. Notably, our methods and results accommodate events occurring in discrete or continuous time (or both). We investigate the practical performance of our methods using numerical studies and an application to the effect of a surgical treatment to prevent metastases of parotid carcinoma on mortality.},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Westling, Ted and Luedtke, Alex and Gilbert, Peter and Carone, Marco},
	month = jun,
	year = {2021},
	note = {Number: arXiv:2106.06602
arXiv:2106.06602 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/KV7I36NK/Westling et al. - 2021 - Inference for treatment-specific survival curves u.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/SXQSG6BH/2106.html:text/html},
}

@misc{cui_estimating_2022,
	title = {Estimating heterogeneous treatment effects with right-censored data via causal survival forests},
	url = {http://arxiv.org/abs/2001.09887},
	doi = {10.48550/arXiv.2001.09887},
	abstract = {Forest-based methods have recently gained in popularity for non-parametric treatment effect estimation. Building on this line of work, we introduce causal survival forests, which can be used to estimate heterogeneous treatment effects in a survival and observational setting where outcomes may be right-censored. Our approach relies on orthogonal estimating equations to robustly adjust for both censoring and selection effects under unconfoundedness. In our experiments, we find our approach to perform well relative to a number of baselines.},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Cui, Yifan and Kosorok, Michael R. and Sverdrup, Erik and Wager, Stefan and Zhu, Ruoqing},
	month = sep,
	year = {2022},
	note = {Number: arXiv:2001.09887
arXiv:2001.09887 [cs, stat]},
	keywords = {Statistics - Methodology, 62N01, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/2FWR4EPB/Cui et al. - 2022 - Estimating heterogeneous treatment effects with ri.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/2Y6EEQTP/2001.html:text/html},
}

@article{leger_causal_nodate,
	title = {Causal inference in case of near-violation of positivity: comparison of methods},
	volume = {n/a},
	issn = {1521-4036},
	shorttitle = {Causal inference in case of near-violation of positivity},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.202000323},
	doi = {10.1002/bimj.202000323},
	abstract = {In causal studies, the near-violation of the positivity may occur by chance, because of sample-to-sample fluctuation despite the theoretical veracity of the positivity assumption in the population. It may mostly happen when the exposure prevalence is low or when the sample size is small. We aimed to compare the robustness of g-computation (GC), inverse probability weighting (IPW), truncated IPW, targeted maximum likelihood estimation (TMLE), and truncated TMLE in this situation, using simulations and one real application. We also tested different extrapolation situations for the sub-group with a positivity violation. The results illustrated that the near-violation of the positivity impacted all methods. We demonstrated the robustness of GC and TMLE-based methods. Truncation helped in limiting the bias in near-violation situations, but at the cost of bias in normal conditions. The application illustrated the variability of the results between the methods and the importance of choosing the most appropriate one. In conclusion, compared to propensity score-based methods, methods based on outcome regression should be preferred when suspecting near-violation of the positivity assumption.},
	language = {en},
	number = {n/a},
	urldate = {2022-12-09},
	journal = {Biometrical Journal},
	author = {Léger, Maxime and Chatton, Arthur and Le Borgne, Florent and Pirracchio, Romain and Lasocki, Sigismond and Foucher, Yohann},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.202000323},
	keywords = {propensity score, causal inference, simulations, doubly robust estimators, g-computation, positivity, real-world evidence},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/MFD484Y5/Léger et al. - Causal inference in case of near-violation of posi.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/Y7E2CLLQ/bimj.html:text/html},
}

@article{dutz_competing_2019,
	title = {Competing risks in survival data analysis},
	volume = {130},
	issn = {0167-8140},
	url = {https://www.sciencedirect.com/science/article/pii/S016781401833490X},
	doi = {10.1016/j.radonc.2018.09.007},
	abstract = {Clinical trials and retrospective studies in the field of radiation oncology often consider time-to-event data as their primary endpoint. Such studies are susceptible to competing risks, i.e. competing events may preclude the occurrence of the event of interest or modify the chance that the primary endpoint occurs. Competing risks are frequently neglected and the event of interest is analysed with standard statistical methods. Here, we would like to create awareness of the problem and demonstrate different methods for survival data analysis in the presence of competing risks.},
	language = {en},
	urldate = {2023-03-16},
	journal = {Radiotherapy and Oncology},
	author = {Dutz, Almut and Löck, Steffen},
	month = jan,
	year = {2019},
	keywords = {Competing risk, Cox regression, Survival data, Time-to-event data},
	pages = {185--189},
	file = {ScienceDirect Snapshot:/home/imbroglio/Zotero/storage/8RYFWCIR/S016781401833490X.html:text/html},
}

@article{poythress_planning_2020,
	title = {Planning and analyzing clinical trials with competing risks: {Recommendations} for choosing appropriate statistical methodology},
	volume = {19},
	issn = {1539-1612},
	shorttitle = {Planning and analyzing clinical trials with competing risks},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.1966},
	doi = {10.1002/pst.1966},
	abstract = {In the analysis of time-to-event data, competing risks occur when multiple event types are possible, and the occurrence of a competing event precludes the occurrence of the event of interest. In this situation, statistical methods that ignore competing risks can result in biased inference regarding the event of interest. We review the mechanisms that lead to bias and describe several statistical methods that have been proposed to avoid bias by formally accounting for competing risks in the analyses of the event of interest. Through simulation, we illustrate that Gray's test should be used in lieu of the logrank test for nonparametric hypothesis testing. We also compare the two most popular models for semiparametric modelling: the cause-specific hazards (CSH) model and Fine-Gray (F-G) model. We explain how to interpret estimates obtained from each model and identify conditions under which the estimates of the hazard ratio and subhazard ratio differ numerically. Finally, we evaluate several model diagnostic methods with respect to their sensitivity to detect lack of fit when the CSH model holds, but the F-G model is misspecified and vice versa. Our results illustrate that adequacy of model fit can strongly impact the validity of statistical inference. We recommend analysts incorporate a model diagnostic procedure and contingency to explore other appropriate models when designing trials in which competing risks are anticipated.},
	language = {en},
	number = {1},
	urldate = {2023-03-16},
	journal = {Pharmaceutical Statistics},
	author = {Poythress, J.c. and Lee, Misun Yu and Young, James},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.1966},
	keywords = {competing risks, cause-specific hazards model, Fine-Gray model, goodness of fit, logrank test},
	pages = {4--21},
	file = {Snapshot:/home/imbroglio/Zotero/storage/XHF6KEN6/pst.html:text/html},
}

@misc{noauthor_primer_nodate,
	title = {{PRIMER}},
	url = {http://bayes.cs.ucla.edu/PRIMER/},
	urldate = {2023-03-02},
	file = {PRIMER:/home/imbroglio/Zotero/storage/3QYNAHWV/PRIMER.html:text/html},
}

@article{klein_regression_2005,
	title = {Regression {Modeling} of {Competing} {Risks} {Data} {Based} on {Pseudovalues} of the {Cumulative} {Incidence} {Function}},
	volume = {61},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2005.031209.x},
	doi = {10.1111/j.0006-341X.2005.031209.x},
	abstract = {Typically, regression models for competing risks outcomes are based on proportional hazards models for the crude hazard rates. These estimates often do not agree with impressions drawn from plots of cumulative incidence functions for each level of a risk factor. We present a technique which models the cumulative incidence functions directly. The method is based on the pseudovalues from a jackknife statistic constructed from the cumulative incidence curve. These pseudovalues are used in a generalized estimating equation to obtain estimates of model parameters. We study the properties of this estimator and apply the technique to a study of the effect of alternative donors on relapse for patients given a bone marrow transplant for leukemia.},
	language = {en},
	number = {1},
	urldate = {2023-02-21},
	journal = {Biometrics},
	author = {Klein, John P. and Andersen, Per Kragh},
	year = {2005},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0006-341X.2005.031209.x},
	keywords = {Bone marrow transplantation, Generalized estimating equations, Jackknife statistics, Regression models},
	pages = {223--229},
}

@article{scheike_predicting_2008,
	title = {Predicting cumulative incidence probability by direct binomial regression},
	volume = {95},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/asm096},
	doi = {10.1093/biomet/asm096},
	abstract = {We suggest a new simple approach for estimation and assessment of covariate effects for the cumulative incidence curve in the competing risks model. We consider a semiparametric regression model where some effects may be time-varying and some may be constant over time. Our estimator can be implemented by standard software. Our simulation study shows that the estimator works well and has finite-sample properties comparable with the subdistribution approach. We apply the method to bone marrow transplant data and estimate the cumulative incidence of death in complete remission following a bone marrow transplantation. Here death in complete remission and relapse are two competing events.},
	number = {1},
	urldate = {2023-02-21},
	journal = {Biometrika},
	author = {Scheike, Thomas H. and Zhang, Mei-Jie and Gerds, Thomas A.},
	month = mar,
	year = {2008},
	pages = {205--220},
	file = {Snapshot:/home/imbroglio/Zotero/storage/Y6ISS5RS/219426.html:text/html},
}

@article{gerds_absolute_2012,
	title = {Absolute risk regression for competing risks: interpretation, link functions, and prediction},
	volume = {31},
	issn = {1097-0258},
	shorttitle = {Absolute risk regression for competing risks},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.5459},
	doi = {10.1002/sim.5459},
	abstract = {In survival analysis with competing risks, the transformation model allows different functions between the outcome and explanatory variables. However, the model's prediction accuracy and the interpretation of parameters may be sensitive to the choice of link function. We review the practical implications of different link functions for regression of the absolute risk (or cumulative incidence) of an event. Specifically, we consider models in which the regression coefficients β have the following interpretation: The probability of dying from cause D during the next t years changes with a factor exp(β) for a one unit change of the corresponding predictor variable, given fixed values for the other predictor variables. The models have a direct interpretation for the predictive ability of the risk factors. We propose some tools to justify the models in comparison with traditional approaches that combine a series of cause-specific Cox regression models or use the Fine–Gray model. We illustrate the methods with the use of bone marrow transplant data. Copyright © 2012 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {29},
	urldate = {2023-02-21},
	journal = {Statistics in Medicine},
	author = {Gerds, Thomas A. and Scheike, Thomas H. and Andersen, Per K.},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.5459},
	keywords = {absolute risk, competing risk, cumulative incidence, prediction model, regression model},
	pages = {3921--3930},
	file = {Accepted Version:/home/imbroglio/Zotero/storage/D3V9YHG9/Gerds et al. - 2012 - Absolute risk regression for competing risks inte.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/7WA6EVMW/sim.html:text/html},
}

@article{fine_proportional_1999,
	title = {A {Proportional} {Hazards} {Model} for the {Subdistribution} of a {Competing} {Risk}},
	volume = {94},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10474144},
	doi = {10.1080/01621459.1999.10474144},
	abstract = {With explanatory covariates, the standard analysis for competing risks data involves modeling the cause-specific hazard functions via a proportional hazards assumption. Unfortunately, the cause-specific hazard function does not have a direct interpretation in terms of survival probabilities for the particular failure type. In recent years many clinicians have begun using the cumulative incidence function, the marginal failure probabilities for a particular cause, which is intuitively appealing and more easily explained to the nonstatistician. The cumulative incidence is especially relevant in cost-effectiveness analyses in which the survival probabilities are needed to determine treatment utility. Previously, authors have considered methods for combining estimates of the cause-specific hazard functions under the proportional hazards formulation. However, these methods do not allow the analyst to directly assess the effect of a covariate on the marginal probability function. In this article we propose a novel semiparametric proportional hazards model for the subdistribution. Using the partial likelihood principle and weighting techniques, we derive estimation and inference procedures for the finite-dimensional regression parameter under a variety of censoring scenarios. We give a uniformly consistent estimator for the predicted cumulative incidence for an individual with certain covariates; confidence intervals and bands can be obtained analytically or with an easy-to-implement simulation technique. To contrast the two approaches, we analyze a dataset from a breast cancer clinical trial under both models.},
	number = {446},
	urldate = {2023-02-21},
	journal = {Journal of the American Statistical Association},
	author = {Fine, Jason P. and Gray, Robert J.},
	month = jun,
	year = {1999},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1999.10474144},
	keywords = {Hazard of subdistribution, Martingale, Partial likelihood, Transformation model},
	pages = {496--509},
}

@misc{noauthor_sustain6-concrete_nodate,
	title = {{SUSTAIN6}-concrete},
	url = {https://www.overleaf.com/project/645ae958e5acefe73246363a},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2023-07-17},
	file = {Snapshot:/home/imbroglio/Zotero/storage/T8EZLBYT/645ae958e5acefe73246363a.html:text/html},
}

@article{marso_semaglutide_2016,
	title = {Semaglutide and {Cardiovascular} {Outcomes} in {Patients} with {Type} 2 {Diabetes}},
	volume = {375},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMoa1607141},
	doi = {10.1056/NEJMoa1607141},
	number = {19},
	urldate = {2023-07-01},
	journal = {New England Journal of Medicine},
	author = {Marso, Steven P. and Bain, Stephen C. and Consoli, Agostino and Eliaschewitz, Freddy G. and Jódar, Esteban and Leiter, Lawrence A. and Lingvay, Ildiko and Rosenstock, Julio and Seufert, Jochen and Warren, Mark L. and Woo, Vincent and Hansen, Oluf and Holst, Anders G. and Pettersson, Jonas and Vilsbøll, Tina},
	month = nov,
	year = {2016},
	pmid = {27633186},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/NEJMoa1607141},
	pages = {1834--1844},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/UH2X4ISC/Marso et al. - 2016 - Semaglutide and Cardiovascular Outcomes in Patient.pdf:application/pdf},
}

@article{denz_comparison_2023,
	title = {A comparison of different methods to adjust survival curves for confounders},
	volume = {42},
	copyright = {© 2023 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9681},
	doi = {10.1002/sim.9681},
	abstract = {Treatment specific survival curves are an important tool to illustrate the treatment effect in studies with time-to-event outcomes. In non-randomized studies, unadjusted estimates can lead to biased depictions due to confounding. Multiple methods to adjust survival curves for confounders exist. However, it is currently unclear which method is the most appropriate in which situation. Our goal is to compare forms of inverse probability of treatment weighting, the G-Formula, propensity score matching, empirical likelihood estimation and augmented estimators as well as their pseudo-values based counterparts in different scenarios with a focus on their bias and goodness-of-fit. We provide a short review of all methods and illustrate their usage by contrasting the survival of smokers and non-smokers, using data from the German Epidemiological Trial on Ankle-Brachial-Index. Subsequently, we compare the methods using a Monte-Carlo simulation. We consider scenarios in which correctly or incorrectly specified models for describing the treatment assignment and the time-to-event outcome are used with varying sample sizes. The bias and goodness-of-fit is determined by taking the entire survival curve into account. When used properly, all methods showed no systematic bias in medium to large samples. Cox regression based methods, however, showed systematic bias in small samples. The goodness-of-fit varied greatly between different methods and scenarios. Methods utilizing an outcome model were more efficient than other techniques, while augmented estimators using an additional treatment assignment model were unbiased when either model was correct with a goodness-of-fit comparable to other methods. These “doubly-robust” methods have important advantages in every considered scenario.},
	language = {en},
	number = {10},
	urldate = {2023-06-29},
	journal = {Statistics in Medicine},
	author = {Denz, Robin and Klaaßen-Mielke, Renate and Timmesfeld, Nina},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.9681},
	keywords = {causal inference, confounding, adjusted survival curves, simulation, time-to-event},
	pages = {1461--1479},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/8KM986ST/Denz et al. - 2023 - A comparison of different methods to adjust surviv.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/BXXVXQQ3/sim.html:text/html},
}

@misc{westling_inference_2021-1,
	title = {Inference for treatment-specific survival curves using machine learning},
	url = {http://arxiv.org/abs/2106.06602},
	doi = {10.48550/arXiv.2106.06602},
	abstract = {In the absence of data from a randomized trial, researchers often aim to use observational data to draw causal inference about the effect of a treatment on a time-to-event outcome. In this context, interest often focuses on the treatment-specific survival curves; that is, the survival curves were the entire population under study to be assigned to receive the treatment or not. Under certain causal conditions, including that all confounders of the treatment-outcome relationship are observed, the treatment-specific survival can be identified with a covariate-adjusted survival function. Several estimators of this function have been proposed, including estimators based on outcome regression, inverse probability weighting, and doubly robust estimators. In this article, we propose a new cross-fitted doubly-robust estimator that incorporates data-adaptive (e.g. machine learning) estimators of the conditional survival functions. We establish conditions on the nuisance estimators under which our estimator is consistent and asymptotically linear, both pointwise and uniformly in time. We also propose a novel ensemble learner for combining multiple candidate estimators of the conditional survival estimators. Notably, our methods and results accommodate events occurring in discrete or continuous time (or both). We investigate the practical performance of our methods using numerical studies and an application to the effect of a surgical treatment to prevent metastases of parotid carcinoma on mortality.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Westling, Ted and Luedtke, Alex and Gilbert, Peter and Carone, Marco},
	month = jun,
	year = {2021},
	note = {arXiv:2106.06602 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/XINH6AUY/Westling et al. - 2021 - Inference for treatment-specific survival curves u.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/CCTFPBS8/2106.html:text/html},
}

@article{rytgaard_estimation_2023,
	title = {Estimation of time-specific intervention effects on continuously distributed time-to-event outcomes by targeted maximum likelihood estimation},
	volume = {n/a},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13856},
	doi = {10.1111/biom.13856},
	abstract = {This work considers targeted maximum likelihood estimation (TMLE) of treatment effects on absolute risk and survival probabilities in classical time-to-event settings characterized by right-censoring and competing risks. TMLE is a general methodology combining flexible ensemble learning and semiparametric efficiency theory in a two-step procedure for substitution estimation of causal parameters. We specialize and extend the continuous-time TMLE methods for competing risks settings, proposing a targeting algorithm that iteratively updates cause-specific hazards to solve the efficient influence curve equation for the target parameter. As part of the work, we further detail and implement the recently proposed highly adaptive lasso estimator for continuous-time conditional hazards with L1-penalized Poisson regression. The resulting estimation procedure benefits from relying solely on very mild nonparametric restrictions on the statistical model, thus providing a novel tool for machine-learning-based semiparametric causal inference for continuous-time time-to-event data. We apply the methods to a publicly available dataset on follicular cell lymphoma where subjects are followed over time until disease relapse or death without relapse. The data display important time-varying effects that can be captured by the highly adaptive lasso. In our simulations that are designed to imitate the data, we compare our methods to a similar approach based on random survival forests and to the discrete-time TMLE.},
	language = {en},
	number = {00},
	urldate = {2023-06-27},
	journal = {Biometrics},
	author = {Rytgaard, Helene C. W. and Eriksson, Frank and van der Laan, Mark J.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13856},
	keywords = {causal inference, targeted maximum likelihood estimation, survival analysis, competing risks, nonparametric inference, semiparametric efficiency theory},
	pages = {1--12},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/T7DGPHPT/Rytgaard et al. - Estimation of time-specific intervention effects o.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/LPFBCCIP/biom.html:text/html;Submitted Version:/home/imbroglio/Zotero/storage/RHP4PW3D/Rytgaard et al. - Estimation of time-specific intervention effects o.pdf:application/pdf},
}

@article{sofrygin_targeted_2019,
	title = {Targeted learning with daily {EHR} data},
	volume = {38},
	issn = {0277-6715, 1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.8164},
	doi = {10.1002/sim.8164},
	language = {en},
	number = {16},
	urldate = {2023-06-27},
	journal = {Statistics in Medicine},
	author = {Sofrygin, Oleg and Zhu, Zheng and Schmittdiel, Julie A. and Adams, Alyce S. and Grant, Richard W. and Laan, Mark J. and Neugebauer, Romain},
	month = jul,
	year = {2019},
	pages = {3073--3090},
	file = {Submitted Version:/home/imbroglio/Zotero/storage/Y2ESEGYY/Sofrygin et al. - 2019 - Targeted learning with daily EHR data.pdf:application/pdf},
}

@article{ferreira_guerra_impact_2020,
	title = {Impact of discretization of the timeline for longitudinal causal inference methods},
	volume = {39},
	copyright = {© 2020 John Wiley \& Sons Ltd},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8710},
	doi = {10.1002/sim.8710},
	abstract = {In longitudinal settings, causal inference methods usually rely on a discretization of the patient timeline that may not reflect the underlying data generation process. This article investigates the estimation of causal parameters under discretized data. It presents the implicit assumptions practitioners make but do not acknowledge when discretizing data to assess longitudinal causal parameters. We illustrate that differences in point estimates under different discretizations are due to the data coarsening resulting in both a modified definition of the parameter of interest and loss of information about time-dependent confounders. We further investigate several tools to advise analysts in selecting a timeline discretization for use with pooled longitudinal targeted maximum likelihood estimation for the estimation of the parameters of a marginal structural model. We use a simulation study to empirically evaluate bias at different discretizations and assess the use of the cross-validated variance as a measure of data support to select a discretization under a chosen data coarsening mechanism. We then apply our approach to a study on the relative effect of alternative asthma treatments during pregnancy on pregnancy duration. The results of the simulation study illustrate how coarsening changes the target parameter of interest as well as how it may create bias due to a lack of appropriate control for time-dependent confounders. We also observe evidence that the cross-validated variance acts well as a measure of support in the data, by being minimized at finer discretizations as the sample size increases.},
	language = {en},
	number = {27},
	urldate = {2023-06-27},
	journal = {Statistics in Medicine},
	author = {Ferreira Guerra, Steve and Schnitzer, Mireille E. and Forget, Amélie and Blais, Lucie},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8710},
	keywords = {cross-validation, TMLE, coarsening, electronic health data, semiparametric estimation},
	pages = {4069--4085},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/99X6ZNVQ/Ferreira Guerra et al. - 2020 - Impact of discretization of the timeline for longi.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/4L7SAPKE/sim.html:text/html},
}

@article{sloma_empirical_2021,
	title = {Empirical {Comparison} of {Continuous} and {Discrete}-time {Representations} for {Survival} {Prediction}},
	volume = {146},
	issn = {2640-3498},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8232898/},
	abstract = {Survival prediction aims to predict the time of occurrence of a particular event of interest, such as the time until a patient dies. The main challenge in survival prediction is the presence of incomplete observations due to censoring. The classical formulation for survival prediction treats the survival time as a continuous outcome, which leads to a censored regression problem. Recent work has reformulated the survival prediction problem by discretizing time into a finite number of bins and then applying multi-task binary classification. While the discrete-time formulation is convenient and potentially requires less assumptions than the continuous-time approach, it also loses information by discretizing time. In this paper, we empirically investigate continuous and discrete-time representations for survival prediction to try to quantify the trade-offs between the two formulations. We find that discretizing time does not necessarily decrease prediction accuracy. Furthermore, discrete-time models can result in even more accurate predictors than continuous-time models, but the number of time bins used for discretization has a significant effect on accuracy and should thus be tuned as a hyperparameter rather than specified for convenience.},
	urldate = {2023-06-27},
	journal = {Proceedings of machine learning research},
	author = {Sloma, Michael and Syed, Fayeq Jeelani and Nemati, Mohammadreza and Xu, Kevin S.},
	month = mar,
	year = {2021},
	pmid = {34179790},
	pmcid = {PMC8232898},
	pages = {118--131},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/X9EALGI5/Sloma et al. - 2021 - Empirical Comparison of Continuous and Discrete-ti.pdf:application/pdf},
}

@misc{tran_robust_2018,
	title = {Robust variance estimation and inference for causal effect estimation},
	url = {http://arxiv.org/abs/1810.03030},
	doi = {10.48550/arXiv.1810.03030},
	abstract = {We consider a longitudinal data structure consisting of baseline covariates, time-varying treatment variables, intermediate time-dependent covariates, and a possibly time dependent outcome. Previous studies have shown that estimating the variance of asymptotically linear estimators using empirical influence functions in this setting result in anti-conservative estimates with increasing magnitudes of positivity violations, leading to poor coverage and uncontrolled Type I errors. In this paper, we present two alternative approaches of estimating the variance of these estimators: (i) a robust approach which directly targets the variance of the influence function as a counterfactual mean outcome, and (ii) a non-parametric bootstrap based approach that is theoretically valid and lowers the computational cost, thereby increasing the feasibility in non-parametric settings using complex machine learning algorithms. The performance of these approaches are compared to that of the empirical influence function in simulations across different levels of positivity violations and treatment effect sizes.},
	urldate = {2023-06-11},
	publisher = {arXiv},
	author = {Tran, Linh and Petersen, Maya and Schwab, Joshua and van der Laan, Mark J.},
	month = oct,
	year = {2018},
	note = {arXiv:1810.03030 [math, stat]},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory},
	annote = {Comment: 20 pages, 8 figures},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/KVEEX3D7/Tran et al. - 2018 - Robust variance estimation and inference for causa.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/XVKYG79L/1810.html:text/html},
}

@inproceedings{chen_xgboost_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {{XGBoost}},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2023-06-11},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Tianqi and Guestrin, Carlos},
	month = aug,
	year = {2016},
	keywords = {large-scale machine learning},
	pages = {785--794},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/GVNTM7UD/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:application/pdf},
}

@article{dudoit_asymptotics_2005,
	title = {Asymptotics of cross-validated risk estimation in estimator selection and performance assessment},
	volume = {2},
	issn = {1572-3127},
	url = {https://www.sciencedirect.com/science/article/pii/S1572312705000158},
	doi = {10.1016/j.stamet.2005.02.003},
	abstract = {Risk estimation is an important statistical question for the purposes of selecting a good estimator (i.e., model selection) and assessing its performance (i.e., estimating generalization error). This article introduces a general framework for cross-validation and derives distributional properties of cross-validated risk estimators in the context of estimator selection and performance assessment. Arbitrary classes of estimators are considered, including density estimators and predictors for both continuous and polychotomous outcomes. Results are provided for general full data loss functions (e.g., absolute and squared error, indicator, negative log density). A broad definition of cross-validation is used in order to cover leave-one-out cross-validation, V-fold cross-validation, Monte Carlo cross-validation, and bootstrap procedures. For estimator selection, finite sample risk bounds are derived and applied to establish the asymptotic optimality of cross-validation, in the sense that a selector based on a cross-validated risk estimator performs asymptotically as well as an optimal oracle selector based on the risk under the true, unknown data generating distribution. The asymptotic results are derived under the assumption that the size of the validation sets converges to infinity and hence do not cover leave-one-out cross-validation. For performance assessment, cross-validated risk estimators are shown to be consistent and asymptotically linear for the risk under the true data generating distribution and confidence intervals are derived for this unknown risk. Unlike previously published results, the theorems derived in this and our related articles apply to general data generating distributions, loss functions (i.e., parameters), estimators, and cross-validation procedures.},
	language = {en},
	number = {2},
	urldate = {2023-06-11},
	journal = {Statistical Methodology},
	author = {Dudoit, Sandrine and van der Laan, Mark J.},
	month = jul,
	year = {2005},
	keywords = {loss function, Risk, Asymptotic linearity, Asymptotic optimality, Classification, Confidence interval, Cross-validation, Density estimation, Estimator selection, Generalization error, Indicator loss function, Loss function, Model selection, Multifold cross-validation, Performance assessment, Prediction, Quadratic loss function, Regression, Resubstitution estimator},
	pages = {131--154},
	file = {Full Text:/home/imbroglio/Zotero/storage/IBYBFPLH/Dudoit and van der Laan - 2005 - Asymptotics of cross-validated risk estimation in .pdf:application/pdf;ScienceDirect Snapshot:/home/imbroglio/Zotero/storage/DF7PUQAA/S1572312705000158.html:text/html},
}

@book{fleming_counting_1991,
	address = {New York},
	series = {Wiley {Series} in {Probability} and {Mathematical} {Statistics}: {Applied} {Probability} and {Statistics}},
	title = {Counting {Processes} and {Survival} {Analysis}},
	isbn = {978-1-118-15067-2},
	language = {English},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Fleming, Thomas R. and Harrington, David P.},
	year = {1991},
}

@book{therneau_modeling_2000,
	address = {New York, NY},
	series = {Statistics for {Biology} and {Health}},
	title = {Modeling {Survival} {Data}: {Extending} the {Cox} {Model}},
	isbn = {978-1-4419-3161-0 978-1-4757-3294-8},
	shorttitle = {Modeling {Survival} {Data}},
	url = {http://link.springer.com/10.1007/978-1-4757-3294-8},
	urldate = {2023-06-09},
	publisher = {Springer},
	author = {Therneau, Terry M. and Grambsch, Patricia M.},
	editor = {Dietz, K. and Gail, M. and Krickeberg, K. and Samet, J. and Tsiatis, A.},
	year = {2000},
	doi = {10.1007/978-1-4757-3294-8},
	keywords = {Survival analysis, Cox Model, Cox Regression Model, Estimator, Radiologieinformationssystem, SAS, statistics, Survival Data},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/2U7IHVPW/Therneau and Grambsch - 2000 - Modeling Survival Data Extending the Cox Model.pdf:application/pdf},
}

@book{noauthor_counting_nodate,
	title = {Counting {Processes} and {Survival} {Analysis} {\textbar} {Wiley}},
	url = {https://www.wiley.com/en-us/Counting+Processes+and+Survival+Analysis-p-9780471769880},
	abstract = {The Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. The book is a valuable completion of the literature in this field. It is written in an ambitious mathematical style and can be recommended to statisticians as well as biostatisticians. -Biometrische Zeitschrift Not many books manage to combine convincingly topics from probability theory over mathematical statistics to applied statistics. This is one of them. The book has other strong points to recommend it: it is written with meticulous care, in a lucid style, general results being illustrated by examples from statistical theory and practice, and a bunch of exercises serve to further elucidate and elaborate on the text. -Mathematical Reviews This book gives a thorough introduction to martingale and counting process methods in survival analysis thereby filling a gap in the literature. -Zentralblatt für Mathematik und ihre Grenzgebiete/Mathematics Abstracts The authors have performed a valuable service to researchers in providing this material in [a] self-contained and accessible form. . . This text [is] essential reading for the probabilist or mathematical statistician working in the area of survival analysis. -Short Book Reviews, International Statistical Institute Counting Processes and Survival Analysis explores the martingale approach to the statistical analysis of counting processes, with an emphasis on the application of those methods to censored failure time data. This approach has proven remarkably successful in yielding results about statistical methods for many problems arising in censored data. A thorough treatment of the calculus of martingales as well as the most important applications of these methods to censored data is offered. Additionally, the book examines classical problems in asymptotic distribution theory for counting process methods and newer methods for graphical analysis and diagnostics of censored data. Exercises are included to provide practice in applying martingale methods and insight into the calculus itself.},
	language = {en-us},
	urldate = {2023-06-09},
	file = {Snapshot:/home/imbroglio/Zotero/storage/9PKK9MYC/Counting+Processes+and+Survival+Analysis-p-9780471769880.html:text/html},
}

@article{andersen_competing_2012-1,
	title = {Competing risks in epidemiology: possibilities and pitfalls},
	volume = {41},
	issn = {0300-5771},
	shorttitle = {Competing risks in epidemiology},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3396320/},
	doi = {10.1093/ije/dyr213},
	abstract = {Background In studies of all-cause mortality, the fundamental epidemiological concepts of rate and risk are connected through a well-defined one-to-one relation. An important consequence of this relation is that regression models such as the proportional hazards model that are defined through the hazard (the rate) immediately dictate how the covariates relate to the survival function (the risk)., Methods This introductory paper reviews the concepts of rate and risk and their one-to-one relation in all-cause mortality studies and introduces the analogous concepts of rate and risk in the context of competing risks, the cause-specific hazard and the cause-specific cumulative incidence function., Results The key feature of competing risks is that the one-to-one correspondence between cause-specific hazard and cumulative incidence, between rate and risk, is lost. This fact has two important implications. First, the naïve Kaplan–Meier that takes the competing events as censored observations, is biased. Secondly, the way in which covariates are associated with the cause-specific hazards may not coincide with the way these covariates are associated with the cumulative incidence. An example with relapse and non-relapse mortality as competing risks in a stem cell transplantation study is used for illustration., Conclusion The two implications of the loss of one-to-one correspondence between cause-specific hazard and cumulative incidence should be kept in mind when deciding on how to make inference in a competing risks situation.},
	number = {3},
	urldate = {2023-06-08},
	journal = {International Journal of Epidemiology},
	author = {Andersen, Per Kragh and Geskus, Ronald B and de Witte, Theo and Putter, Hein},
	month = jun,
	year = {2012},
	pmid = {22253319},
	pmcid = {PMC3396320},
	pages = {861--870},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/SEH8LSCT/Andersen et al. - 2012 - Competing risks in epidemiology possibilities and.pdf:application/pdf},
}

@article{young_causal_2020,
	title = {A causal framework for classical statistical estimands in failure time settings with competing events},
	volume = {39},
	issn = {0277-6715},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7811594/},
	doi = {10.1002/sim.8471},
	abstract = {In failure-time settings, a competing event is any event that makes it impossible for the event of interest to occur. For example, cardiovascular disease death is a competing event for prostate cancer death because an individual cannot die of prostate cancer once he has died of cardiovascular disease. Various statistical estimands have been defined as possible targets of inference in the classical competing risks literature. Many reviews have described these statistical estimands and their estimating procedures with recommendations about their use. However, this previous work has not used a formal framework for characterizing causal effects and their identifying conditions, which makes it difficult to interpret effect estimates and assess recommendations regarding analytic choices. Here we use a counterfactual framework to explicitly define each of these classical estimands. We clarify that, depending on whether competing events are defined as censoring events, contrasts of risks can define a total effect of the treatment on the event of interest, or a direct effect of the treatment on the event of interest not mediated through the competing event. In contrast, regardless of whether competing events are defined as censoring events, counterfactual hazard contrasts cannot generally be interpreted as causal effects. We illustrate how identifying assumptions for all of these counterfactual estimands can be represented in causal diagrams in which competing events are depicted as time-varying covariates. We present an application of these ideas to data from a randomized trial designed to estimate the effect of estrogen therapy on prostate cancer mortality.},
	number = {8},
	urldate = {2023-06-08},
	journal = {Statistics in medicine},
	author = {Young, Jessica G. and Stensrud, Mats J. and Tchetgen, Eric J. Tchetgen and Hernán, Miguel A.},
	month = apr,
	year = {2020},
	pmid = {31985089},
	pmcid = {PMC7811594},
	pages = {1199--1236},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/8Q2AVPEJ/Young et al. - 2020 - A causal framework for classical statistical estim.pdf:application/pdf},
}

@article{aalen_empirical_1978,
	title = {An {Empirical} {Transition} {Matrix} for {Non}-{Homogeneous} {Markov} {Chains} {Based} on {Censored} {Observations}},
	volume = {5},
	issn = {0303-6898},
	url = {https://www.jstor.org/stable/4615704},
	abstract = {A product limit estimator is suggested for the transition probabilities of a non-homogeneous Markov chain with finitely many states. The estimator is expressed as a product integral and its properties are studied by means of the theory of square integrable martingales.},
	number = {3},
	urldate = {2023-05-31},
	journal = {Scandinavian Journal of Statistics},
	author = {Aalen, Odd O. and Johansen, Søren},
	year = {1978},
	note = {Publisher: [Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
	pages = {141--150},
}

@book{andersen_statistical_1995,
	address = {New York Berlin Heidelberg},
	edition = {Corrected edition},
	title = {Statistical {Models} {Based} on {Counting} {Processes}},
	isbn = {978-0-387-94519-4},
	abstract = {Modern survival analysis and more general event history analysis may be effectively handled within the mathematical framework of counting processes. This book presents this theory, which has been the subject of intense research activity over the past 15 years. The exposition of the theory is integrated with careful presentation of many practical examples, drawn almost exclusively from the authors'own experience, with detailed numerical and graphical illustrations. Although Statistical Models Based on Counting Processes may be viewed as a research monograph for mathematical statisticians and biostatisticians, almost all the methods are given in concrete detail for use in practice by other mathematically oriented researchers studying event histories (demographers, econometricians, epidemiologists, actuarial mathematicians, reliability engineers and biologists). Much of the material has so far only been available in the journal literature (if at all), and so a wide variety of researchers will find this an invaluable survey of the subject.},
	language = {English},
	publisher = {Springer},
	author = {Andersen, Per K. and Borgan, Ornulf and Gill, Richard D. and Keiding, Niels},
	month = jun,
	year = {1995},
	file = {Andersen - Statistical Models Based on Counting Processes.pdf:/home/imbroglio/Zotero/storage/7XPM6Z9I/Andersen - Statistical Models Based on Counting Processes.pdf:application/pdf},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction to {Modern} {Causal} {Inference}},
	url = {https://alejandroschuler.github.io/mci/},
	urldate = {2023-05-25},
	file = {Snapshot:/home/imbroglio/Zotero/storage/GXJN9GHA/mci.html:text/html},
}

@incollection{malenica_chapter_nodate,
	title = {Chapter 5 {Cross}-validation},
	url = {https://tlverse.org/tlverse-handbook/origami.html},
	language = {en},
	urldate = {2023-05-23},
	booktitle = {Targeted {Learning} in {R}: {Causal} {Data} {Science} with the tlverse {Software} {Ecosystem}},
	author = {Malenica, Ivana and van der Laan, Mark and Coyle, Jeremy and Hejazi, Nima and Phillips, Rachael and Hubbard, Alan},
	file = {Snapshot:/home/imbroglio/Zotero/storage/WHATSC4I/origami.html:text/html},
}

@article{ema_guideline_2015,
	title = {Guideline on adjustment for baseline covariates in clinical trials. {Reference} number {EMA}/{CHMP}/295050/2013 {Committee} for {Medicinal} {Products} for {Human} {Use}.},
	url = {https://www.ema.europa.eu/en/documents/scientific-guideline/guideline-adjustment-baseline-covariates-clinical-trials_en.pdf},
	language = {en},
	author = {EMA},
	year = {2015},
	file = {Guideline on adjustment for baseline covariates in.pdf:/home/imbroglio/Zotero/storage/LYT7TSWJ/Guideline on adjustment for baseline covariates in.pdf:application/pdf},
}

@article{chen_beyond_2023,
	title = {Beyond the {Cox} {Hazard} {Ratio}: {A} {Targeted} {Learning} {Approach} to {Survival} {Analysis} in a {Cardiovascular} {Outcome} {Trial} {Application}},
	volume = {0},
	issn = {null},
	shorttitle = {Beyond the {Cox} {Hazard} {Ratio}},
	url = {https://doi.org/10.1080/19466315.2023.2173644},
	doi = {10.1080/19466315.2023.2173644},
	abstract = {The Hazard Ratio (HR) is a well-established treatment effect measure in randomized trials involving right-censored time-to-events, and the Cardiovascular Outcome Trials (CVOTs) conducted since the FDA’s 2008 guidance have indeed largely evaluated excess risk by estimating a Cox HR. On the other hand, the limitations of the Cox model and of the HR as a causal estimand are well known, and the FDA’s updated 2020 CVOT guidance invites us to reassess this default approach to survival analyses. We highlight the shortcomings of Cox HR-based analyses and present an alternative following the causal roadmap—moving in a principled way from a counterfactual causal question to identifying a statistical estimand, and finally to targeted estimation in a large statistical model. We show in simulations the robustness of Targeted Maximum Likelihood Estimation (TMLE) to informative censoring and model misspecification and demonstrate a targeted learning analogue of the original Cox HR-based analysis of the Liraglutide Effect and Action in Diabetes: Evaluation of Cardiovascular Outcome Results (LEADER) trial. We discuss the potential reliability, interpretability, and efficiency gains to be had by updating our survival methods to incorporate the recent decades of advancements in formal causal frameworks and efficient nonparametricestimation.},
	number = {0},
	urldate = {2023-05-19},
	journal = {Statistics in Biopharmaceutical Research},
	author = {Chen, David and Petersen, Maya L. and Rytgaard, Helene Charlotte and Grøn, Randi and Lange, Theis and Rasmussen, Søren and Pratley, Richard E. and Marso, Steven P. and Kvist, Kajsa and Buse, John and van der Laan, Mark J.},
	month = apr,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/19466315.2023.2173644},
	keywords = {TMLE, Semiparametric efficiency, Causal roadmap, LEADER},
	pages = {1--27},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/9685RHK4/Chen et al. - 2023 - Beyond the Cox Hazard Ratio A Targeted Learning A.pdf:application/pdf},
}

@article{rudolph_mediation_2018,
	title = {Mediation of neighborhood effects on adolescent substance use by the school and peer environments},
	volume = {29},
	issn = {1044-3983},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5987191/},
	doi = {10.1097/EDE.0000000000000832},
	abstract = {Background
Evidence suggests that aspects of the neighborhood environment may influence risk of problematic drug use among adolescents. Our objective was to examine mediating roles of aspects of the school and peer environments on the effect of receiving a Section 8 housing voucher and using it to move out of public housing on adolescent substance use outcomes.

Methods
We used data from the Moving to Opportunity (MTO) experiment that randomized receipt of a Section 8 housing voucher. Hypothesized mediators included school climate, safety, peer drug use, and participation in an after-school sport or club. We applied a doubly robust, semiparametric estimator to longitudinal MTO data to estimate stochastic direct and indirect effects of randomization on cigarette use, marijuana use, and problematic drug use. Stochastic direct and indirect effects differ from natural direct and indirect effects in that they do not require assuming no post-treatment confounder of the mediator-outcome relationship. Such an assumption would be at odds with any causal model that reflects an intervention affecting a mediator and outcome through adherence to treatment assignment.

Results
Having friends who use drugs and involvement in after-school sports or clubs partially mediated the effect of housing voucher receipt on adolescent substance use (e.g., stochastic indirect effect 0.45\% (95\% CI: 0.12\%, 0.79\%) for having friends who use drugs and 0.04\% (95\% CI: −0.02\%, 0.10\%) for involvement in after-school sports or clubs mediating the relationship between housing voucher receipt and marijuana use among boys). However, these mediating effects were small, contributing only fractions of a percent to the effect of voucher receipt on probability of substance use. No school environment variables were mediators.

Conclusions
Measured school- and peer-environment variables played little role in mediating the effect of housing voucher receipt on subsequent adolescent substance use.},
	number = {4},
	urldate = {2023-05-19},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Rudolph, Kara E. and Sofrygin, Oleg and Schmidt, Nicole M. and Crowder, Rebecca and Glymour, M. Maria and Ahern, Jennifer and Osypuk, Theresa L.},
	month = jul,
	year = {2018},
	pmid = {29851894},
	pmcid = {PMC5987191},
	pages = {590--598},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/8V3XP9XP/Rudolph et al. - 2018 - Mediation of neighborhood effects on adolescent su.pdf:application/pdf},
}

@article{havlir_hiv_2019,
	title = {{HIV} {Testing} and {Treatment} with the {Use} of a {Community} {Health} {Approach} in {Rural} {Africa}},
	volume = {381},
	issn = {0028-4793},
	url = {https://www.nejm.org/doi/10.1056/NEJMoa1809866},
	doi = {10.1056/NEJMoa1809866},
	number = {3},
	urldate = {2023-05-18},
	journal = {New England Journal of Medicine},
	author = {Havlir, Diane V. and Balzer, Laura B. and Charlebois, Edwin D. and Clark, Tamara D. and Kwarisiima, Dalsone and Ayieko, James and Kabami, Jane and Sang, Norton and Liegler, Teri and Chamie, Gabriel and Camlin, Carol S. and Jain, Vivek and Kadede, Kevin and Atukunda, Mucunguzi and Ruel, Theodore and Shade, Starley B. and Ssemmondo, Emmanuel and Byonanebye, Dathan M. and Mwangwa, Florence and Owaraganise, Asiphas and Olilo, Winter and Black, Douglas and Snyman, Katherine and Burger, Rachel and Getahun, Monica and Achando, Jackson and Awuonda, Benard and Nakato, Hellen and Kironde, Joel and Okiror, Samuel and Thirumurthy, Harsha and Koss, Catherine and Brown, Lillian and Marquez, Carina and Schwab, Joshua and Lavoy, Geoff and Plenty, Albert and Mugoma Wafula, Erick and Omanya, Patrick and Chen, Yea-Hung and Rooney, James F. and Bacon, Melanie and van der Laan, Mark and Cohen, Craig R. and Bukusi, Elizabeth and Kamya, Moses R. and Petersen, Maya},
	month = jul,
	year = {2019},
	note = {Publisher: Massachusetts Medical Society},
	pages = {219--229},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/IBV8GDI7/Havlir et al. - 2019 - HIV Testing and Treatment with the Use of a Commun.pdf:application/pdf},
}

@article{benichou_estimates_1990,
	title = {Estimates of absolute cause-specific risk in cohort studies},
	volume = {46},
	issn = {0006-341X},
	abstract = {In this paper we study methods for estimating the absolute risk of an event c1 in a time interval [t1, t2], given that the individual is at risk at t1 and given the presence of competing risks. We discuss some advantages of absolute risk for measuring the prognosis of an individual patient and some difficulties of interpretation for comparing two treatment groups. We also discuss the importance of the concept of absolute risk in evaluating public health measures to prevent disease. Variance calculations permit one to gauge the relative importance of random and systematic errors in estimating absolute risk. Efficiency calculations were also performed to determine how much precision is lost in estimating absolute risk with a nonparametric approach or with a flexible piecewise exponential model rather than a simple exponential model, and other calculations indicate the extent of bias that arises with the simple exponential model when that model is invalid. Such calculations suggest that the more flexible models will be useful in practice. Simulations confirm that asymptotic methods yield reliable variance estimates and confidence interval coverages in samples of practical size.},
	language = {eng},
	number = {3},
	journal = {Biometrics},
	author = {Benichou, J. and Gail, M. H.},
	month = sep,
	year = {1990},
	pmid = {2242416},
	keywords = {Humans, Models, Statistical, Risk, Proportional Hazards Models, Biometry, Clinical Trials as Topic, Cohort Studies},
	pages = {813--826},
}

@misc{noauthor_notitle_nodate,
	url = {https://onlinelibrary.wiley.com/openurl?date=1990&genre=article&issn=0006-341X&issue=3&sid=nlm%3Apubmed&spage=813&volume=46},
	urldate = {2023-05-17},
	file = {https\://onlinelibrary.wiley.com/openurl?date=1990&genre=article&issn=0006-341X&issue=3&sid=nlm%3Apubmed&spage=813&volume=46:/home/imbroglio/Zotero/storage/3CAUNQZL/openurl.html:text/html},
}

@article{cox_partial_1975,
	title = {Partial likelihood},
	volume = {62},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/62.2.269},
	doi = {10.1093/biomet/62.2.269},
	abstract = {A definition is given of partial likelihood generalizing the ideas of conditional and marginal likelihood. Applications include life tables and inference in stochastic processes. It is shown that the usual large-sample properties of maximum likelihood estimates and tests apply when partial likelihood is used.},
	number = {2},
	urldate = {2023-05-08},
	journal = {Biometrika},
	author = {Cox, D. R.},
	month = aug,
	year = {1975},
	pages = {269--276},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/86NZX5VC/COX - 1975 - Partial likelihood.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/JSCB9G5N/337051.html:text/html},
}

@article{rytgaard_targeted_2022,
	title = {Targeted maximum likelihood estimation for causal inference in survival and competing risks analysis},
	issn = {1572-9249},
	url = {https://doi.org/10.1007/s10985-022-09576-2},
	doi = {10.1007/s10985-022-09576-2},
	abstract = {Targeted maximum likelihood estimation (TMLE) provides a general methodology for estimation of causal parameters in presence of high-dimensional nuisance parameters. Generally, TMLE consists of a two-step procedure that combines data-adaptive nuisance parameter estimation with semiparametric efficiency and rigorous statistical inference obtained via a targeted update step. In this paper, we demonstrate the practical applicability of TMLE based causal inference in survival and competing risks settings where event times are not confined to take place on a discrete and finite grid. We focus on estimation of causal effects of time-fixed treatment decisions on survival and absolute risk probabilities, considering different univariate and multidimensional parameters. Besides providing a general guidance to using TMLE for survival and competing risks analysis, we further describe how the previous work can be extended with the use of loss-based cross-validated estimation, also known as super learning, of the conditional hazards. We illustrate the usage of the considered methods using publicly available data from a trial on adjuvant chemotherapy for colon cancer. R software code to implement all considered algorithms and to reproduce all analyses is available in an accompanying online appendix on Github.},
	language = {en},
	urldate = {2023-05-08},
	journal = {Lifetime Data Analysis},
	author = {Rytgaard, Helene C. W. and van der Laan, Mark J.},
	month = nov,
	year = {2022},
	keywords = {Survival analysis, Causal inference, TMLE, Average treatment effects, Competing risks, Highly adaptive lasso, Semiparametric efficiency, Super learning},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/8UE59PWD/Rytgaard and van der Laan - 2022 - Targeted maximum likelihood estimation for causal .pdf:application/pdf},
}

@book{vaart_asymptotic_1998,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {Asymptotic {Statistics}},
	isbn = {978-0-521-78450-4},
	url = {https://www.cambridge.org/core/books/asymptotic-statistics/A3C7DAD3F7E66A1FA60E9C8FE132EE1D},
	abstract = {This book is an introduction to the field of asymptotic statistics. The treatment is both practical and mathematically rigorous. In addition to most of the standard topics of an asymptotics course, including likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures, the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, which gives the book one of its unifying themes. This entails mainly the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation. Thus, even the standard subjects of asymptotic statistics are presented in a novel way. Suitable as a graduate or Master's level statistics text, this book will also give researchers an overview of research in asymptotic statistics.},
	urldate = {2023-07-21},
	publisher = {Cambridge University Press},
	author = {Vaart, A. W. van der},
	year = {1998},
	doi = {10.1017/CBO9780511802256},
	file = {Snapshot:/home/imbroglio/Zotero/storage/2BJJ2FQB/A3C7DAD3F7E66A1FA60E9C8FE132EE1D.html:text/html},
}

@article{ma_book_2008,
	title = {Book review: {Tsiatis}, {A}.{A}. 2006: {Semiparametric} {Theory} and {Missing} {Data}. {Springer}},
	volume = {17},
	issn = {0962-2802},
	shorttitle = {Book review},
	url = {https://doi.org/10.1177/09622802080170051002},
	doi = {10.1177/09622802080170051002},
	language = {en},
	number = {5},
	urldate = {2023-07-21},
	journal = {Statistical Methods in Medical Research},
	author = {Ma, Shuangge},
	month = oct,
	year = {2008},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {538--539},
}

@book{tsiatis_semiparametric_2007,
	title = {Semiparametric {Theory} and {Missing} {Data}},
	isbn = {978-0-387-37345-4},
	abstract = {Missing data arise in almost all scientific disciplines. In many cases, the treatment of missing data in an analysis is carried out in a casual and ad-hoc manner, leading, in many cases, to invalid inference and erroneous conclusions. In the past 20 years or so, there has been a serious attempt to understand the underlying issues and difficulties that come about from missing data and their impact on subsequent analysis. There has been a great deal written on the theory developed for analyzing missing data for finite-dimensional parametric models. This includes an extensive literature on likelihood-based methods and multiple imputation. More recently, there has been increasing interest in semiparametric models which, roughly speaking, are models that include both a parametric and nonparametric component. Such models are popular because estimators in such models are more robust than in traditional parametric models. The theory of missing data applied to semiparametric models is scattered throughout the literature with no thorough comprehensive treatment of the subject.  This book combines much of what is known in regard to the theory of estimation for semiparametric models with missing data in an organized and comprehensive manner. It starts with the study of semiparametric methods when there are no missing data. The description of the theory of estimation for semiparametric models is at a level that is both rigorous and intuitive, relying on geometric ideas to reinforce the intuition and understanding of the theory. These methods are then applied to problems with missing, censored, and coarsened data with the goal of deriving estimators that are as robust and efficient as possible.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Tsiatis, Anastasios},
	month = jan,
	year = {2007},
	note = {Google-Books-ID: xqZFi2EMB40C},
	keywords = {average, estimator, likelihood, probability, semiparametric, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
	file = {Full Text:/home/imbroglio/Zotero/storage/ADD3JME9/2006 - Semiparametric Theory and Missing Data.pdf:application/pdf},
}

@article{balzer_far_2020,
	title = {Far from {MCAR}: {Obtaining} population-level estimates of {HIV} viral suppression},
	volume = {31},
	issn = {1044-3983},
	shorttitle = {Far from {MCAR}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8105880/},
	doi = {10.1097/EDE.0000000000001215},
	abstract = {Background:
Population-level estimates of disease prevalence and control are needed to assess prevention and treatment strategies. However, available data often suffer from differential missingness. For example, population-level HIV viral suppression is the proportion of all HIV-positive persons with suppressed viral replication. Individuals with measured HIV status, and among HIV-positive individuals those with measured viral suppression, likely differ from those without such measurements.

Methods:
We discuss three sets of assumptions to identify population-level suppression in the intervention arm of the SEARCH Study (NCT01864603), a community randomized trial in rural Kenya and Uganda (2013-2017). Using data on nearly 100,000 participants, we compare estimates from i) an unadjusted approach assuming data are missing-completely-at-random (MCAR); ii) stratification on age-group, sex, and community; and, iii) targeted maximum likelihood estimation to adjust for a larger set of baseline and time-updated variables.

Results:
Despite high measurement coverage, estimates of population-level viral suppression varied by identification assumption. Unadjusted estimates were most optimistic: 50\% (95\%CI:46-54\%) of HIV-positive persons suppressed at baseline, 80\% (95\%CI:78-82\%) at Year 1, 85\% (95\%CI:83-86\%) at Year 2, and 85\% (95\%CI:83-87\%) at Year 3. Stratifying on baseline predictors yielded slightly lower estimates, and full adjustment reduced estimates meaningfully: 42\% (95\%CI:37-46\%) of HIV-positive persons suppressed at baseline, 71\% (95\%CI:69-73\%) at Year 1, 76\% (95\%CI:74-78\%) at Year 2, and 79\% (95\%CI:77-81\%) at Year 3.

Conclusions:
Estimation of population-level disease burden and control requires appropriate adjustment for missing data. Even in large studies with limited missingness, estimates relying on the MCAR assumption or baseline stratification should be interpreted cautiously.},
	number = {5},
	urldate = {2023-07-21},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Balzer, Laura B. and Ayieko, James and Kwarisiima, Dalsone and Chamie, Gabriel and Charlebois, Edwin D. and Schwab, Joshua and van der Laan, Mark J. and Kamya, Moses R. and Havlir, Diane V. and Petersen, Maya L.},
	month = sep,
	year = {2020},
	pmid = {32452912},
	pmcid = {PMC8105880},
	pages = {620--627},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/U8JCT679/Balzer et al. - 2020 - Far from MCAR Obtaining population-level estimate.pdf:application/pdf},
}

@article{hickey_effect_2022,
	title = {Effect of a one-time financial incentive on linkage to chronic hypertension care in {Kenya} and {Uganda}: {A} randomized controlled trial},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {Effect of a one-time financial incentive on linkage to chronic hypertension care in {Kenya} and {Uganda}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9639834/},
	doi = {10.1371/journal.pone.0277312},
	abstract = {Background
Fewer than 10\% of people with hypertension in sub-Saharan Africa are diagnosed, linked to care, and achieve hypertension control. We hypothesized that a one-time financial incentive and phone call reminder for missed appointments would increase linkage to hypertension care following community-based screening in rural Uganda and Kenya.

Methods
In a randomized controlled trial, we conducted community-based hypertension screening and enrolled adults ≥25 years with blood pressure ≥140/90 mmHg on three measures; we excluded participants with known hypertension or hypertensive emergency. The intervention was transportation reimbursement upon linkage ({\textasciitilde}\$5 USD) and up to three reminder phone calls for those not linking within seven days. Control participants received a clinic referral only. Outcomes were linkage to hypertension care within 30 days (primary) and hypertension control {\textless}140/90 mmHg measured in all participants at 90 days (secondary). We used targeted minimum loss-based estimation to compute adjusted risk ratios (aRR).

Results
We screened 1,998 participants, identifying 370 (18.5\%) with uncontrolled hypertension and enrolling 199 (100 control, 99 intervention). Reasons for non-enrollment included prior hypertension diagnosis (n = 108) and hypertensive emergency (n = 32). Participants were 60\% female, median age 56 (range 27–99); 10\% were HIV-positive and 42\% had baseline blood pressure ≥160/100 mmHg. Linkage to care within 30 days was 96\% in intervention and 66\% in control (aRR 1.45, 95\%CI 1.25–1.68). Hypertension control at 90 days was 51\% intervention and 41\% control (aRR 1.22, 95\%CI 0.92–1.66).

Conclusion
A one-time financial incentive and reminder call for missed visits resulted in a 30\% absolute increase in linkage to hypertension care following community-based screening. Financial incentives can improve the critical step of linkage to care for people newly diagnosed with hypertension in the community.},
	number = {11},
	urldate = {2023-07-21},
	journal = {PLOS ONE},
	author = {Hickey, Matthew D. and Owaraganise, Asiphas and Sang, Norton and Opel, Fredrick J. and Mugoma, Erick Wafula and Ayieko, James and Kabami, Jane and Chamie, Gabriel and Kakande, Elijah and Petersen, Maya L. and Balzer, Laura B. and Kamya, Moses R. and Havlir, Diane V.},
	month = nov,
	year = {2022},
	pmid = {36342940},
	pmcid = {PMC9639834},
	pages = {e0277312},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/8QXI8IVJ/Hickey et al. - 2022 - Effect of a one-time financial incentive on linkag.pdf:application/pdf},
}

@article{wester_effect_2012,
	title = {Effect {Modification} by {Sex} and {Baseline} {CD4}+ {Cell} {Count} {Among} {Adults} {Receiving} {Combination} {Antiretroviral} {Therapy} in {Botswana}: {Results} from a {Clinical} {Trial}},
	volume = {28},
	issn = {0889-2229},
	shorttitle = {Effect {Modification} by {Sex} and {Baseline} {CD4}+ {Cell} {Count} {Among} {Adults} {Receiving} {Combination} {Antiretroviral} {Therapy} in {Botswana}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3423643/},
	doi = {10.1089/aid.2011.0349},
	abstract = {The Tshepo study was the first clinical trial to evaluate outcomes of adults receiving nevirapine (NVP)-based versus efavirenz (EFV)-based combination antiretroviral therapy (cART) in Botswana. This was a 3 year study (n=650) comparing the efficacy and tolerability of various first-line cART regimens, stratified by baseline CD4+: {\textless}200 (low) vs. 201-350 (high). Using targeted maximum likelihood estimation (TMLE), we retrospectively evaluated the causal effect of assigned NNRTI on time to virologic failure or death [intent-to-treat (ITT)] and time to minimum of virologic failure, death, or treatment modifying toxicity [time to loss of virological response (TLOVR)] by sex and baseline CD4+. Sex did significantly modify the effect of EFV versus NVP for both the ITT and TLOVR outcomes with risk differences in the probability of survival of males versus the females of approximately 6\% (p=0.015) and 12\% (p=0.001), respectively. Baseline CD4+ also modified the effect of EFV versus NVP for the TLOVR outcome, with a mean difference in survival probability of approximately 12\% (p=0.023) in the high versus low CD4+ cell count group. TMLE appears to be an efficient technique that allows for the clinically meaningful delineation and interpretation of the causal effect of NNRTI treatment and effect modification by sex and baseline CD4+ cell count strata in this study. EFV-treated women and NVP-treated men had more favorable cART outcomes. In addition, adults initiating EFV-based cART at higher baseline CD4+ cell count values had more favorable outcomes compared to those initiating NVP-based cART.},
	number = {9},
	urldate = {2023-07-21},
	journal = {AIDS Research and Human Retroviruses},
	author = {Wester, C. William and Stitelman, Ori M. and deGruttola, Victor and Bussmann, Hermann and Marlink, Richard G. and van der Laan, Mark J.},
	month = sep,
	year = {2012},
	pmid = {22309114},
	pmcid = {PMC3423643},
	pages = {781--788},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/DE2BABQL/Wester et al. - 2012 - Effect Modification by Sex and Baseline CD4+ Cell .pdf:application/pdf},
}

@article{nguyen_dynamic_2020,
	title = {Dynamic {Impact} of {Transfusion} {Ratios} on {Outcomes} in {Severely} {Injured} {Patients}: {Targeted} {Machine} {Learning} {Analysis} of the {PROPPR} {Randomized} {Clinical} {Trial}},
	volume = {89},
	issn = {2163-0755},
	shorttitle = {Dynamic {Impact} of {Transfusion} {Ratios} on {Outcomes} in {Severely} {Injured} {Patients}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7830749/},
	doi = {10.1097/TA.0000000000002819},
	abstract = {Background:
Massive transfusion protocols to treat post-injury hemorrhage are based on pre-defined blood product transfusion ratios followed by goal-directed transfusion based on patient’s clinical evolution. However, it remains unclear how these transfusion ratios impact patient outcomes over time from injury.

Methods:
The Pragmatic, Randomized Optimal Platelet and Plasma Ratios (PROPPR) is a phase 3, randomized controlled trial, across 12 level-I trauma centers in North America. From 2012 to 2013, 680 severely injured patients required massive transfusion. We used semi-parametric machine learning techniques and causal inference methods to augment the intent-to-treat analysis of PROPPR, estimating the dynamic relationship between transfusion ratios and outcomes: mortality and hemostasis at different time-points during the first 24 hours after admission.

Results:
In the intention-to-treat analysis, the 1:1:1 group tended to have decreased mortality, but with no statistical significance. For patients in whom hemostasis took longer than 2 hours, the 1:1:1 ratio was associated with a higher probability of hemostasis, statistically significant from the 4th hour on. In the per-protocol, actual-transfusion-ratios-received analysis, during four successive time intervals, no significant association was found between the actual ratios and mortality. When comparing patient groups who received both high plasma:PRBC and high platelet:PRBC ratios to the group of low ratios in both, the relative risk of achieving hemostasis was 2.49 (95\% CI = 1.19–5.22) during the 3rd hour after admission, suggesting a significant beneficial impact of higher transfusion ratios of plasma and platelets on hemostasis.

Conclusions:
Our results suggest that the impact of transfusion ratios on hemostasis is dynamic. Overall, the transfusion ratios had no significant impact on mortality over time. However, receiving higher ratios of platelets and plasma relative to red blood cells hastens hemostasis in subjects who have yet to achieve hemostasis within 3 hours after hospital admission.},
	number = {3},
	urldate = {2023-07-21},
	journal = {The journal of trauma and acute care surgery},
	author = {Nguyen, Minh and Pirracchio, Romain and Kornblith, Lucy Z. and Callcut, Rachael and Fox, Erin E. and Wade, Charles E. and Schreiber, Martin and Holcomb, John B. and Coyle, Jeremy and Cohen, Mitchell and Hubbard, Alan},
	month = sep,
	year = {2020},
	pmid = {32520897},
	pmcid = {PMC7830749},
	pages = {505--513},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/5BRBIPDP/Nguyen et al. - 2020 - Dynamic Impact of Transfusion Ratios on Outcomes i.pdf:application/pdf},
}

@article{hickey_effect_2021,
	title = {Effect of a patient-centered hypertension delivery strategy on all-cause mortality: {Secondary} analysis of {SEARCH}, a community-randomized trial in rural {Kenya} and {Uganda}},
	volume = {18},
	issn = {1549-1277},
	shorttitle = {Effect of a patient-centered hypertension delivery strategy on all-cause mortality},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8489716/},
	doi = {10.1371/journal.pmed.1003803},
	abstract = {Matthew Hickey and co-workers report on outcomes of the SEARCH trial of hypertension screening and care in sub-Saharan Africa.},
	number = {9},
	urldate = {2023-07-21},
	journal = {PLoS Medicine},
	author = {Hickey, Matthew D. and Ayieko, James and Owaraganise, Asiphas and Sim, Nicholas and Balzer, Laura B. and Kabami, Jane and Atukunda, Mucunguzi and Opel, Fredrick J. and Wafula, Erick and Nyabuti, Marilyn and Brown, Lillian and Chamie, Gabriel and Jain, Vivek and Peng, James and Kwarisiima, Dalsone and Camlin, Carol S. and Charlebois, Edwin D. and Cohen, Craig R. and Bukusi, Elizabeth A. and Kamya, Moses R. and Petersen, Maya L. and Havlir, Diane V.},
	month = sep,
	year = {2021},
	pmid = {34543267},
	pmcid = {PMC8489716},
	pages = {e1003803},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/WY2P44L8/Hickey et al. - 2021 - Effect of a patient-centered hypertension delivery.pdf:application/pdf},
}

@misc{us_food_and_drug_administration_adjusting_2023,
	title = {Adjusting for {Covariates} in {Randomized} {Clinical} {Trials} for {Drugs} and {Biological} {Products}},
	url = {https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adjusting-covariates-randomized-clinical-trials-drugs-and-biological-products},
	abstract = {Adjusting for Covariates in Randomized Clinical Trials for Drugs and Biological Products},
	language = {en},
	urldate = {2023-07-21},
	journal = {U.S. Food and Drug Administration},
	author = {{U.S. Food and Drug Administration.}},
	month = may,
	year = {2023},
	note = {Publisher: FDA},
	file = {Snapshot:/home/imbroglio/Zotero/storage/5SGYH9GW/adjusting-covariates-randomized-clinical-trials-drugs-and-biological-products.html:text/html},
}

@article{van_der_laan_super_2007,
	title = {Super {Learner}},
	volume = {6},
	issn = {1544-6115, 2194-6302},
	url = {https://www.degruyter.com/document/doi/10.2202/1544-6115.1309/html},
	doi = {10.2202/1544-6115.1309},
	abstract = {When trying to learn a model for the prediction of an outcome given a set of covariates, a statistician has many estimation procedures in their toolbox. A few examples of these candidate learners are: least squares, least angle regression, random forests, and spline regression. Previous articles (van der Laan and Dudoit (2003); van der Laan et al. (2006); Sinisi et al. (2007)) theoretically validated the use of cross validation to select an optimal learner among many candidate learners. Motivated by this use of cross validation, we propose a new prediction method for creating a weighted combination of many candidate learners to build the super learner. This article proposes a fast algorithm for constructing a super learner in prediction which uses V-fold cross-validation to select weights to combine an initial set of candidate learners. In addition, this paper contains a practical demonstration of the adaptivity of this so called super learner to various true data generating distributions. This approach for construction of a super learner generalizes to any parameter which can be defined as a minimizer of a loss function.},
	number = {1},
	urldate = {2023-09-07},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Van Der Laan, Mark J. and Polley, Eric C and Hubbard, Alan E.},
	month = jan,
	year = {2007},
}

@book{hubbard_chapter_nodate,
	title = {Chapter 6 {Super} {Learning} {\textbar} {Targeted} {Learning} in {R}},
	url = {https://tlverse.org/tlverse-handbook/sl3.html},
	abstract = {Rachael Phillips Based on the sl3 R package by Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, and Oleg Sofrygin. Learning Objectives  By the end of this chapter you will be able to:...},
	language = {en},
	urldate = {2023-09-07},
	author = {Hubbard, Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Alan, Mark van der Laan},
	file = {Snapshot:/home/imbroglio/Zotero/storage/H338F3XU/sl3.html:text/html},
}
